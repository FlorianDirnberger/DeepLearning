Loaded module: cuda/11.8
Loaded dependency [python3/3.10.13]: sqlite3/3.45.1
Loaded dependency [python3/3.10.13]: gcc/12.3.0-binutils-2.40
Loaded module: python3/3.10.13

Loading python3/3.10.13
  Loading requirement: sqlite3/3.45.1 gcc/12.3.0-binutils-2.40
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: raimo-sieber (raimo-sieber-technical-university-of-munich). Use `wandb login --relogin` to force relogin
wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.
wandb: WARNING To avoid this, please fix the sweep config schema violations below:
wandb: WARNING   Violation 1. Additional properties are not allowed ('count' was unexpected)
wandb: Agent Starting Run: v3wdq5kc with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_034455-v3wdq5kc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-1
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v3wdq5kc
wandb:                                                                                
wandb: 🚀 View run young-sweep-1 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v3wdq5kc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_034455-v3wdq5kc/logs
wandb: Agent Starting Run: jbqfevop with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_034505-jbqfevop
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-2
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jbqfevop
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▄▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▇▆▅▂▁▁▁▁▂▁▁▁▁▂▂▁▁▁▁▁▂▁▂▁▂▁▁▂▁▂▁▂▂▂▁▂▂▂▂
wandb: validation_rmse █▇▆▆▃▁▂▁▂▂▂▁▂▁▁▁▂▁▂▁▁▂▂▂▁▂▁▂▂▂▂▂▂▂▂▁▂▂▂▂
wandb: 
wandb: Run summary:
wandb:      train_loss 11.1904
wandb:      train_rmse 3.3452
wandb: validation_loss 74.45666
wandb: validation_rmse 8.62883
wandb: 
wandb: 🚀 View run devoted-sweep-2 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jbqfevop
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_034505-jbqfevop/logs
wandb: Agent Starting Run: ssz3u8y5 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035455-ssz3u8y5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-3
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ssz3u8y5
wandb:                                                                                
wandb: 🚀 View run graceful-sweep-3 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ssz3u8y5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035455-ssz3u8y5/logs
wandb: Agent Starting Run: 4f77z6h6 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035500-4f77z6h6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-4
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4f77z6h6
wandb:                                                                                
wandb: 🚀 View run lemon-sweep-4 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4f77z6h6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035500-4f77z6h6/logs
wandb: Agent Starting Run: rr8avlg6 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035505-rr8avlg6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-5
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rr8avlg6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▂▁▂▂▁▁▁
wandb:      train_rmse █▄▃▂▁▂▂▁▁▁
wandb: validation_loss █▅▄▃▂▂▂▁▁▁
wandb: validation_rmse █▅▄▃▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 933.18149
wandb:      train_rmse 30.54802
wandb: validation_loss 929.15375
wandb: validation_rmse 30.48202
wandb: 
wandb: 🚀 View run restful-sweep-5 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rr8avlg6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035505-rr8avlg6/logs
wandb: Agent Starting Run: 7jlg3s5z with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035649-7jlg3s5z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-6
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7jlg3s5z
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run absurd-sweep-6 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7jlg3s5z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035649-7jlg3s5z/logs
wandb: Agent Starting Run: 7k1qxtg2 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035654-7k1qxtg2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-7
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7k1qxtg2
wandb:                                                                                
wandb: 🚀 View run winter-sweep-7 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7k1qxtg2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035654-7k1qxtg2/logs
wandb: Agent Starting Run: 3y2kqlty with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035700-3y2kqlty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-8
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3y2kqlty
wandb:                                                                                
wandb: 🚀 View run quiet-sweep-8 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3y2kqlty
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035700-3y2kqlty/logs
wandb: Agent Starting Run: fcxv95h5 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035705-fcxv95h5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-9
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fcxv95h5
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 🚀 View run vocal-sweep-9 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fcxv95h5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035705-fcxv95h5/logs
wandb: Agent Starting Run: rnwedm1n with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035715-rnwedm1n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-10
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rnwedm1n
wandb:                                                                                
wandb: 🚀 View run peachy-sweep-10 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rnwedm1n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035715-rnwedm1n/logs
wandb: Agent Starting Run: 547evr28 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035721-547evr28
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-11
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/547evr28
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run silvery-sweep-11 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/547evr28
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035721-547evr28/logs
Run 547evr28 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 547evr28 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: urooa989 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035726-urooa989
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-12
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/urooa989
wandb:                                                                                
wandb: 🚀 View run balmy-sweep-12 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/urooa989
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035726-urooa989/logs
wandb: Agent Starting Run: baaw9alx with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035731-baaw9alx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-13
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/baaw9alx
wandb:                                                                                
wandb: 🚀 View run decent-sweep-13 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/baaw9alx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035731-baaw9alx/logs
wandb: Agent Starting Run: 2eg5rtji with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035737-2eg5rtji
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-14
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2eg5rtji
wandb:                                                                                
wandb: 🚀 View run celestial-sweep-14 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2eg5rtji
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035737-2eg5rtji/logs
wandb: Agent Starting Run: q7bxr8ov with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_035742-q7bxr8ov
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-15
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q7bxr8ov
wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:      train_loss nan
wandb:      train_rmse nan
wandb: validation_loss nan
wandb: validation_rmse nan
wandb: 
wandb: 🚀 View run winter-sweep-15 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q7bxr8ov
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_035742-q7bxr8ov/logs
wandb: Agent Starting Run: q0m8cohp with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_040614-q0m8cohp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-16
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q0m8cohp
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 🚀 View run avid-sweep-16 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q0m8cohp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_040614-q0m8cohp/logs
wandb: Agent Starting Run: ap8xlzvk with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_040619-ap8xlzvk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-17
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ap8xlzvk
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▇▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▆▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▇▃▃▃▂▂▂▂▁▂▃▃▁▂▁▁▁▁▁▂▂▁▂▁▁▂▁▁▁▁▂▁▂▁▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 36.31559
wandb:      train_rmse 6.02624
wandb: validation_loss 13.47642
wandb: validation_rmse 3.67102
wandb: 
wandb: 🚀 View run generous-sweep-17 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ap8xlzvk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_040619-ap8xlzvk/logs
wandb: Agent Starting Run: h64inm05 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_041517-h64inm05
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-18
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h64inm05
wandb:                                                                                
wandb: 🚀 View run dauntless-sweep-18 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h64inm05
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_041517-h64inm05/logs
wandb: Agent Starting Run: 56l007v6 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_041522-56l007v6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-19
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/56l007v6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▆▅▄▃▃▃▁
wandb:      train_rmse █▇▆▆▅▄▃▃▃▁
wandb: validation_loss █▇▆▆▅▄▃▃▂▁
wandb: validation_rmse █▇▆▆▅▄▃▃▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 808.00592
wandb:      train_rmse 28.42544
wandb: validation_loss 805.03824
wandb: validation_rmse 28.3732
wandb: 
wandb: 🚀 View run proud-sweep-19 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/56l007v6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_041522-56l007v6/logs
wandb: Agent Starting Run: nnjqqptz with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_041706-nnjqqptz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-20
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nnjqqptz
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [128, -32]
wandb:                                                                                
wandb: 🚀 View run curious-sweep-20 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nnjqqptz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_041706-nnjqqptz/logs
Run nnjqqptz errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [128, -32]

wandb: ERROR Run nnjqqptz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -32: [128, -32]
wandb: ERROR 
wandb: Agent Starting Run: 1ldtn8wo with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_041711-1ldtn8wo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-21
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1ldtn8wo
wandb:                                                                                
wandb: 🚀 View run honest-sweep-21 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1ldtn8wo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_041711-1ldtn8wo/logs
wandb: Agent Starting Run: uaa4jqdw with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_041716-uaa4jqdw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-22
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uaa4jqdw
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-22 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uaa4jqdw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_041716-uaa4jqdw/logs
wandb: Agent Starting Run: 4g0tjevr with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_041722-4g0tjevr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-23
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4g0tjevr
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▆▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▆▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▆▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 14.78977
wandb:      train_rmse 3.84575
wandb: validation_loss 13.98591
wandb: validation_rmse 3.73977
wandb: 
wandb: 🚀 View run olive-sweep-23 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4g0tjevr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_041722-4g0tjevr/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: sbl98zwt with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_042548-sbl98zwt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-24
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sbl98zwt
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb:                                                                                
wandb: 🚀 View run fast-sweep-24 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sbl98zwt
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_042548-sbl98zwt/logs
wandb: Agent Starting Run: i5okl5xa with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_042625-i5okl5xa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-25
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i5okl5xa
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -448: [32, -448]
wandb:                                                                                
wandb: 🚀 View run pretty-sweep-25 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i5okl5xa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_042625-i5okl5xa/logs
Run i5okl5xa errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -448: [32, -448]

wandb: ERROR Run i5okl5xa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -448: [32, -448]
wandb: ERROR 
wandb: Agent Starting Run: ue14dj34 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_042630-ue14dj34
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-26
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ue14dj34
wandb:                                                                                
wandb: 🚀 View run dark-sweep-26 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ue14dj34
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_042630-ue14dj34/logs
wandb: Agent Starting Run: oesbeba3 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_042635-oesbeba3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-27
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oesbeba3
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -256: [64, -256]
wandb:                                                                                
wandb: 🚀 View run visionary-sweep-27 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oesbeba3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_042635-oesbeba3/logs
Run oesbeba3 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -256: [64, -256]

wandb: ERROR Run oesbeba3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -256: [64, -256]
wandb: ERROR 
wandb: Agent Starting Run: s293maoq with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_042641-s293maoq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-28
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s293maoq
wandb:                                                                                
wandb: 🚀 View run visionary-sweep-28 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s293maoq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_042641-s293maoq/logs
wandb: Agent Starting Run: hedsjv7b with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_042646-hedsjv7b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-29
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hedsjv7b
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▄▃▃▄▃▂▂▂▄▂▂▂▂▂▁▁▃▁▄▂▁▁▁▁▂▃▁▂▁▂▁▂▁▁▁▁▄▁▁
wandb: validation_rmse █▅▄▃▅▃▂▂▅▂▃▂▂▃▃▁▃▁▅▁▁▁▁▁▁▃▁▃▁▁▂▂▁▁▁▁▅▁▁▂
wandb: 
wandb: Run summary:
wandb:      train_loss 3.86807
wandb:      train_rmse 1.96674
wandb: validation_loss 7.44757
wandb: validation_rmse 2.72902
wandb: 
wandb: 🚀 View run summer-sweep-29 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hedsjv7b
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_042646-hedsjv7b/logs
wandb: Agent Starting Run: jsrzfiz8 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043507-jsrzfiz8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-30
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jsrzfiz8
wandb:                                                                                
wandb: 🚀 View run dulcet-sweep-30 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jsrzfiz8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043507-jsrzfiz8/logs
wandb: Agent Starting Run: rhauaq3e with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043512-rhauaq3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-31
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rhauaq3e
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-31 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rhauaq3e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043512-rhauaq3e/logs
wandb: Agent Starting Run: qgswlse2 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043518-qgswlse2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-32
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qgswlse2
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run lemon-sweep-32 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qgswlse2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043518-qgswlse2/logs
wandb: Agent Starting Run: 4sn9nka1 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043523-4sn9nka1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-33
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4sn9nka1
wandb:                                                                                
wandb: 🚀 View run cool-sweep-33 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4sn9nka1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043523-4sn9nka1/logs
wandb: Agent Starting Run: 1jr1fj2l with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043528-1jr1fj2l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-34
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1jr1fj2l
wandb:                                                                                
wandb: 🚀 View run lunar-sweep-34 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1jr1fj2l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043528-1jr1fj2l/logs
wandb: Agent Starting Run: qa17l7g5 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043534-qa17l7g5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-35
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qa17l7g5
wandb:                                                                                
wandb: 🚀 View run rural-sweep-35 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qa17l7g5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043534-qa17l7g5/logs
wandb: Agent Starting Run: x57clu4q with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043539-x57clu4q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-36
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x57clu4q
wandb:                                                                                
wandb: 🚀 View run hearty-sweep-36 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x57clu4q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043539-x57clu4q/logs
wandb: Agent Starting Run: gf7yjugq with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043545-gf7yjugq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-37
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gf7yjugq
wandb:                                                                                
wandb: 🚀 View run prime-sweep-37 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gf7yjugq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043545-gf7yjugq/logs
wandb: Agent Starting Run: gb380w2n with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043550-gb380w2n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-38
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gb380w2n
wandb:                                                                                
wandb: 🚀 View run earnest-sweep-38 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gb380w2n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043550-gb380w2n/logs
wandb: Agent Starting Run: ic9u0ttz with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043556-ic9u0ttz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-39
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ic9u0ttz
wandb:                                                                                
wandb: 🚀 View run visionary-sweep-39 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ic9u0ttz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043556-ic9u0ttz/logs
wandb: Agent Starting Run: sgb0jyo5 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043601-sgb0jyo5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-40
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sgb0jyo5
wandb:                                                                                
wandb: 🚀 View run still-sweep-40 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sgb0jyo5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043601-sgb0jyo5/logs
wandb: Agent Starting Run: 8rm5d1sc with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043606-8rm5d1sc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-41
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8rm5d1sc
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run woven-sweep-41 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8rm5d1sc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043606-8rm5d1sc/logs
wandb: Agent Starting Run: tt3ww3vg with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043612-tt3ww3vg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-42
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tt3ww3vg
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fine-sweep-42 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tt3ww3vg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043612-tt3ww3vg/logs
wandb: Agent Starting Run: redftbvg with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043617-redftbvg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-43
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/redftbvg
wandb:                                                                                
wandb: 🚀 View run rural-sweep-43 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/redftbvg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043617-redftbvg/logs
wandb: Agent Starting Run: rbnl34rv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043622-rbnl34rv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-44
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rbnl34rv
wandb:                                                                                
wandb: 🚀 View run serene-sweep-44 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rbnl34rv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043622-rbnl34rv/logs
wandb: Agent Starting Run: 7cfojh9c with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043627-7cfojh9c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-45
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7cfojh9c
wandb:                                                                                
wandb: 🚀 View run curious-sweep-45 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7cfojh9c
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043627-7cfojh9c/logs
wandb: Agent Starting Run: r29t1z0s with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043633-r29t1z0s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-46
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r29t1z0s
wandb:                                                                                
wandb: 🚀 View run exalted-sweep-46 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r29t1z0s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043633-r29t1z0s/logs
wandb: Agent Starting Run: h9p7s5q1 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043638-h9p7s5q1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-47
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h9p7s5q1
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 🚀 View run rose-sweep-47 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h9p7s5q1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043638-h9p7s5q1/logs
wandb: Agent Starting Run: 80zcj0k4 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043643-80zcj0k4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-48
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/80zcj0k4
wandb:                                                                                
wandb: 🚀 View run playful-sweep-48 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/80zcj0k4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043643-80zcj0k4/logs
wandb: Agent Starting Run: 42gwd9md with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043649-42gwd9md
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-49
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/42gwd9md
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run sparkling-sweep-49 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/42gwd9md
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043649-42gwd9md/logs
Run 42gwd9md errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 42gwd9md errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: iwduxuku with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043654-iwduxuku
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-50
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/iwduxuku
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-50 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/iwduxuku
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043654-iwduxuku/logs
wandb: Agent Starting Run: 9lf3x3ub with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043700-9lf3x3ub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-51
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9lf3x3ub
wandb:                                                                                
wandb: 🚀 View run good-sweep-51 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9lf3x3ub
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043700-9lf3x3ub/logs
wandb: Agent Starting Run: sk49kbm8 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043705-sk49kbm8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-52
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sk49kbm8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▅▄▃▄▂▁▁▁▁
wandb:      train_rmse █▅▄▃▄▂▁▁▁▁
wandb: validation_loss ▂▁▂▃▅▅▅▆▇█
wandb: validation_rmse ▂▁▂▃▅▅▅▆▇█
wandb: 
wandb: Run summary:
wandb:      train_loss 946.77392
wandb:      train_rmse 30.76969
wandb: validation_loss 1100.31506
wandb: validation_rmse 33.171
wandb: 
wandb: 🚀 View run charmed-sweep-52 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sk49kbm8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043705-sk49kbm8/logs
wandb: Agent Starting Run: h2j1y5tb with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043854-h2j1y5tb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-53
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h2j1y5tb
wandb:                                                                                
wandb: 🚀 View run whole-sweep-53 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h2j1y5tb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043854-h2j1y5tb/logs
wandb: Agent Starting Run: yp031513 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043859-yp031513
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-54
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yp031513
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run silver-sweep-54 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yp031513
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043859-yp031513/logs
wandb: Agent Starting Run: b583tont with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043904-b583tont
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-55
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b583tont
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x2x26). Calculated output size: (32x0x6). Output size is too small
wandb:                                                                                
wandb: 🚀 View run dutiful-sweep-55 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b583tont
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043904-b583tont/logs
Run b583tont errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x2x26). Calculated output size: (32x0x6). Output size is too small

wandb: ERROR Run b583tont errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (32x2x26). Calculated output size: (32x0x6). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: ewcwv0kd with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043910-ewcwv0kd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-56
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ewcwv0kd
wandb:                                                                                
wandb: 🚀 View run ethereal-sweep-56 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ewcwv0kd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043910-ewcwv0kd/logs
wandb: Agent Starting Run: dlrcsnwq with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043915-dlrcsnwq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-57
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dlrcsnwq
wandb:                                                                                
wandb: 🚀 View run fresh-sweep-57 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dlrcsnwq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043915-dlrcsnwq/logs
wandb: Agent Starting Run: 9od92sqc with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043920-9od92sqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-58
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9od92sqc
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run whole-sweep-58 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9od92sqc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043920-9od92sqc/logs
wandb: Agent Starting Run: cdm0xf9p with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043926-cdm0xf9p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-59
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cdm0xf9p
wandb:                                                                                
wandb: 🚀 View run smart-sweep-59 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cdm0xf9p
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043926-cdm0xf9p/logs
wandb: Agent Starting Run: jr9pp32m with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043931-jr9pp32m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-60
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jr9pp32m
wandb:                                                                                
wandb: 🚀 View run quiet-sweep-60 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jr9pp32m
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043931-jr9pp32m/logs
wandb: Agent Starting Run: 9ib9yi63 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043937-9ib9yi63
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-61
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9ib9yi63
wandb:                                                                                
wandb: 🚀 View run breezy-sweep-61 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9ib9yi63
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043937-9ib9yi63/logs
wandb: Agent Starting Run: pud8kvsr with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_043942-pud8kvsr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-62
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pud8kvsr
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▆▅▄▃▃▂▂▁▁
wandb:      train_rmse █▆▅▄▃▃▂▂▁▁
wandb: validation_loss █▇▆▅▄▃▃▂▂▁
wandb: validation_rmse █▇▆▅▄▃▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 890.03397
wandb:      train_rmse 29.83344
wandb: validation_loss 921.65494
wandb: validation_rmse 30.35877
wandb: 
wandb: 🚀 View run logical-sweep-62 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pud8kvsr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_043942-pud8kvsr/logs
wandb: Agent Starting Run: 98i4f6dn with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_044125-98i4f6dn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-63
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/98i4f6dn
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▃▃▂▃▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 6.80012
wandb:      train_rmse 2.6077
wandb: validation_loss 5.78713
wandb: validation_rmse 2.40565
wandb: 
wandb: 🚀 View run silvery-sweep-63 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/98i4f6dn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_044125-98i4f6dn/logs
wandb: Agent Starting Run: aw370wnr with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_044947-aw370wnr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-64
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aw370wnr
wandb:                                                                                
wandb: 🚀 View run icy-sweep-64 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aw370wnr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_044947-aw370wnr/logs
wandb: Agent Starting Run: eu799mrf with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_044952-eu799mrf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-65
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eu799mrf
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▇▆▅▄▃▃▂▁
wandb:      train_rmse █▇▇▆▅▄▃▃▂▁
wandb: validation_loss █▇▆▅▅▄▃▂▂▁
wandb: validation_rmse █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 849.32647
wandb:      train_rmse 29.14321
wandb: validation_loss 846.36243
wandb: validation_rmse 29.09231
wandb: 
wandb: 🚀 View run cool-sweep-65 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eu799mrf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_044952-eu799mrf/logs
wandb: Agent Starting Run: 9f7dw8gr with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045136-9f7dw8gr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-66
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9f7dw8gr
wandb:                                                                                
wandb: 🚀 View run exalted-sweep-66 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9f7dw8gr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045136-9f7dw8gr/logs
wandb: Agent Starting Run: 4oskui6s with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045141-4oskui6s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-67
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4oskui6s
wandb:                                                                                
wandb: 🚀 View run silver-sweep-67 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4oskui6s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045141-4oskui6s/logs
wandb: Agent Starting Run: ew9sq0wy with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045146-ew9sq0wy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-68
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ew9sq0wy
wandb:                                                                                
wandb: 🚀 View run trim-sweep-68 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ew9sq0wy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045146-ew9sq0wy/logs
wandb: Agent Starting Run: ggiqk97a with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045152-ggiqk97a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-69
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ggiqk97a
wandb:                                                                                
wandb: 🚀 View run wild-sweep-69 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ggiqk97a
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045152-ggiqk97a/logs
wandb: Agent Starting Run: 7s0tmwfp with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045157-7s0tmwfp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-70
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7s0tmwfp
wandb:                                                                                
wandb: 🚀 View run vocal-sweep-70 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7s0tmwfp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045157-7s0tmwfp/logs
wandb: Agent Starting Run: vkrn11il with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045202-vkrn11il
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-71
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vkrn11il
wandb:                                                                                
wandb: 🚀 View run ethereal-sweep-71 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vkrn11il
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045202-vkrn11il/logs
wandb: Agent Starting Run: cj1ee0gr with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045208-cj1ee0gr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-72
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cj1ee0gr
wandb:                                                                                
wandb: 🚀 View run curious-sweep-72 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cj1ee0gr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045208-cj1ee0gr/logs
wandb: Agent Starting Run: sfzd43ao with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045214-sfzd43ao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-73
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sfzd43ao
wandb:                                                                                
wandb: 🚀 View run lively-sweep-73 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sfzd43ao
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045214-sfzd43ao/logs
wandb: Agent Starting Run: yi3bzrpl with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045219-yi3bzrpl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-74
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yi3bzrpl
wandb:                                                                                
wandb: 🚀 View run super-sweep-74 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yi3bzrpl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045219-yi3bzrpl/logs
wandb: Agent Starting Run: 8geibgt8 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045224-8geibgt8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-75
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8geibgt8
wandb:                                                                                
wandb: 🚀 View run clear-sweep-75 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8geibgt8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045224-8geibgt8/logs
wandb: Agent Starting Run: vsegb6d9 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045230-vsegb6d9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-76
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vsegb6d9
wandb:                                                                                
wandb: 🚀 View run fancy-sweep-76 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vsegb6d9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045230-vsegb6d9/logs
wandb: Agent Starting Run: doqied16 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045235-doqied16
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-77
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/doqied16
wandb:                                                                                
wandb: 🚀 View run neat-sweep-77 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/doqied16
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045235-doqied16/logs
wandb: Agent Starting Run: 1yqlwxf1 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045240-1yqlwxf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-78
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1yqlwxf1
wandb:                                                                                
wandb: 🚀 View run graceful-sweep-78 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1yqlwxf1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045240-1yqlwxf1/logs
wandb: Agent Starting Run: czmze3y7 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045246-czmze3y7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-79
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/czmze3y7
wandb:                                                                                
wandb: 🚀 View run peachy-sweep-79 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/czmze3y7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045246-czmze3y7/logs
wandb: Agent Starting Run: qgg0zxsw with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045251-qgg0zxsw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-80
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qgg0zxsw
wandb:                                                                                
wandb: 🚀 View run light-sweep-80 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qgg0zxsw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045251-qgg0zxsw/logs
wandb: Agent Starting Run: ve5vk0oc with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045256-ve5vk0oc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-81
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ve5vk0oc
wandb:                                                                                
wandb: 🚀 View run genial-sweep-81 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ve5vk0oc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045256-ve5vk0oc/logs
wandb: Agent Starting Run: d6sb8quz with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045302-d6sb8quz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-82
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d6sb8quz
wandb:                                                                                
wandb: 🚀 View run bumbling-sweep-82 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d6sb8quz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045302-d6sb8quz/logs
wandb: Agent Starting Run: c2ssrdk1 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045307-c2ssrdk1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-83
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c2ssrdk1
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run solar-sweep-83 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c2ssrdk1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045307-c2ssrdk1/logs
wandb: Agent Starting Run: 98rfwrb2 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045312-98rfwrb2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-84
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/98rfwrb2
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (16x2x27). Calculated output size: (16x0x6). Output size is too small
wandb:                                                                                
wandb: 🚀 View run pious-sweep-84 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/98rfwrb2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045312-98rfwrb2/logs
Run 98rfwrb2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (16x2x27). Calculated output size: (16x0x6). Output size is too small

wandb: ERROR Run 98rfwrb2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (16x2x27). Calculated output size: (16x0x6). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: dslar2tk with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045318-dslar2tk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-85
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dslar2tk
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 67.91731
wandb:      train_rmse 8.2412
wandb: validation_loss 73.49888
wandb: validation_rmse 8.57315
wandb: 
wandb: 🚀 View run vital-sweep-85 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dslar2tk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045318-dslar2tk/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: d2ct5ke4 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045516-d2ct5ke4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-86
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d2ct5ke4
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-86 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d2ct5ke4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045516-d2ct5ke4/logs
wandb: Agent Starting Run: hxnsdwpo with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045522-hxnsdwpo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-87
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hxnsdwpo
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▁▁▁▁▁▁▁▁
wandb:      train_rmse █▂▁▁▁▁▁▁▁▁
wandb: validation_loss █▂▁▁▁▁▁▁▁▁
wandb: validation_rmse █▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 82.24909
wandb:      train_rmse 9.06913
wandb: validation_loss 73.48386
wandb: validation_rmse 8.57227
wandb: 
wandb: 🚀 View run fluent-sweep-87 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hxnsdwpo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045522-hxnsdwpo/logs
wandb: Agent Starting Run: fb1yusnu with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045706-fb1yusnu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-88
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fb1yusnu
wandb:                                                                                
wandb: 🚀 View run curious-sweep-88 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fb1yusnu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045706-fb1yusnu/logs
wandb: Agent Starting Run: 8p6ket5w with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045711-8p6ket5w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-89
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8p6ket5w
wandb:                                                                                
wandb: 🚀 View run upbeat-sweep-89 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8p6ket5w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045711-8p6ket5w/logs
wandb: Agent Starting Run: gnh9jw2y with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045716-gnh9jw2y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-90
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gnh9jw2y
wandb:                                                                                
wandb: 🚀 View run different-sweep-90 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gnh9jw2y
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045716-gnh9jw2y/logs
wandb: Agent Starting Run: yek7vr68 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045722-yek7vr68
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-91
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yek7vr68
wandb:                                                                                
wandb: 🚀 View run amber-sweep-91 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yek7vr68
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045722-yek7vr68/logs
wandb: Agent Starting Run: 7f5j5qb9 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_045727-7f5j5qb9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-92
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7f5j5qb9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▅▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▇▆▆▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▇▆▆▅▄▃▃▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 37.70467
wandb:      train_rmse 6.14041
wandb: validation_loss 8.95423
wandb: validation_rmse 2.99236
wandb: 
wandb: 🚀 View run confused-sweep-92 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7f5j5qb9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_045727-7f5j5qb9/logs
wandb: Agent Starting Run: fqgot08t with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050548-fqgot08t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-93
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fqgot08t
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -64: [128, -64]
wandb:                                                                                
wandb: 🚀 View run fanciful-sweep-93 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fqgot08t
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050548-fqgot08t/logs
Run fqgot08t errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -64: [128, -64]

wandb: ERROR Run fqgot08t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -64: [128, -64]
wandb: ERROR 
wandb: Agent Starting Run: l0hq6psp with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050554-l0hq6psp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-94
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l0hq6psp
wandb:                                                                                
wandb: 🚀 View run rare-sweep-94 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l0hq6psp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050554-l0hq6psp/logs
wandb: Agent Starting Run: x8t8hjq2 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050559-x8t8hjq2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-95
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x8t8hjq2
wandb:                                                                                
wandb: 🚀 View run polished-sweep-95 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x8t8hjq2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050559-x8t8hjq2/logs
wandb: Agent Starting Run: h3bwfnla with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050604-h3bwfnla
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-96
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h3bwfnla
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss ██▇▆▅▅▄▃▂▁
wandb:      train_rmse ██▇▇▅▅▄▃▂▁
wandb: validation_loss █▇▇▇▅▇▅▃▂▁
wandb: validation_rmse █▇▇▇▅▇▆▃▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 729.51642
wandb:      train_rmse 27.00956
wandb: validation_loss 595.17615
wandb: validation_rmse 24.39623
wandb: 
wandb: 🚀 View run fast-sweep-96 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h3bwfnla
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050604-h3bwfnla/logs
wandb: Agent Starting Run: ioe2f6ho with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050748-ioe2f6ho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-97
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ioe2f6ho
wandb:                                                                                
wandb: 🚀 View run wobbly-sweep-97 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ioe2f6ho
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050748-ioe2f6ho/logs
wandb: Agent Starting Run: v9klfvpi with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050753-v9klfvpi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-98
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v9klfvpi
wandb:                                                                                
wandb: 🚀 View run radiant-sweep-98 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v9klfvpi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050753-v9klfvpi/logs
wandb: Agent Starting Run: xk06ocri with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050759-xk06ocri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-99
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xk06ocri
wandb:                                                                                
wandb: 🚀 View run azure-sweep-99 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xk06ocri
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050759-xk06ocri/logs
wandb: Agent Starting Run: pnqz8g50 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050804-pnqz8g50
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-100
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pnqz8g50
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run summer-sweep-100 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pnqz8g50
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050804-pnqz8g50/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: zqg4qyit with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_050848-zqg4qyit
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-101
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zqg4qyit
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂
wandb: 
wandb: Run summary:
wandb:      train_loss 19.1927
wandb:      train_rmse 4.38095
wandb: validation_loss 35.18223
wandb: validation_rmse 5.93146
wandb: 
wandb: 🚀 View run misty-sweep-101 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zqg4qyit
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_050848-zqg4qyit/logs
wandb: Agent Starting Run: adt1zisz with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_051706-adt1zisz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-102
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/adt1zisz
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▆▄▃▃▂▃▂▁▁
wandb:      train_rmse █▆▄▃▃▂▃▂▁▁
wandb: validation_loss ▇▇█▆▃▄▅▃▄▁
wandb: validation_rmse ▇▇█▆▃▄▅▃▄▁
wandb: 
wandb: Run summary:
wandb:      train_loss 963.90087
wandb:      train_rmse 31.04675
wandb: validation_loss 956.26636
wandb: validation_rmse 30.92356
wandb: 
wandb: 🚀 View run likely-sweep-102 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/adt1zisz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_051706-adt1zisz/logs
wandb: Agent Starting Run: uz6icwkl with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_051855-uz6icwkl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-103
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uz6icwkl
wandb:                                                                                
wandb: 🚀 View run skilled-sweep-103 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uz6icwkl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_051855-uz6icwkl/logs
wandb: Agent Starting Run: y5fpxgyv with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_051900-y5fpxgyv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-104
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y5fpxgyv
wandb:                                                                                
wandb: 🚀 View run brisk-sweep-104 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y5fpxgyv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_051900-y5fpxgyv/logs
wandb: Agent Starting Run: 28gh8723 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_051906-28gh8723
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-105
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/28gh8723
wandb:                                                                                
wandb: 🚀 View run robust-sweep-105 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/28gh8723
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_051906-28gh8723/logs
wandb: Agent Starting Run: esuyyryx with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_051911-esuyyryx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-106
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/esuyyryx
wandb:                                                                                
wandb: 🚀 View run flowing-sweep-106 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/esuyyryx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_051911-esuyyryx/logs
wandb: Agent Starting Run: 50rcz1yx with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_051916-50rcz1yx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-107
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/50rcz1yx
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run wandering-sweep-107 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/50rcz1yx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_051916-50rcz1yx/logs
Run 50rcz1yx errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 50rcz1yx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: 0amj8yfa with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_051921-0amj8yfa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-108
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0amj8yfa
wandb:                                                                                
wandb: 🚀 View run graceful-sweep-108 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0amj8yfa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_051921-0amj8yfa/logs
wandb: Agent Starting Run: xzii00hh with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_051927-xzii00hh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-109
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xzii00hh
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▂▁▁▁▁▁▁▁▁
wandb: validation_loss █▂▁▁▁▁▁▁▁▁
wandb: validation_rmse █▂▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 69.21181
wandb:      train_rmse 8.31936
wandb: validation_loss 82.85308
wandb: validation_rmse 9.10237
wandb: 
wandb: 🚀 View run fragrant-sweep-109 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xzii00hh
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_051927-xzii00hh/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: anxz5dpq with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_052118-anxz5dpq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-110
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/anxz5dpq
wandb:                                                                                
wandb: 🚀 View run quiet-sweep-110 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/anxz5dpq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_052118-anxz5dpq/logs
wandb: Agent Starting Run: dc5r4g0t with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_052125-dc5r4g0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-111
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dc5r4g0t
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▆▄▅▄▄▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▁█▅▃▂▂▂▃▁▇▂▁▂▂▁▂▂▁▃▁▁▃▁▂▁▁▂▂▂▁▁▂▂▂▂▂▂▁▂
wandb: validation_rmse █▂█▆▄▃▂▃▄▂▇▂▁▃▃▂▃▂▂▄▁▂▄▁▃▁▂▂▂▃▂▂▃▂▂▂▃▃▂▃
wandb: 
wandb: Run summary:
wandb:      train_loss 44.1544
wandb:      train_rmse 6.64488
wandb: validation_loss 93.7986
wandb: validation_rmse 9.68497
wandb: 
wandb: 🚀 View run neat-sweep-111 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dc5r4g0t
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_052125-dc5r4g0t/logs
wandb: Agent Starting Run: rsrqp8bw with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_052941-rsrqp8bw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-112
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rsrqp8bw
wandb:                                                                                
wandb: 🚀 View run scarlet-sweep-112 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rsrqp8bw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_052941-rsrqp8bw/logs
wandb: Agent Starting Run: g2b7jav1 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_052946-g2b7jav1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-113
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g2b7jav1
wandb:                                                                                
wandb: 🚀 View run rich-sweep-113 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g2b7jav1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_052946-g2b7jav1/logs
wandb: Agent Starting Run: 0f89uhua with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_052951-0f89uhua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-114
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0f89uhua
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▃▄▂▃▂▂▁▂▅▁▁▂▁▁▁▁▂▁▂▅▄▁▁▁▂▁▂▂▁▁▁▃▁▂▁▁▁▁▁
wandb: validation_rmse █▅▃▃▂▂▂▂▁▁▄▁▁▂▁▁▁▁▂▁▂▄▃▁▁▂▁▂▂▁▂▁▁▃▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 3.16609
wandb:      train_rmse 1.77935
wandb: validation_loss 5.17301
wandb: validation_rmse 2.27443
wandb: 
wandb: 🚀 View run helpful-sweep-114 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0f89uhua
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_052951-0f89uhua/logs
wandb: Agent Starting Run: px1u0zan with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_053807-px1u0zan
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-115
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/px1u0zan
wandb:                                                                                
wandb: 🚀 View run solar-sweep-115 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/px1u0zan
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_053807-px1u0zan/logs
wandb: Agent Starting Run: sgl1focv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_053813-sgl1focv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-116
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sgl1focv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▇▇▆▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▇▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 6.79595
wandb:      train_rmse 2.6069
wandb: validation_loss 10.04255
wandb: validation_rmse 3.169
wandb: 
wandb: 🚀 View run worthy-sweep-116 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sgl1focv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_053813-sgl1focv/logs
wandb: Agent Starting Run: szg4m02z with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054639-szg4m02z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-117
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/szg4m02z
wandb:                                                                                
wandb: 🚀 View run true-sweep-117 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/szg4m02z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054639-szg4m02z/logs
wandb: Agent Starting Run: qax3sw2j with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054645-qax3sw2j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-118
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qax3sw2j
wandb:                                                                                
wandb: 🚀 View run driven-sweep-118 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qax3sw2j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054645-qax3sw2j/logs
wandb: Agent Starting Run: wk2zsigp with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054650-wk2zsigp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-119
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wk2zsigp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▅▄▃▂▁▁▁▁
wandb:      train_rmse █▇▆▅▄▃▂▁▁▁
wandb: validation_loss █▆▅▄▂▂▁▁▁▁
wandb: validation_rmse █▇▆▄▃▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 67.64487
wandb:      train_rmse 8.22465
wandb: validation_loss 71.16176
wandb: validation_rmse 8.43574
wandb: 
wandb: 🚀 View run classic-sweep-119 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wk2zsigp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054650-wk2zsigp/logs
wandb: Agent Starting Run: r4q51w29 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054839-r4q51w29
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-120
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r4q51w29
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run grateful-sweep-120 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r4q51w29
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054839-r4q51w29/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: szthyg2d with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054852-szthyg2d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-121
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/szthyg2d
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run iconic-sweep-121 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/szthyg2d
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054852-szthyg2d/logs
Run szthyg2d errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run szthyg2d errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: xqsn5q0w with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054858-xqsn5q0w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-122
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xqsn5q0w
wandb:                                                                                
wandb: 🚀 View run earnest-sweep-122 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xqsn5q0w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054858-xqsn5q0w/logs
wandb: Agent Starting Run: 1kk818dm with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054904-1kk818dm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-123
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1kk818dm
wandb:                                                                                
wandb: 🚀 View run iconic-sweep-123 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1kk818dm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054904-1kk818dm/logs
wandb: Agent Starting Run: feh5ktgj with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054909-feh5ktgj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-124
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/feh5ktgj
wandb:                                                                                
wandb: 🚀 View run flowing-sweep-124 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/feh5ktgj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054909-feh5ktgj/logs
wandb: Agent Starting Run: js5nc9zc with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054914-js5nc9zc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-125
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/js5nc9zc
wandb:                                                                                
wandb: 🚀 View run robust-sweep-125 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/js5nc9zc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054914-js5nc9zc/logs
wandb: Agent Starting Run: rzogvw7k with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054920-rzogvw7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-126
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rzogvw7k
wandb:                                                                                
wandb: 🚀 View run legendary-sweep-126 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rzogvw7k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054920-rzogvw7k/logs
wandb: Agent Starting Run: mnyf07mq with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054925-mnyf07mq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-127
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mnyf07mq
wandb:                                                                                
wandb: 🚀 View run stoic-sweep-127 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mnyf07mq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054925-mnyf07mq/logs
wandb: Agent Starting Run: i8jjxu2w with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054930-i8jjxu2w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-128
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i8jjxu2w
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [128, -32]
wandb:                                                                                
wandb: 🚀 View run azure-sweep-128 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i8jjxu2w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054930-i8jjxu2w/logs
Run i8jjxu2w errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [128, -32]

wandb: ERROR Run i8jjxu2w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -32: [128, -32]
wandb: ERROR 
wandb: Agent Starting Run: 9x1mhnfq with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054936-9x1mhnfq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-129
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9x1mhnfq
wandb:                                                                                
wandb: 🚀 View run generous-sweep-129 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9x1mhnfq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054936-9x1mhnfq/logs
wandb: Agent Starting Run: tlsy2a4k with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054941-tlsy2a4k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-130
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tlsy2a4k
wandb:                                                                                
wandb: 🚀 View run legendary-sweep-130 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tlsy2a4k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054941-tlsy2a4k/logs
wandb: Agent Starting Run: uhfaql3c with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054947-uhfaql3c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-131
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uhfaql3c
wandb:                                                                                
wandb: 🚀 View run hopeful-sweep-131 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uhfaql3c
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054947-uhfaql3c/logs
wandb: Agent Starting Run: 7lnxhk0l with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_054952-7lnxhk0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-132
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7lnxhk0l
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▆▅▅▅▄▄▃▁▁
wandb:      train_rmse █▆▅▅▅▄▄▃▁▁
wandb: validation_loss █▅▄▄▄▃▃▂▂▁
wandb: validation_rmse █▅▄▄▄▃▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 964.73642
wandb:      train_rmse 31.06021
wandb: validation_loss 959.81961
wandb: validation_rmse 30.98096
wandb: 
wandb: 🚀 View run cosmic-sweep-132 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7lnxhk0l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_054952-7lnxhk0l/logs
wandb: Agent Starting Run: smor15an with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055136-smor15an
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-133
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/smor15an
wandb:                                                                                
wandb: 🚀 View run dainty-sweep-133 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/smor15an
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055136-smor15an/logs
wandb: Agent Starting Run: jfjra95b with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055141-jfjra95b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-134
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jfjra95b
wandb:                                                                                
wandb: 🚀 View run denim-sweep-134 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jfjra95b
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055141-jfjra95b/logs
wandb: Agent Starting Run: trscwy82 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055146-trscwy82
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-135
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/trscwy82
wandb:                                                                                
wandb: 🚀 View run proud-sweep-135 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/trscwy82
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055146-trscwy82/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: bygxfbr1 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055202-bygxfbr1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-136
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bygxfbr1
wandb:                                                                                
wandb: 🚀 View run hearty-sweep-136 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bygxfbr1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055202-bygxfbr1/logs
wandb: Agent Starting Run: 71alyy4j with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055207-71alyy4j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-137
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/71alyy4j
wandb:                                                                                
wandb: 🚀 View run fresh-sweep-137 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/71alyy4j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055207-71alyy4j/logs
wandb: Agent Starting Run: 6rd4qb8v with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055213-6rd4qb8v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-138
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6rd4qb8v
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x2x8). Calculated output size: (32x0x2). Output size is too small
wandb:                                                                                
wandb: 🚀 View run fiery-sweep-138 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6rd4qb8v
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055213-6rd4qb8v/logs
Run 6rd4qb8v errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x2x8). Calculated output size: (32x0x2). Output size is too small

wandb: ERROR Run 6rd4qb8v errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (32x2x8). Calculated output size: (32x0x2). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: quknaxbv with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055223-quknaxbv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-139
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/quknaxbv
wandb:                                                                                
wandb: 🚀 View run wobbly-sweep-139 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/quknaxbv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055223-quknaxbv/logs
wandb: Agent Starting Run: 39173whz with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055228-39173whz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-140
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/39173whz
wandb:                                                                                
wandb: 🚀 View run divine-sweep-140 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/39173whz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055228-39173whz/logs
wandb: Agent Starting Run: h921xxam with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055234-h921xxam
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-141
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h921xxam
wandb:                                                                                
wandb: 🚀 View run silvery-sweep-141 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h921xxam
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055234-h921xxam/logs
wandb: Agent Starting Run: 8x4umrui with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055239-8x4umrui
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-142
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8x4umrui
wandb:                                                                                
wandb: 🚀 View run fluent-sweep-142 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8x4umrui
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055239-8x4umrui/logs
wandb: Agent Starting Run: gyaro79u with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055245-gyaro79u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-143
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gyaro79u
wandb:                                                                                
wandb: 🚀 View run generous-sweep-143 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gyaro79u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055245-gyaro79u/logs
wandb: Agent Starting Run: uooilyzv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055250-uooilyzv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-144
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uooilyzv
wandb:                                                                                
wandb: 🚀 View run electric-sweep-144 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uooilyzv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055250-uooilyzv/logs
wandb: Agent Starting Run: wj6ar5mc with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_055256-wj6ar5mc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-145
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wj6ar5mc
wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:      train_loss nan
wandb:      train_rmse nan
wandb: validation_loss nan
wandb: validation_rmse nan
wandb: 
wandb: 🚀 View run likely-sweep-145 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wj6ar5mc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_055256-wj6ar5mc/logs
wandb: Agent Starting Run: 9tl169gb with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_060102-9tl169gb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-146
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9tl169gb
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run happy-sweep-146 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9tl169gb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_060102-9tl169gb/logs
wandb: Agent Starting Run: j0ebxc1a with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_060108-j0ebxc1a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-147
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j0ebxc1a
wandb:                                                                                
wandb: 🚀 View run zesty-sweep-147 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j0ebxc1a
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_060108-j0ebxc1a/logs
wandb: Agent Starting Run: 0a2nzmz3 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_060113-0a2nzmz3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-148
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0a2nzmz3
wandb:                                                                                
wandb: 🚀 View run fine-sweep-148 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0a2nzmz3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_060113-0a2nzmz3/logs
wandb: Agent Starting Run: ij6g28f1 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_060118-ij6g28f1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-149
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ij6g28f1
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▅▅▃▃▂▄▂▃▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▅▆▃▃▄▂▃▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 0.53797
wandb:      train_rmse 0.73346
wandb: validation_loss 7.3729
wandb: validation_rmse 2.71531
wandb: 
wandb: 🚀 View run logical-sweep-149 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ij6g28f1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_060118-ij6g28f1/logs
wandb: Agent Starting Run: vqe65rol with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_060939-vqe65rol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-150
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vqe65rol
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss ▅▄█▄▄▄▆▇▁▄
wandb:      train_rmse ▅▄█▄▄▄▆▇▁▄
wandb: validation_loss ▃█▇▁▇▅▆▂▄▃
wandb: validation_rmse ▃█▇▁▇▅▆▂▄▃
wandb: 
wandb: Run summary:
wandb:      train_loss 1015.51551
wandb:      train_rmse 31.86715
wandb: validation_loss 1035.44098
wandb: validation_rmse 32.17827
wandb: 
wandb: 🚀 View run iconic-sweep-150 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vqe65rol
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_060939-vqe65rol/logs
wandb: Agent Starting Run: s08e0jps with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_061123-s08e0jps
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-151
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s08e0jps
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▃▂▂▂▂▁▁▁▁
wandb:      train_rmse █▃▃▂▂▂▁▂▁▁
wandb: validation_loss ▁▄▇▇█▇▇▆▇█
wandb: validation_rmse ▁▄▇▇█▇▇▆▇█
wandb: 
wandb: Run summary:
wandb:      train_loss 242.57491
wandb:      train_rmse 15.57482
wandb: validation_loss 358.51036
wandb: validation_rmse 18.93437
wandb: 
wandb: 🚀 View run gallant-sweep-151 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s08e0jps
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_061123-s08e0jps/logs
wandb: Agent Starting Run: wxyfqlo6 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_061311-wxyfqlo6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-152
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wxyfqlo6
wandb:                                                                                
wandb: 🚀 View run lunar-sweep-152 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wxyfqlo6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_061311-wxyfqlo6/logs
wandb: Agent Starting Run: ikv3ealj with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_061317-ikv3ealj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-153
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ikv3ealj
wandb:                                                                                
wandb: 🚀 View run golden-sweep-153 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ikv3ealj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_061317-ikv3ealj/logs
wandb: Agent Starting Run: 4novfrh9 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_061322-4novfrh9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-154
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4novfrh9
wandb:                                                                                
wandb: 🚀 View run kind-sweep-154 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4novfrh9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_061322-4novfrh9/logs
wandb: Agent Starting Run: 2blnr6bs with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_061328-2blnr6bs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-155
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2blnr6bs
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fast-sweep-155 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2blnr6bs
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_061328-2blnr6bs/logs
wandb: Agent Starting Run: 2ufop5lb with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_061333-2ufop5lb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-156
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2ufop5lb
wandb:                                                                                
wandb: 🚀 View run sparkling-sweep-156 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2ufop5lb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_061333-2ufop5lb/logs
wandb: Agent Starting Run: ho4c5wrp with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_061338-ho4c5wrp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-157
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ho4c5wrp
wandb:                                                                                
wandb: 🚀 View run eager-sweep-157 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ho4c5wrp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_061338-ho4c5wrp/logs
wandb: Agent Starting Run: ey9gkt0y with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_061344-ey9gkt0y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-158
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ey9gkt0y
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 112.60107
wandb:      train_rmse 10.61137
wandb: validation_loss 23.25594
wandb: validation_rmse 4.82244
wandb: 
wandb: 🚀 View run confused-sweep-158 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ey9gkt0y
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_061344-ey9gkt0y/logs
wandb: Agent Starting Run: x7ii25dg with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062326-x7ii25dg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-159
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x7ii25dg
wandb:                                                                                
wandb: 🚀 View run rare-sweep-159 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x7ii25dg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062326-x7ii25dg/logs
wandb: Agent Starting Run: m43cketj with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062331-m43cketj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-160
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m43cketj
wandb:                                                                                
wandb: 🚀 View run trim-sweep-160 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m43cketj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062331-m43cketj/logs
wandb: Agent Starting Run: 2fzkddjm with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062337-2fzkddjm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-161
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2fzkddjm
wandb:                                                                                
wandb: 🚀 View run stoic-sweep-161 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2fzkddjm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062337-2fzkddjm/logs
wandb: Agent Starting Run: 6j390bgk with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062342-6j390bgk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-162
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6j390bgk
wandb:                                                                                
wandb: 🚀 View run leafy-sweep-162 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6j390bgk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062342-6j390bgk/logs
wandb: Agent Starting Run: 1kjqq8fl with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062347-1kjqq8fl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-163
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1kjqq8fl
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-163 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1kjqq8fl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062347-1kjqq8fl/logs
wandb: Agent Starting Run: 3t2thu7m with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062353-3t2thu7m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-164
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3t2thu7m
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▇▆▅▄▄▃▂▂▁
wandb: validation_rmse █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 723.17423
wandb:      train_rmse 26.8919
wandb: validation_loss 705.51001
wandb: validation_rmse 26.56144
wandb: 
wandb: 🚀 View run balmy-sweep-164 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3t2thu7m
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062353-3t2thu7m/logs
wandb: Agent Starting Run: 33z84oi0 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062541-33z84oi0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-165
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/33z84oi0
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-165 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/33z84oi0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062541-33z84oi0/logs
wandb: Agent Starting Run: 6g780h3u with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062546-6g780h3u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-166
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6g780h3u
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▅▄▃▃▂▁▁
wandb:      train_rmse █▇▇▆▅▄▃▃▂▁
wandb: validation_loss █▇▆▅▄▄▃▂▁▁
wandb: validation_rmse █▇▇▅▅▄▃▃▁▂
wandb: 
wandb: Run summary:
wandb:      train_loss 96.97922
wandb:      train_rmse 9.8478
wandb: validation_loss 110.8898
wandb: validation_rmse 10.53042
wandb: 
wandb: 🚀 View run glowing-sweep-166 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6g780h3u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062546-6g780h3u/logs
wandb: Agent Starting Run: 5q3rtah3 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062756-5q3rtah3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-167
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5q3rtah3
wandb:                                                                                
wandb: 🚀 View run glamorous-sweep-167 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5q3rtah3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062756-5q3rtah3/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qroczglj with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062811-qroczglj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-168
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qroczglj
wandb:                                                                                
wandb: 🚀 View run gentle-sweep-168 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qroczglj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062811-qroczglj/logs
wandb: Agent Starting Run: kx84eltx with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062816-kx84eltx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-169
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kx84eltx
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (64x2x54). Calculated output size: (64x0x13). Output size is too small
wandb:                                                                                
wandb: 🚀 View run royal-sweep-169 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kx84eltx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062816-kx84eltx/logs
Run kx84eltx errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (64x2x54). Calculated output size: (64x0x13). Output size is too small

wandb: ERROR Run kx84eltx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (64x2x54). Calculated output size: (64x0x13). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: bvqwhqjz with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062822-bvqwhqjz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-170
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bvqwhqjz
wandb:                                                                                
wandb: 🚀 View run charmed-sweep-170 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bvqwhqjz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062822-bvqwhqjz/logs
wandb: Agent Starting Run: l5hwfpai with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062828-l5hwfpai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-171
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l5hwfpai
wandb:                                                                                
wandb: 🚀 View run smart-sweep-171 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l5hwfpai
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062828-l5hwfpai/logs
wandb: Agent Starting Run: 1dkbtnu5 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062833-1dkbtnu5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-172
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1dkbtnu5
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run wise-sweep-172 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1dkbtnu5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062833-1dkbtnu5/logs
wandb: Agent Starting Run: u2wi5ywp with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062838-u2wi5ywp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-173
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u2wi5ywp
wandb:                                                                                
wandb: 🚀 View run fluent-sweep-173 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u2wi5ywp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062838-u2wi5ywp/logs
wandb: Agent Starting Run: vjn4iuut with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062844-vjn4iuut
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-174
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vjn4iuut
wandb:                                                                                
wandb: 🚀 View run autumn-sweep-174 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vjn4iuut
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062844-vjn4iuut/logs
wandb: Agent Starting Run: kgycpxxf with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062849-kgycpxxf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-175
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kgycpxxf
wandb:                                                                                
wandb: 🚀 View run super-sweep-175 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kgycpxxf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062849-kgycpxxf/logs
wandb: Agent Starting Run: rlenihjf with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_062854-rlenihjf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-176
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rlenihjf
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss ██▇▇▇▆▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse ███▇▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 14.57408
wandb:      train_rmse 3.8176
wandb: validation_loss 11.2122
wandb: validation_rmse 3.34846
wandb: 
wandb: 🚀 View run wise-sweep-176 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rlenihjf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062854-rlenihjf/logs
wandb: Agent Starting Run: e0xwdyk2 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063707-e0xwdyk2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-177
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e0xwdyk2
wandb:                                                                                
wandb: 🚀 View run splendid-sweep-177 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e0xwdyk2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063707-e0xwdyk2/logs
wandb: Agent Starting Run: 188ukjyk with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063713-188ukjyk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-178
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/188ukjyk
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run jolly-sweep-178 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/188ukjyk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063713-188ukjyk/logs
wandb: Agent Starting Run: rbe0cc1h with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063718-rbe0cc1h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-179
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rbe0cc1h
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-179 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rbe0cc1h
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063718-rbe0cc1h/logs
wandb: Agent Starting Run: zneo549e with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063723-zneo549e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-180
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zneo549e
wandb:                                                                                
wandb: 🚀 View run drawn-sweep-180 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zneo549e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063723-zneo549e/logs
wandb: Agent Starting Run: whaf1vzg with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063729-whaf1vzg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-181
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/whaf1vzg
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run hearty-sweep-181 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/whaf1vzg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063729-whaf1vzg/logs
Run whaf1vzg errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run whaf1vzg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: 5i8q42eo with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063734-5i8q42eo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-182
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5i8q42eo
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fearless-sweep-182 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5i8q42eo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063734-5i8q42eo/logs
wandb: Agent Starting Run: e2010urm with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063740-e2010urm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-183
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e2010urm
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▂▁▁▁▁▁▁▁
wandb:      train_rmse █▄▂▁▁▁▁▁▁▁
wandb: validation_loss █▄▂▂▁▁▁▁▁▁
wandb: validation_rmse █▅▃▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 69.59584
wandb:      train_rmse 8.34241
wandb: validation_loss 90.72998
wandb: validation_rmse 9.52523
wandb: 
wandb: 🚀 View run smart-sweep-183 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e2010urm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063740-e2010urm/logs
wandb: Agent Starting Run: zrj7qavr with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063923-zrj7qavr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-184
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zrj7qavr
wandb:                                                                                
wandb: 🚀 View run fallen-sweep-184 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zrj7qavr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063923-zrj7qavr/logs
wandb: Agent Starting Run: axtabwfe with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063928-axtabwfe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-185
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/axtabwfe
wandb:                                                                                
wandb: 🚀 View run bumbling-sweep-185 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/axtabwfe
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063928-axtabwfe/logs
wandb: Agent Starting Run: cvcf8fcy with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063934-cvcf8fcy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-186
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cvcf8fcy
wandb:                                                                                
wandb: 🚀 View run electric-sweep-186 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cvcf8fcy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063934-cvcf8fcy/logs
wandb: Agent Starting Run: w9p9x2pa with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063939-w9p9x2pa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-187
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w9p9x2pa
wandb:                                                                                
wandb: 🚀 View run fresh-sweep-187 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w9p9x2pa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063939-w9p9x2pa/logs
wandb: Agent Starting Run: zxi1j0ui with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063944-zxi1j0ui
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-188
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zxi1j0ui
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run logical-sweep-188 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zxi1j0ui
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063944-zxi1j0ui/logs
wandb: Agent Starting Run: 9ajjbr2c with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063949-9ajjbr2c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-189
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9ajjbr2c
wandb:                                                                                
wandb: 🚀 View run worldly-sweep-189 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9ajjbr2c
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063949-9ajjbr2c/logs
wandb: Agent Starting Run: 19p8x2ha with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_063955-19p8x2ha
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-190
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/19p8x2ha
wandb:                                                                                
wandb: 🚀 View run daily-sweep-190 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/19p8x2ha
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_063955-19p8x2ha/logs
wandb: Agent Starting Run: gv0847j0 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064000-gv0847j0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-191
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gv0847j0
wandb:                                                                                
wandb: 🚀 View run summer-sweep-191 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gv0847j0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064000-gv0847j0/logs
wandb: Agent Starting Run: b24z2flx with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064006-b24z2flx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-192
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b24z2flx
wandb:                                                                                
wandb: 🚀 View run zesty-sweep-192 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b24z2flx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064006-b24z2flx/logs
wandb: Agent Starting Run: 7f37n9tt with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064011-7f37n9tt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-193
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7f37n9tt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▅▄▃▃▂▂▁▁
wandb:      train_rmse █▇▆▅▄▃▃▂▁▁
wandb: validation_loss █▇▆▅▄▄▃▂▁▁
wandb: validation_rmse █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 97.41943
wandb:      train_rmse 9.87013
wandb: validation_loss 665.37299
wandb: validation_rmse 25.79482
wandb: 
wandb: 🚀 View run hopeful-sweep-193 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7f37n9tt
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064011-7f37n9tt/logs
wandb: Agent Starting Run: t4r34of2 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064155-t4r34of2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-194
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t4r34of2
wandb:                                                                                
wandb: 🚀 View run rose-sweep-194 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t4r34of2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064155-t4r34of2/logs
wandb: Agent Starting Run: gvmwhu5k with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064200-gvmwhu5k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-195
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gvmwhu5k
wandb:                                                                                
wandb: 🚀 View run major-sweep-195 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gvmwhu5k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064200-gvmwhu5k/logs
wandb: Agent Starting Run: gk8gnyys with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064206-gk8gnyys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-196
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gk8gnyys
wandb:                                                                                
wandb: 🚀 View run solar-sweep-196 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gk8gnyys
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064206-gk8gnyys/logs
wandb: Agent Starting Run: uhijsb5e with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064211-uhijsb5e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-197
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uhijsb5e
wandb:                                                                                
wandb: 🚀 View run comic-sweep-197 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uhijsb5e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064211-uhijsb5e/logs
wandb: Agent Starting Run: 92b21303 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064216-92b21303
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-198
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/92b21303
wandb:                                                                                
wandb: 🚀 View run brisk-sweep-198 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/92b21303
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064216-92b21303/logs
wandb: Agent Starting Run: xtvpw7yp with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064222-xtvpw7yp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-199
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xtvpw7yp
wandb:                                                                                
wandb: 🚀 View run noble-sweep-199 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xtvpw7yp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064222-xtvpw7yp/logs
wandb: Agent Starting Run: u5f0zzmn with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064227-u5f0zzmn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-200
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u5f0zzmn
wandb:                                                                                
wandb: 🚀 View run twilight-sweep-200 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u5f0zzmn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064227-u5f0zzmn/logs
wandb: Agent Starting Run: 02nzf9wa with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064232-02nzf9wa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-201
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/02nzf9wa
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run azure-sweep-201 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/02nzf9wa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064232-02nzf9wa/logs
Run 02nzf9wa errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 02nzf9wa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: klnd42ld with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064238-klnd42ld
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-202
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/klnd42ld
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-202 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/klnd42ld
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064238-klnd42ld/logs
wandb: Agent Starting Run: xpfxlb3l with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064243-xpfxlb3l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-203
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xpfxlb3l
wandb:                                                                                
wandb: 🚀 View run good-sweep-203 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xpfxlb3l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064243-xpfxlb3l/logs
wandb: Agent Starting Run: mbf97smh with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064248-mbf97smh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-204
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mbf97smh
wandb:                                                                                
wandb: 🚀 View run proud-sweep-204 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mbf97smh
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064248-mbf97smh/logs
wandb: Agent Starting Run: bg0vqa7e with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064253-bg0vqa7e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-205
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bg0vqa7e
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▁▁▁▁▁▁▁▁▁
wandb: validation_loss ▃█▃▃▁▅▂▇▃▁
wandb: validation_rmse ▃█▃▃▁▅▂▇▃▁
wandb: 
wandb: Run summary:
wandb:      train_loss 67.97362
wandb:      train_rmse 8.24461
wandb: validation_loss 86.1335
wandb: validation_rmse 9.28081
wandb: 
wandb: 🚀 View run copper-sweep-205 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bg0vqa7e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064253-bg0vqa7e/logs
wandb: Agent Starting Run: ppgme0ku with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064442-ppgme0ku
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-206
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ppgme0ku
wandb:                                                                                
wandb: 🚀 View run sparkling-sweep-206 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ppgme0ku
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064442-ppgme0ku/logs
wandb: Agent Starting Run: 3ilxh4ql with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064447-3ilxh4ql
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-207
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3ilxh4ql
wandb:                                                                                
wandb: 🚀 View run polar-sweep-207 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3ilxh4ql
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064447-3ilxh4ql/logs
wandb: Agent Starting Run: gak0axiq with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064453-gak0axiq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-208
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gak0axiq
wandb:                                                                                
wandb: 🚀 View run copper-sweep-208 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gak0axiq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064453-gak0axiq/logs
wandb: Agent Starting Run: pq0lpsbm with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064458-pq0lpsbm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-209
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pq0lpsbm
wandb:                                                                                
wandb: 🚀 View run hopeful-sweep-209 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pq0lpsbm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064458-pq0lpsbm/logs
wandb: Agent Starting Run: ifsrrgux with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064504-ifsrrgux
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-210
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ifsrrgux
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x2x28). Calculated output size: (32x0x7). Output size is too small
wandb:                                                                                
wandb: 🚀 View run lunar-sweep-210 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ifsrrgux
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064504-ifsrrgux/logs
Run ifsrrgux errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x2x28). Calculated output size: (32x0x7). Output size is too small

wandb: ERROR Run ifsrrgux errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (32x2x28). Calculated output size: (32x0x7). Output size is too small
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: eot8uzeb with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064519-eot8uzeb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-211
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eot8uzeb
wandb:                                                                                
wandb: 🚀 View run expert-sweep-211 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eot8uzeb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064519-eot8uzeb/logs
wandb: Agent Starting Run: oprm8kcj with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064525-oprm8kcj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-212
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oprm8kcj
wandb:                                                                                
wandb: 🚀 View run brisk-sweep-212 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oprm8kcj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064525-oprm8kcj/logs
wandb: Agent Starting Run: m38ejpix with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064530-m38ejpix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-213
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m38ejpix
wandb:                                                                                
wandb: 🚀 View run driven-sweep-213 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m38ejpix
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064530-m38ejpix/logs
wandb: Agent Starting Run: b3ttvj4z with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064535-b3ttvj4z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-214
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b3ttvj4z
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▅▅▅▅▄▃▁▂▁
wandb:      train_rmse █▅▅▅▅▄▃▁▂▁
wandb: validation_loss █▃▂▃▄▂▁▁▅▂
wandb: validation_rmse █▃▂▃▄▂▁▁▅▂
wandb: 
wandb: Run summary:
wandb:      train_loss 972.80191
wandb:      train_rmse 31.18977
wandb: validation_loss 959.72827
wandb: validation_rmse 30.97948
wandb: 
wandb: 🚀 View run jolly-sweep-214 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b3ttvj4z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064535-b3ttvj4z/logs
wandb: Agent Starting Run: 0eyx3exr with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064719-0eyx3exr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-215
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0eyx3exr
wandb:                                                                                
wandb: 🚀 View run pretty-sweep-215 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0eyx3exr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064719-0eyx3exr/logs
wandb: Agent Starting Run: 45e648y9 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064724-45e648y9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-216
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/45e648y9
wandb:                                                                                
wandb: 🚀 View run sandy-sweep-216 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/45e648y9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064724-45e648y9/logs
wandb: Agent Starting Run: l3z4pt27 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064730-l3z4pt27
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-217
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l3z4pt27
wandb:                                                                                
wandb: 🚀 View run peach-sweep-217 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l3z4pt27
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064730-l3z4pt27/logs
wandb: Agent Starting Run: dsv4brrn with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064735-dsv4brrn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-218
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dsv4brrn
wandb:                                                                                
wandb: 🚀 View run dandy-sweep-218 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dsv4brrn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064735-dsv4brrn/logs
wandb: Agent Starting Run: q6hry1vn with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064740-q6hry1vn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-219
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q6hry1vn
wandb:                                                                                
wandb: 🚀 View run grateful-sweep-219 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q6hry1vn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064740-q6hry1vn/logs
wandb: Agent Starting Run: vidt8b0n with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064746-vidt8b0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-220
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vidt8b0n
wandb:                                                                                
wandb: 🚀 View run glorious-sweep-220 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vidt8b0n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064746-vidt8b0n/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9504w6lf with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064810-9504w6lf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-221
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9504w6lf
wandb:                                                                                
wandb: 🚀 View run glowing-sweep-221 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9504w6lf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064810-9504w6lf/logs
wandb: Agent Starting Run: rscl6giw with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064817-rscl6giw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-222
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rscl6giw
wandb:                                                                                
wandb: 🚀 View run honest-sweep-222 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rscl6giw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064817-rscl6giw/logs
wandb: Agent Starting Run: jsh6fyya with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064822-jsh6fyya
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-223
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jsh6fyya
wandb:                                                                                
wandb: 🚀 View run rare-sweep-223 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jsh6fyya
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064822-jsh6fyya/logs
wandb: Agent Starting Run: bnwrt2m7 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064828-bnwrt2m7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-224
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bnwrt2m7
wandb:                                                                                
wandb: 🚀 View run wild-sweep-224 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bnwrt2m7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064828-bnwrt2m7/logs
wandb: Agent Starting Run: 7ahyzxnq with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064833-7ahyzxnq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-225
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7ahyzxnq
wandb:                                                                                
wandb: 🚀 View run breezy-sweep-225 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7ahyzxnq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064833-7ahyzxnq/logs
wandb: Agent Starting Run: wyfnwbgn with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064839-wyfnwbgn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-226
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wyfnwbgn
wandb:                                                                                
wandb: 🚀 View run balmy-sweep-226 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wyfnwbgn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064839-wyfnwbgn/logs
wandb: Agent Starting Run: occfui09 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_064844-occfui09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-227
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/occfui09
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▅▅▆▆▆▄▅▂▄▄▃▂▃▆▂▂▂▃▁▂▁▁▂▄▂▅▁▂▁▂▃▂▁▂▁▂▂▁▁
wandb: validation_rmse █▆▆▆▆▇▅▆▅▃▂▃▆▂▂▃▂▄▂▄▁▁▂▄▄▆▁▂▁▂▄▂▁▃▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 18.6551
wandb:      train_rmse 4.31916
wandb: validation_loss 6.53527
wandb: validation_rmse 2.55642
wandb: 
wandb: 🚀 View run sparkling-sweep-227 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/occfui09
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_064844-occfui09/logs
wandb: Agent Starting Run: 5q0ckzrz with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065650-5q0ckzrz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-228
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5q0ckzrz
wandb:                                                                                
wandb: 🚀 View run happy-sweep-228 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5q0ckzrz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065650-5q0ckzrz/logs
wandb: Agent Starting Run: nbql1lmq with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065655-nbql1lmq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-229
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nbql1lmq
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -896: [1, -896]
wandb:                                                                                
wandb: 🚀 View run earthy-sweep-229 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nbql1lmq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065655-nbql1lmq/logs
Run nbql1lmq errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -896: [1, -896]

wandb: ERROR Run nbql1lmq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -896: [1, -896]
wandb: ERROR 
wandb: Agent Starting Run: yzdik5dt with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065700-yzdik5dt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-230
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yzdik5dt
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run royal-sweep-230 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yzdik5dt
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065700-yzdik5dt/logs
wandb: Agent Starting Run: 6g68yn66 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065706-6g68yn66
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-231
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6g68yn66
wandb:                                                                                
wandb: 🚀 View run vibrant-sweep-231 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6g68yn66
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065706-6g68yn66/logs
wandb: Agent Starting Run: gums8phh with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065711-gums8phh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-232
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gums8phh
wandb: - 0.000 MB of 0.000 MB uploadedwandb: \ 0.000 MB of 0.000 MB uploadedwandb: | 0.000 MB of 0.000 MB uploadedwandb: / 0.000 MB of 0.000 MB uploadedwandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 🚀 View run rural-sweep-232 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gums8phh
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065711-gums8phh/logs
wandb: Agent Starting Run: e10l4f9k with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065721-e10l4f9k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-233
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e10l4f9k
wandb:                                                                                
wandb: 🚀 View run gentle-sweep-233 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e10l4f9k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065721-e10l4f9k/logs
wandb: Agent Starting Run: yov1i2vq with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065727-yov1i2vq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-234
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yov1i2vq
wandb:                                                                                
wandb: 🚀 View run solar-sweep-234 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yov1i2vq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065727-yov1i2vq/logs
wandb: Agent Starting Run: 0hkb1p0t with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065732-0hkb1p0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-235
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0hkb1p0t
wandb:                                                                                
wandb: 🚀 View run usual-sweep-235 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0hkb1p0t
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065732-0hkb1p0t/logs
wandb: Agent Starting Run: pqhn6vca with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065737-pqhn6vca
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-236
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pqhn6vca
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▅▅▃▃▃▂▂▁▁
wandb:      train_rmse █▅▅▃▃▃▂▂▁▁
wandb: validation_loss ▃█▂▃▄▃▄▄▃▁
wandb: validation_rmse ▃█▂▃▄▃▄▄▃▁
wandb: 
wandb: Run summary:
wandb:      train_loss 901.85344
wandb:      train_rmse 30.03087
wandb: validation_loss 869.78397
wandb: validation_rmse 29.4921
wandb: 
wandb: 🚀 View run royal-sweep-236 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pqhn6vca
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065737-pqhn6vca/logs
wandb: Agent Starting Run: xemoohai with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065942-xemoohai
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-237
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xemoohai
wandb:                                                                                
wandb: 🚀 View run solar-sweep-237 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xemoohai
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065942-xemoohai/logs
wandb: Agent Starting Run: c9kagrvw with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065947-c9kagrvw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-238
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c9kagrvw
wandb:                                                                                
wandb: 🚀 View run devout-sweep-238 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c9kagrvw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065947-c9kagrvw/logs
wandb: Agent Starting Run: 6zgrdtcg with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065952-6zgrdtcg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-239
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6zgrdtcg
wandb:                                                                                
wandb: 🚀 View run snowy-sweep-239 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6zgrdtcg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065952-6zgrdtcg/logs
wandb: Agent Starting Run: goaeep56 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_065958-goaeep56
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-240
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/goaeep56
wandb:                                                                                
wandb: 🚀 View run effortless-sweep-240 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/goaeep56
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_065958-goaeep56/logs
wandb: Agent Starting Run: udehp0p1 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070003-udehp0p1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-241
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/udehp0p1
wandb:                                                                                
wandb: 🚀 View run wild-sweep-241 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/udehp0p1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070003-udehp0p1/logs
wandb: Agent Starting Run: 220ub0it with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070008-220ub0it
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-242
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/220ub0it
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run expert-sweep-242 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/220ub0it
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070008-220ub0it/logs
wandb: Agent Starting Run: 6c6cfnkq with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070014-6c6cfnkq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-243
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6c6cfnkq
wandb:                                                                                
wandb: 🚀 View run solar-sweep-243 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6c6cfnkq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070014-6c6cfnkq/logs
wandb: Agent Starting Run: e5i75ciz with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070020-e5i75ciz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-244
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e5i75ciz
wandb:                                                                                
wandb: 🚀 View run confused-sweep-244 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e5i75ciz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070020-e5i75ciz/logs
wandb: Agent Starting Run: ogtnhdge with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070025-ogtnhdge
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-245
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ogtnhdge
wandb:                                                                                
wandb: 🚀 View run copper-sweep-245 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ogtnhdge
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070025-ogtnhdge/logs
wandb: Agent Starting Run: 1956bir8 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070030-1956bir8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-246
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1956bir8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▄▄▃▄▂▂▂▂▄▂▂▅▃▃▄▃▂▂▂▄▁▂▂▄▂▂▃▂▁▁▁▂▂▁▂▂▁▃▂
wandb: validation_rmse █▅▄▄▄▄▃▃▃▃▃▂▂▅▃▂▅▄▂▃▄▁▃▂▂▂▃▃▂▁▁▂▃▁▃▃▁▄▃▄
wandb: 
wandb: Run summary:
wandb:      train_loss 2.01699
wandb:      train_rmse 1.42021
wandb: validation_loss 13.37218
wandb: validation_rmse 3.6568
wandb: 
wandb: 🚀 View run happy-sweep-246 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1956bir8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070030-1956bir8/logs
wandb: Agent Starting Run: f694kxh2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070841-f694kxh2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-247
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f694kxh2
wandb:                                                                                
wandb: 🚀 View run copper-sweep-247 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f694kxh2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070841-f694kxh2/logs
wandb: Agent Starting Run: 4ibgx8ym with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070846-4ibgx8ym
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-248
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4ibgx8ym
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.41 GiB. GPU 0 has a total capacity of 39.50 GiB of which 9.34 GiB is free. Including non-PyTorch memory, this process has 30.15 GiB memory in use. Of the allocated memory 18.36 GiB is allocated by PyTorch, and 11.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run pious-sweep-248 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4ibgx8ym
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070846-4ibgx8ym/logs
Run 4ibgx8ym errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.41 GiB. GPU 0 has a total capacity of 39.50 GiB of which 9.34 GiB is free. Including non-PyTorch memory, this process has 30.15 GiB memory in use. Of the allocated memory 18.36 GiB is allocated by PyTorch, and 11.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4ibgx8ym errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.41 GiB. GPU 0 has a total capacity of 39.50 GiB of which 9.34 GiB is free. Including non-PyTorch memory, this process has 30.15 GiB memory in use. Of the allocated memory 18.36 GiB is allocated by PyTorch, and 11.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gaayf01a with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_070902-gaayf01a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-249
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gaayf01a
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▅▅▄▃▂▂▁
wandb:      train_rmse █▇▆▆▅▄▃▃▂▁
wandb: validation_loss ██▆▆▄▄▃▃▂▁
wandb: validation_rmse ██▆▆▅▄▃▃▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 611.0834
wandb:      train_rmse 24.7201
wandb: validation_loss 624.97784
wandb: validation_rmse 24.99956
wandb: 
wandb: 🚀 View run apricot-sweep-249 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gaayf01a
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_070902-gaayf01a/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ytck0plo with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071055-ytck0plo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-250
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ytck0plo
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-250 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ytck0plo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071055-ytck0plo/logs
wandb: Agent Starting Run: 9dtanr0g with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071101-9dtanr0g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-251
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9dtanr0g
wandb:                                                                                
wandb: 🚀 View run avid-sweep-251 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9dtanr0g
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071101-9dtanr0g/logs
wandb: Agent Starting Run: w8slcm7v with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071107-w8slcm7v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-252
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w8slcm7v
wandb:                                                                                
wandb: 🚀 View run dry-sweep-252 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w8slcm7v
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071107-w8slcm7v/logs
wandb: Agent Starting Run: 4b503af1 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071112-4b503af1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-253
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4b503af1
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss ▇██▅▅▃▂▂▁▁
wandb:      train_rmse ▇██▅▅▃▂▂▁▁
wandb: validation_loss ██▇▆▅▄▃▃▂▁
wandb: validation_rmse ██▇▆▆▄▃▃▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 914.59303
wandb:      train_rmse 30.24224
wandb: validation_loss 902.96271
wandb: validation_rmse 30.04934
wandb: 
wandb: 🚀 View run light-sweep-253 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4b503af1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071112-4b503af1/logs
wandb: Agent Starting Run: keqoz5tp with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071301-keqoz5tp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-254
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/keqoz5tp
wandb:                                                                                
wandb: 🚀 View run zesty-sweep-254 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/keqoz5tp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071301-keqoz5tp/logs
wandb: Agent Starting Run: 4pc7slip with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071306-4pc7slip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-255
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4pc7slip
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▁▁▁▁▁▁▁▁
wandb:      train_rmse █▂▁▂▁▁▁▁▁▁
wandb: validation_loss ▃█▆▄▆▃▁▂▂▁
wandb: validation_rmse ▃█▇▄▆▃▁▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 121.16135
wandb:      train_rmse 11.00733
wandb: validation_loss 68.48232
wandb: validation_rmse 8.2754
wandb: 
wandb: 🚀 View run kind-sweep-255 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4pc7slip
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071306-4pc7slip/logs
wandb: Agent Starting Run: zin6ytv0 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071449-zin6ytv0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-256
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zin6ytv0
wandb:                                                                                
wandb: 🚀 View run polar-sweep-256 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zin6ytv0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071449-zin6ytv0/logs
wandb: Agent Starting Run: wmvr10bl with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071454-wmvr10bl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-257
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wmvr10bl
wandb:                                                                                
wandb: 🚀 View run helpful-sweep-257 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wmvr10bl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071454-wmvr10bl/logs
wandb: Agent Starting Run: dm77h3x9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071500-dm77h3x9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-258
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dm77h3x9
wandb:                                                                                
wandb: 🚀 View run leafy-sweep-258 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dm77h3x9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071500-dm77h3x9/logs
wandb: Agent Starting Run: ipl8th2f with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071505-ipl8th2f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-259
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ipl8th2f
wandb:                                                                                
wandb: 🚀 View run bumbling-sweep-259 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ipl8th2f
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071505-ipl8th2f/logs
wandb: Agent Starting Run: rkgmu3xf with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071510-rkgmu3xf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-260
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rkgmu3xf
wandb:                                                                                
wandb: 🚀 View run cerulean-sweep-260 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rkgmu3xf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071510-rkgmu3xf/logs
wandb: Agent Starting Run: u33bkppk with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071516-u33bkppk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-261
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u33bkppk
wandb:                                                                                
wandb: 🚀 View run resilient-sweep-261 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u33bkppk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071516-u33bkppk/logs
wandb: Agent Starting Run: g45pswop with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071521-g45pswop
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-262
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g45pswop
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run lilac-sweep-262 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g45pswop
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071521-g45pswop/logs
wandb: Agent Starting Run: g588t7e5 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071527-g588t7e5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-263
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g588t7e5
wandb:                                                                                
wandb: 🚀 View run dazzling-sweep-263 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g588t7e5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071527-g588t7e5/logs
wandb: Agent Starting Run: wj4z7xop with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071532-wj4z7xop
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-264
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wj4z7xop
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -64: [1, -64]
wandb:                                                                                
wandb: 🚀 View run earthy-sweep-264 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wj4z7xop
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071532-wj4z7xop/logs
Run wj4z7xop errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -64: [1, -64]

wandb: ERROR Run wj4z7xop errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -64: [1, -64]
wandb: ERROR 
wandb: Agent Starting Run: cuhcvg3s with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071537-cuhcvg3s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-265
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cuhcvg3s
wandb:                                                                                
wandb: 🚀 View run decent-sweep-265 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cuhcvg3s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071537-cuhcvg3s/logs
wandb: Agent Starting Run: oaordhnw with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071543-oaordhnw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-266
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oaordhnw
wandb:                                                                                
wandb: 🚀 View run smooth-sweep-266 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oaordhnw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071543-oaordhnw/logs
wandb: Agent Starting Run: wgipbsl4 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071548-wgipbsl4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-267
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wgipbsl4
wandb:                                                                                
wandb: 🚀 View run prime-sweep-267 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wgipbsl4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071548-wgipbsl4/logs
wandb: Agent Starting Run: wth86a6e with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071553-wth86a6e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-268
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wth86a6e
wandb:                                                                                
wandb: 🚀 View run quiet-sweep-268 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wth86a6e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071553-wth86a6e/logs
wandb: Agent Starting Run: k5lt0m0l with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071559-k5lt0m0l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-269
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k5lt0m0l
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run comfy-sweep-269 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k5lt0m0l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071559-k5lt0m0l/logs
Run k5lt0m0l errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run k5lt0m0l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: yfx8fvs6 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071604-yfx8fvs6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-270
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yfx8fvs6
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -256: [64, -256]
wandb:                                                                                
wandb: 🚀 View run dry-sweep-270 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yfx8fvs6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071604-yfx8fvs6/logs
Run yfx8fvs6 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -256: [64, -256]

wandb: ERROR Run yfx8fvs6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -256: [64, -256]
wandb: ERROR 
wandb: Agent Starting Run: hu8ssi9i with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071609-hu8ssi9i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-271
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hu8ssi9i
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▃▂▂▁▁▁▁
wandb:      train_rmse █▄▃▃▂▂▂▁▁▁
wandb: validation_loss █▇▆▅▄▃▃▂▂▁
wandb: validation_rmse █▇▆▅▄▃▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 672.52226
wandb:      train_rmse 25.93303
wandb: validation_loss 666.06735
wandb: validation_rmse 25.80828
wandb: 
wandb: 🚀 View run divine-sweep-271 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hu8ssi9i
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071609-hu8ssi9i/logs
wandb: Agent Starting Run: 5cdijorm with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071753-5cdijorm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-272
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5cdijorm
wandb:                                                                                
wandb: 🚀 View run restful-sweep-272 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5cdijorm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071753-5cdijorm/logs
wandb: Agent Starting Run: z8mn7441 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071758-z8mn7441
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-273
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z8mn7441
wandb:                                                                                
wandb: 🚀 View run stilted-sweep-273 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z8mn7441
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071758-z8mn7441/logs
wandb: Agent Starting Run: 2aiysmcv with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_071804-2aiysmcv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-274
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2aiysmcv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▇▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▇▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 10.66373
wandb:      train_rmse 3.26554
wandb: validation_loss 11.54
wandb: validation_rmse 3.39706
wandb: 
wandb: 🚀 View run atomic-sweep-274 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2aiysmcv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_071804-2aiysmcv/logs
wandb: Agent Starting Run: pycowin4 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_072614-pycowin4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-275
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pycowin4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▅▅▄▃▂▂▁
wandb:      train_rmse █▇▆▅▅▄▃▂▂▁
wandb: validation_loss ▁▆▅█▇▇█▄▄▂
wandb: validation_rmse ▁▆▅█▇▇█▄▅▂
wandb: 
wandb: Run summary:
wandb:      train_loss 808.79217
wandb:      train_rmse 28.43927
wandb: validation_loss 1102.11035
wandb: validation_rmse 33.19805
wandb: 
wandb: 🚀 View run fragrant-sweep-275 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pycowin4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_072614-pycowin4/logs
wandb: Agent Starting Run: gf9dl6en with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_072758-gf9dl6en
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-276
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gf9dl6en
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▁▂▁▁▁▁▁
wandb:      train_rmse █▂▂▂▂▂▁▁▁▁
wandb: validation_loss █▂▃▁▂▂▂▁▁▂
wandb: validation_rmse █▂▃▂▂▂▂▂▁▂
wandb: 
wandb: Run summary:
wandb:      train_loss 174.93859
wandb:      train_rmse 13.22644
wandb: validation_loss 334.24123
wandb: validation_rmse 18.28227
wandb: 
wandb: 🚀 View run amber-sweep-276 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gf9dl6en
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_072758-gf9dl6en/logs
wandb: Agent Starting Run: w4e4va9k with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_072941-w4e4va9k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-277
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w4e4va9k
wandb:                                                                                
wandb: 🚀 View run azure-sweep-277 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w4e4va9k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_072941-w4e4va9k/logs
wandb: Agent Starting Run: ld8ozq1d with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_072946-ld8ozq1d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-278
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ld8ozq1d
wandb:                                                                                
wandb: 🚀 View run glamorous-sweep-278 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ld8ozq1d
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_072946-ld8ozq1d/logs
wandb: Agent Starting Run: d0odhvg5 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_072952-d0odhvg5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-279
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d0odhvg5
wandb:                                                                                
wandb: 🚀 View run dulcet-sweep-279 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d0odhvg5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_072952-d0odhvg5/logs
wandb: Agent Starting Run: 4uhxtu1c with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_072957-4uhxtu1c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-280
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4uhxtu1c
wandb:                                                                                
wandb: 🚀 View run celestial-sweep-280 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4uhxtu1c
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_072957-4uhxtu1c/logs
wandb: Agent Starting Run: stmn96cg with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073002-stmn96cg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-281
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/stmn96cg
wandb:                                                                                
wandb: 🚀 View run playful-sweep-281 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/stmn96cg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073002-stmn96cg/logs
wandb: Agent Starting Run: b0sibc0f with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073008-b0sibc0f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-282
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b0sibc0f
wandb:                                                                                
wandb: 🚀 View run jolly-sweep-282 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b0sibc0f
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073008-b0sibc0f/logs
wandb: Agent Starting Run: 94oj25qa with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073013-94oj25qa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-283
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/94oj25qa
wandb:                                                                                
wandb: 🚀 View run light-sweep-283 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/94oj25qa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073013-94oj25qa/logs
wandb: Agent Starting Run: mfra19h6 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073019-mfra19h6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-284
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mfra19h6
wandb:                                                                                
wandb: 🚀 View run vague-sweep-284 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mfra19h6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073019-mfra19h6/logs
wandb: Agent Starting Run: r6k6rd0b with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073024-r6k6rd0b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-285
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r6k6rd0b
wandb:                                                                                
wandb: 🚀 View run honest-sweep-285 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r6k6rd0b
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073024-r6k6rd0b/logs
wandb: Agent Starting Run: l7als7hg with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073029-l7als7hg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-286
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l7als7hg
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run rare-sweep-286 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l7als7hg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073029-l7als7hg/logs
wandb: Agent Starting Run: yojdbceg with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073035-yojdbceg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-287
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yojdbceg
wandb:                                                                                
wandb: 🚀 View run revived-sweep-287 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yojdbceg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073035-yojdbceg/logs
wandb: Agent Starting Run: bf67a6va with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073040-bf67a6va
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-288
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bf67a6va
wandb:                                                                                
wandb: 🚀 View run honest-sweep-288 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bf67a6va
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073040-bf67a6va/logs
wandb: Agent Starting Run: u93fyd9u with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073045-u93fyd9u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-289
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u93fyd9u
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇█▅▄▃▁▁▂▃
wandb:      train_rmse █▇█▅▄▃▁▁▂▃
wandb: validation_loss █▆▅▄▃▂▂▁▁▂
wandb: validation_rmse █▆▅▄▃▂▂▁▁▂
wandb: 
wandb: Run summary:
wandb:      train_loss 1000.67189
wandb:      train_rmse 31.6334
wandb: validation_loss 981.78
wandb: validation_rmse 31.33337
wandb: 
wandb: 🚀 View run rose-sweep-289 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u93fyd9u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073045-u93fyd9u/logs
wandb: Agent Starting Run: sosuhp5q with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073234-sosuhp5q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-290
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sosuhp5q
wandb:                                                                                
wandb: 🚀 View run fine-sweep-290 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sosuhp5q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073234-sosuhp5q/logs
wandb: Agent Starting Run: am2qwm55 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073239-am2qwm55
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-291
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/am2qwm55
wandb:                                                                                
wandb: 🚀 View run decent-sweep-291 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/am2qwm55
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073239-am2qwm55/logs
wandb: Agent Starting Run: luv4bxa7 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073245-luv4bxa7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-292
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/luv4bxa7
wandb:                                                                                
wandb: 🚀 View run electric-sweep-292 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/luv4bxa7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073245-luv4bxa7/logs
wandb: Agent Starting Run: 1hzam69t with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073250-1hzam69t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-293
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1hzam69t
wandb:                                                                                
wandb: 🚀 View run toasty-sweep-293 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1hzam69t
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073250-1hzam69t/logs
wandb: Agent Starting Run: zurjykwa with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073255-zurjykwa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bumbling-sweep-294
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zurjykwa
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run bumbling-sweep-294 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zurjykwa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073255-zurjykwa/logs
wandb: Agent Starting Run: pmelp8k9 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073301-pmelp8k9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-sweep-295
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pmelp8k9
wandb:                                                                                
wandb: 🚀 View run elated-sweep-295 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pmelp8k9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073301-pmelp8k9/logs
wandb: Agent Starting Run: va0de9lc with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073306-va0de9lc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-296
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/va0de9lc
wandb:                                                                                
wandb: 🚀 View run smooth-sweep-296 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/va0de9lc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073306-va0de9lc/logs
wandb: Agent Starting Run: n2kwyesj with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_073311-n2kwyesj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-297
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n2kwyesj
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▃▂▃▁▂▅▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse ▆▄▆▂▃█▄▂▂▂▂▃▂▁▂▃▂▂▁▂▁▂▂▃▂▂▂▁▁▁▃▁▂▁▁▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 36.24381
wandb:      train_rmse 6.02028
wandb: validation_loss 9.87633
wandb: validation_rmse 3.14266
wandb: 
wandb: 🚀 View run crimson-sweep-297 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n2kwyesj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_073311-n2kwyesj/logs
wandb: Agent Starting Run: wf1xoimw with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074117-wf1xoimw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-298
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wf1xoimw
wandb:                                                                                
wandb: 🚀 View run drawn-sweep-298 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wf1xoimw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074117-wf1xoimw/logs
wandb: Agent Starting Run: p20atkhi with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074122-p20atkhi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-299
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p20atkhi
wandb:                                                                                
wandb: 🚀 View run peachy-sweep-299 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p20atkhi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074122-p20atkhi/logs
wandb: Agent Starting Run: 9nrabne5 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074127-9nrabne5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-300
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9nrabne5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▁▁▁▁▁▁▁
wandb:      train_rmse █▂▂▁▁▁▁▁▁▁
wandb: validation_loss ▅▁▆▆▄▆▅█▄▄
wandb: validation_rmse ▅▁▆▆▄▇▅█▄▄
wandb: 
wandb: Run summary:
wandb:      train_loss 40.28874
wandb:      train_rmse 6.34734
wandb: validation_loss 492.94649
wandb: validation_rmse 22.2024
wandb: 
wandb: 🚀 View run golden-sweep-300 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9nrabne5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074127-9nrabne5/logs
wandb: Agent Starting Run: 84lk736d with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074311-84lk736d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-301
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/84lk736d
wandb:                                                                                
wandb: 🚀 View run azure-sweep-301 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/84lk736d
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074311-84lk736d/logs
wandb: Agent Starting Run: 1bwwf5dg with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074316-1bwwf5dg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-302
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1bwwf5dg
wandb:                                                                                
wandb: 🚀 View run twilight-sweep-302 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1bwwf5dg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074316-1bwwf5dg/logs
wandb: Agent Starting Run: zivtphc7 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074322-zivtphc7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-303
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zivtphc7
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [1, -32]
wandb:                                                                                
wandb: 🚀 View run silvery-sweep-303 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zivtphc7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074322-zivtphc7/logs
Run zivtphc7 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [1, -32]

wandb: ERROR Run zivtphc7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -32: [1, -32]
wandb: ERROR 
wandb: Agent Starting Run: 2klu31ed with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074327-2klu31ed
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-304
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2klu31ed
wandb:                                                                                
wandb: 🚀 View run misty-sweep-304 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2klu31ed
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074327-2klu31ed/logs
wandb: Agent Starting Run: klc4e5lc with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074333-klc4e5lc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-305
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/klc4e5lc
wandb:                                                                                
wandb: 🚀 View run serene-sweep-305 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/klc4e5lc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074333-klc4e5lc/logs
wandb: Agent Starting Run: wpkkb7gy with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074338-wpkkb7gy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-306
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wpkkb7gy
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▂▂▂▂▁▁▁
wandb:      train_rmse █▄▄▃▂▂▂▁▁▁
wandb: validation_loss █▅▄▃▂▃▂▁▁▁
wandb: validation_rmse █▆▄▄▂▃▂▂▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 17.65477
wandb:      train_rmse 4.20176
wandb: validation_loss 87.87715
wandb: validation_rmse 9.37428
wandb: 
wandb: 🚀 View run amber-sweep-306 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wpkkb7gy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074338-wpkkb7gy/logs
wandb: Agent Starting Run: e415ub98 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074521-e415ub98
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-307
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e415ub98
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▆▅▄▃▃▂▁
wandb:      train_rmse █▇▇▆▅▅▄▃▂▁
wandb: validation_loss █▇▆▅▅▄▃▃▁▁
wandb: validation_rmse █▇▆▆▅▅▃▃▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 446.60753
wandb:      train_rmse 21.13309
wandb: validation_loss 368.94453
wandb: validation_rmse 19.20793
wandb: 
wandb: 🚀 View run celestial-sweep-307 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e415ub98
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074521-e415ub98/logs
wandb: Agent Starting Run: j9dhkqgh with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074705-j9dhkqgh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-308
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j9dhkqgh
wandb:                                                                                
wandb: 🚀 View run honest-sweep-308 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j9dhkqgh
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074705-j9dhkqgh/logs
wandb: Agent Starting Run: kvwmnlil with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074710-kvwmnlil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-309
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kvwmnlil
wandb:                                                                                
wandb: 🚀 View run pretty-sweep-309 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kvwmnlil
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074710-kvwmnlil/logs
wandb: Agent Starting Run: 04oy239w with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074715-04oy239w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-310
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/04oy239w
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-310 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/04oy239w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074715-04oy239w/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jd2itt8l with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074729-jd2itt8l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-311
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jd2itt8l
wandb:                                                                                
wandb: 🚀 View run decent-sweep-311 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jd2itt8l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074729-jd2itt8l/logs
wandb: Agent Starting Run: slel627y with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074734-slel627y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-312
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/slel627y
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▅▄▃▃▂▁▁
wandb:      train_rmse █▇▆▅▅▄▃▂▂▁
wandb: validation_loss █▇▆▅▄▃▃▂▁▁
wandb: validation_rmse █▇▆▅▄▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 263.43917
wandb:      train_rmse 16.23081
wandb: validation_loss 246.19363
wandb: validation_rmse 15.69056
wandb: 
wandb: 🚀 View run stellar-sweep-312 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/slel627y
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074734-slel627y/logs
wandb: Agent Starting Run: dlgrueff with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074917-dlgrueff
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-313
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dlgrueff
wandb:                                                                                
wandb: 🚀 View run autumn-sweep-313 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dlgrueff
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074917-dlgrueff/logs
wandb: Agent Starting Run: nukybkbr with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074923-nukybkbr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-314
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nukybkbr
wandb:                                                                                
wandb: 🚀 View run brisk-sweep-314 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nukybkbr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074923-nukybkbr/logs
wandb: Agent Starting Run: 7wyg3g1z with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074928-7wyg3g1z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-315
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7wyg3g1z
wandb:                                                                                
wandb: 🚀 View run revived-sweep-315 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7wyg3g1z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074928-7wyg3g1z/logs
wandb: Agent Starting Run: wep3pswf with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_074933-wep3pswf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-316
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wep3pswf
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▆▅▄▃▂▂▁
wandb:      train_rmse █▇▆▆▅▄▃▂▂▁
wandb: validation_loss █▇▆▅▄▄▃▂▂▁
wandb: validation_rmse █▇▆▅▄▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 649.52531
wandb:      train_rmse 25.48579
wandb: validation_loss 639.41437
wandb: validation_rmse 25.28664
wandb: 
wandb: 🚀 View run sage-sweep-316 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wep3pswf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_074933-wep3pswf/logs
wandb: Agent Starting Run: 6lvyp0xl with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075117-6lvyp0xl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-317
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6lvyp0xl
wandb:                                                                                
wandb: 🚀 View run fine-sweep-317 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6lvyp0xl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075117-6lvyp0xl/logs
wandb: Agent Starting Run: mexyt6jv with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075122-mexyt6jv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-318
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mexyt6jv
wandb:                                                                                
wandb: 🚀 View run lucky-sweep-318 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mexyt6jv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075122-mexyt6jv/logs
wandb: Agent Starting Run: fsfrgtr3 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075127-fsfrgtr3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-319
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fsfrgtr3
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▆▅▄▃▂▂▁▁▁
wandb:      train_rmse █▇▆▄▄▃▂▂▁▁
wandb: validation_loss █▆▄▃▂▂▂▁▁▁
wandb: validation_rmse █▇▅▄▃▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 95.762
wandb:      train_rmse 9.78581
wandb: validation_loss 96.44327
wandb: validation_rmse 9.82055
wandb: 
wandb: 🚀 View run dulcet-sweep-319 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fsfrgtr3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075127-fsfrgtr3/logs
wandb: Agent Starting Run: 9wz7we89 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075311-9wz7we89
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-320
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9wz7we89
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.85 GiB. GPU 0 has a total capacity of 39.50 GiB of which 417.38 MiB is free. Including non-PyTorch memory, this process has 39.08 GiB memory in use. Of the allocated memory 32.20 GiB is allocated by PyTorch, and 6.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run driven-sweep-320 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9wz7we89
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075311-9wz7we89/logs
Run 9wz7we89 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.85 GiB. GPU 0 has a total capacity of 39.50 GiB of which 417.38 MiB is free. Including non-PyTorch memory, this process has 39.08 GiB memory in use. Of the allocated memory 32.20 GiB is allocated by PyTorch, and 6.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 9wz7we89 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.85 GiB. GPU 0 has a total capacity of 39.50 GiB of which 417.38 MiB is free. Including non-PyTorch memory, this process has 39.08 GiB memory in use. Of the allocated memory 32.20 GiB is allocated by PyTorch, and 6.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: s1va0zgr with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075336-s1va0zgr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-321
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s1va0zgr
wandb:                                                                                
wandb: 🚀 View run lucky-sweep-321 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s1va0zgr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075336-s1va0zgr/logs
wandb: Agent Starting Run: 9ueuzksz with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075342-9ueuzksz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-322
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9ueuzksz
wandb:                                                                                
wandb: 🚀 View run vocal-sweep-322 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9ueuzksz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075342-9ueuzksz/logs
wandb: Agent Starting Run: vixmp42d with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075347-vixmp42d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-323
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vixmp42d
wandb:                                                                                
wandb: 🚀 View run smooth-sweep-323 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vixmp42d
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075347-vixmp42d/logs
wandb: Agent Starting Run: 1ol48gyy with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075353-1ol48gyy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-324
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1ol48gyy
wandb:                                                                                
wandb: 🚀 View run kind-sweep-324 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1ol48gyy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075353-1ol48gyy/logs
wandb: Agent Starting Run: 1akefwin with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075358-1akefwin
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-325
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1akefwin
wandb:                                                                                
wandb: 🚀 View run noble-sweep-325 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1akefwin
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075358-1akefwin/logs
wandb: Agent Starting Run: emievesn with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075403-emievesn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-326
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/emievesn
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run eternal-sweep-326 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/emievesn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075403-emievesn/logs
wandb: Agent Starting Run: lv6n6b3q with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075408-lv6n6b3q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-327
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lv6n6b3q
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-327 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lv6n6b3q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075408-lv6n6b3q/logs
wandb: Agent Starting Run: 6clf8l32 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075414-6clf8l32
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-328
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6clf8l32
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▇▆▅▄▃▂▂▁
wandb:      train_rmse █▇▇▆▅▄▃▂▂▁
wandb: validation_loss ██▇▅▅▆▅▄▂▁
wandb: validation_rmse ██▇▅▅▆▅▄▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 837.95788
wandb:      train_rmse 28.9475
wandb: validation_loss 880.57458
wandb: validation_rmse 29.67448
wandb: 
wandb: 🚀 View run lunar-sweep-328 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6clf8l32
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075414-6clf8l32/logs
wandb: Agent Starting Run: qhxjfmi7 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075557-qhxjfmi7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-329
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qhxjfmi7
wandb:                                                                                
wandb: 🚀 View run balmy-sweep-329 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qhxjfmi7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075557-qhxjfmi7/logs
wandb: Agent Starting Run: 4ldyyipm with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075603-4ldyyipm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-330
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4ldyyipm
wandb:                                                                                
wandb: 🚀 View run unique-sweep-330 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4ldyyipm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075603-4ldyyipm/logs
wandb: Agent Starting Run: fisyed1y with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075609-fisyed1y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-331
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fisyed1y
wandb:                                                                                
wandb: 🚀 View run valiant-sweep-331 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fisyed1y
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075609-fisyed1y/logs
wandb: Agent Starting Run: mub33sel with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075614-mub33sel
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-332
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mub33sel
wandb:                                                                                
wandb: 🚀 View run rich-sweep-332 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mub33sel
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075614-mub33sel/logs
wandb: Agent Starting Run: m4q1v2dx with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075619-m4q1v2dx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-333
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m4q1v2dx
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (3 x 16). Kernel size: (5 x 3). Kernel size can't be greater than actual input size
wandb:                                                                                
wandb: 🚀 View run logical-sweep-333 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m4q1v2dx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075619-m4q1v2dx/logs
Run m4q1v2dx errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (3 x 16). Kernel size: (5 x 3). Kernel size can't be greater than actual input size

wandb: ERROR Run m4q1v2dx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
wandb: ERROR     return self._conv_forward(input, self.weight, self.bias)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
wandb: ERROR     return F.conv2d(input, weight, bias, self.stride,
wandb: ERROR RuntimeError: Calculated padded input size per channel: (3 x 16). Kernel size: (5 x 3). Kernel size can't be greater than actual input size
wandb: ERROR 
wandb: Agent Starting Run: fvb0x0c4 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_075625-fvb0x0c4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-334
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fvb0x0c4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▅▃▃▃▂▂▃▂▂▁▁▃▂▂▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▆▄▃▃▂▂▃▂▂▁▁▃▂▃▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 0.64479
wandb:      train_rmse 0.80299
wandb: validation_loss 7.02408
wandb: validation_rmse 2.6503
wandb: 
wandb: 🚀 View run balmy-sweep-334 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fvb0x0c4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_075625-fvb0x0c4/logs
wandb: Agent Starting Run: 7bbqcwdk with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_080434-7bbqcwdk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-335
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7bbqcwdk
wandb:                                                                                
wandb: 🚀 View run good-sweep-335 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7bbqcwdk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_080434-7bbqcwdk/logs
wandb: Agent Starting Run: qoqi6uic with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_080440-qoqi6uic
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-336
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qoqi6uic
wandb:                                                                                
wandb: 🚀 View run upbeat-sweep-336 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qoqi6uic
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_080440-qoqi6uic/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: xd18p420 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_080455-xd18p420
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-337
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xd18p420
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (64x1x29). Calculated output size: (64x0x14). Output size is too small
wandb:                                                                                
wandb: 🚀 View run zany-sweep-337 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xd18p420
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_080455-xd18p420/logs
Run xd18p420 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (64x1x29). Calculated output size: (64x0x14). Output size is too small

wandb: ERROR Run xd18p420 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (64x1x29). Calculated output size: (64x0x14). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: 36d5u0ga with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_080501-36d5u0ga
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-sweep-338
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/36d5u0ga
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run hopeful-sweep-338 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/36d5u0ga
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_080501-36d5u0ga/logs
wandb: Agent Starting Run: lz80dbnx with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_080506-lz80dbnx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-339
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lz80dbnx
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 5.50914
wandb:      train_rmse 2.34715
wandb: validation_loss 8.18376
wandb: validation_rmse 2.86073
wandb: 
wandb: 🚀 View run earnest-sweep-339 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lz80dbnx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_080506-lz80dbnx/logs
wandb: Agent Starting Run: q5629ch2 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_081317-q5629ch2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-340
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q5629ch2
wandb:                                                                                
wandb: 🚀 View run brisk-sweep-340 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q5629ch2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_081317-q5629ch2/logs
wandb: Agent Starting Run: urt1cc88 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_081323-urt1cc88
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-341
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/urt1cc88
wandb:                                                                                
wandb: 🚀 View run cosmic-sweep-341 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/urt1cc88
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_081323-urt1cc88/logs
wandb: Agent Starting Run: 53qz5lrc with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_081328-53qz5lrc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-342
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/53qz5lrc
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 16.23762
wandb:      train_rmse 4.02959
wandb: validation_loss 16.75642
wandb: validation_rmse 4.09346
wandb: 
wandb: 🚀 View run glamorous-sweep-342 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/53qz5lrc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_081328-53qz5lrc/logs
wandb: Agent Starting Run: 3skyik8a with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082133-3skyik8a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-343
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3skyik8a
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run warm-sweep-343 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3skyik8a
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082133-3skyik8a/logs
wandb: Agent Starting Run: pxgcg1mj with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082138-pxgcg1mj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-344
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pxgcg1mj
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run warm-sweep-344 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pxgcg1mj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082138-pxgcg1mj/logs
wandb: Agent Starting Run: uhxcy9lj with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082144-uhxcy9lj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-345
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uhxcy9lj
wandb:                                                                                
wandb: 🚀 View run earthy-sweep-345 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uhxcy9lj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082144-uhxcy9lj/logs
wandb: Agent Starting Run: j7u1rbuu with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082149-j7u1rbuu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-346
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j7u1rbuu
wandb:                                                                                
wandb: 🚀 View run swift-sweep-346 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j7u1rbuu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082149-j7u1rbuu/logs
wandb: Agent Starting Run: i7pq6d7q with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082154-i7pq6d7q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-347
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i7pq6d7q
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.49 GiB. GPU 0 has a total capacity of 39.50 GiB of which 413.38 MiB is free. Including non-PyTorch memory, this process has 39.08 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run ruby-sweep-347 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i7pq6d7q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082154-i7pq6d7q/logs
Run i7pq6d7q errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.49 GiB. GPU 0 has a total capacity of 39.50 GiB of which 413.38 MiB is free. Including non-PyTorch memory, this process has 39.08 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run i7pq6d7q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
wandb: ERROR     return F.batch_norm(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
wandb: ERROR     return torch.batch_norm(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.49 GiB. GPU 0 has a total capacity of 39.50 GiB of which 413.38 MiB is free. Including non-PyTorch memory, this process has 39.08 GiB memory in use. Of the allocated memory 28.68 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lmn845xr with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082210-lmn845xr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-348
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lmn845xr
wandb:                                                                                
wandb: 🚀 View run daily-sweep-348 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lmn845xr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082210-lmn845xr/logs
wandb: Agent Starting Run: froyrg6t with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082215-froyrg6t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-349
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/froyrg6t
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (16x1x24). Calculated output size: (16x0x6). Output size is too small
wandb:                                                                                
wandb: 🚀 View run smart-sweep-349 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/froyrg6t
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082215-froyrg6t/logs
Run froyrg6t errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (16x1x24). Calculated output size: (16x0x6). Output size is too small

wandb: ERROR Run froyrg6t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (16x1x24). Calculated output size: (16x0x6). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: fueaw4bg with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082221-fueaw4bg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-350
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fueaw4bg
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run polished-sweep-350 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fueaw4bg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082221-fueaw4bg/logs
wandb: Agent Starting Run: 95u4aqkd with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082226-95u4aqkd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-351
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/95u4aqkd
wandb:                                                                                
wandb: 🚀 View run copper-sweep-351 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/95u4aqkd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082226-95u4aqkd/logs
wandb: Agent Starting Run: oa4yswsi with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082231-oa4yswsi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-352
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oa4yswsi
wandb:                                                                                
wandb: 🚀 View run brisk-sweep-352 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oa4yswsi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082231-oa4yswsi/logs
wandb: Agent Starting Run: xok74pc8 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082237-xok74pc8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-353
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xok74pc8
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [32, -32]
wandb:                                                                                
wandb: 🚀 View run dashing-sweep-353 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xok74pc8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082237-xok74pc8/logs
Run xok74pc8 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [32, -32]

wandb: ERROR Run xok74pc8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -32: [32, -32]
wandb: ERROR 
wandb: Agent Starting Run: c35o16uv with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082242-c35o16uv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-354
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c35o16uv
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run deft-sweep-354 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c35o16uv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082242-c35o16uv/logs
wandb: Agent Starting Run: hguipqxs with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082248-hguipqxs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-355
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hguipqxs
wandb:                                                                                
wandb: 🚀 View run neat-sweep-355 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hguipqxs
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082248-hguipqxs/logs
wandb: Agent Starting Run: cc3ejb7r with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082253-cc3ejb7r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-356
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cc3ejb7r
wandb:                                                                                
wandb: 🚀 View run tough-sweep-356 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cc3ejb7r
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082253-cc3ejb7r/logs
wandb: Agent Starting Run: azp2rjqc with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082258-azp2rjqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-357
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/azp2rjqc
wandb:                                                                                
wandb: 🚀 View run rich-sweep-357 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/azp2rjqc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082258-azp2rjqc/logs
wandb: Agent Starting Run: 0z8cj98d with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_082304-0z8cj98d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-358
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0z8cj98d
wandb:                                                                                
wandb: 
wandb: Run summary:
wandb:      train_loss nan
wandb:      train_rmse nan
wandb: validation_loss nan
wandb: validation_rmse nan
wandb: 
wandb: 🚀 View run restful-sweep-358 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0z8cj98d
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_082304-0z8cj98d/logs
wandb: Agent Starting Run: 7czh47d9 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083125-7czh47d9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-359
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7czh47d9
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run quiet-sweep-359 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7czh47d9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083125-7czh47d9/logs
Run 7czh47d9 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 7czh47d9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: zp6bvn3s with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083130-zp6bvn3s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-360
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zp6bvn3s
wandb:                                                                                
wandb: 🚀 View run lunar-sweep-360 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zp6bvn3s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083130-zp6bvn3s/logs
wandb: Agent Starting Run: hq5ng446 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083135-hq5ng446
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-361
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hq5ng446
wandb:                                                                                
wandb: 🚀 View run resilient-sweep-361 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hq5ng446
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083135-hq5ng446/logs
wandb: Agent Starting Run: ikop2gi6 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083140-ikop2gi6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-362
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ikop2gi6
wandb:                                                                                
wandb: 🚀 View run stoic-sweep-362 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ikop2gi6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083140-ikop2gi6/logs
wandb: Agent Starting Run: b9h39kl0 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083146-b9h39kl0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-363
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b9h39kl0
wandb:                                                                                
wandb: 🚀 View run expert-sweep-363 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b9h39kl0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083146-b9h39kl0/logs
wandb: Agent Starting Run: z7f1rxut with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083151-z7f1rxut
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-364
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z7f1rxut
wandb:                                                                                
wandb: 🚀 View run dazzling-sweep-364 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z7f1rxut
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083151-z7f1rxut/logs
wandb: Agent Starting Run: fm2wgws5 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083156-fm2wgws5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-365
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fm2wgws5
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run charmed-sweep-365 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fm2wgws5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083156-fm2wgws5/logs
Run fm2wgws5 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run fm2wgws5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: bgpbbz0d with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083202-bgpbbz0d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-366
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bgpbbz0d
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -768: [32, -768]
wandb:                                                                                
wandb: 🚀 View run desert-sweep-366 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bgpbbz0d
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083202-bgpbbz0d/logs
Run bgpbbz0d errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -768: [32, -768]

wandb: ERROR Run bgpbbz0d errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -768: [32, -768]
wandb: ERROR 
wandb: Agent Starting Run: bnq2rxt2 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_083207-bnq2rxt2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-367
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bnq2rxt2
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss ▂▂▂▁▁▁▂▁▁▁▂▁▁▄▁▄▁▄▁▃▁▁▆▃▂▁▁▁▁▅▃▁▃█▂▂▁▅▄▁
wandb: validation_rmse ▄▄▃▃▂▂▂▃▂▂▁▄▂▂▆▁▁▅▂▆▄▁▁▁█▃▃▂▁▁▁▅▂▅▃▃▁▇▆▂
wandb: 
wandb: Run summary:
wandb:      train_loss 26.2568
wandb:      train_rmse 5.12414
wandb: validation_loss 8.48483
wandb: validation_rmse 2.91287
wandb: 
wandb: 🚀 View run pretty-sweep-367 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bnq2rxt2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_083207-bnq2rxt2/logs
wandb: Agent Starting Run: puxdjs1s with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084013-puxdjs1s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-368
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/puxdjs1s
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▂▂▁▁▁▁▁▁
wandb: validation_loss █▂▂▁▁▁▁▁▁▁
wandb: validation_rmse █▃▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 75.33697
wandb:      train_rmse 8.67969
wandb: validation_loss 66.60637
wandb: validation_rmse 8.16127
wandb: 
wandb: 🚀 View run atomic-sweep-368 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/puxdjs1s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084013-puxdjs1s/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 41b3a3ws with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084206-41b3a3ws
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-369
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/41b3a3ws
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run sandy-sweep-369 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/41b3a3ws
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084206-41b3a3ws/logs
wandb: Agent Starting Run: j6sme85e with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084212-j6sme85e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-370
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j6sme85e
wandb:                                                                                
wandb: 🚀 View run rural-sweep-370 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j6sme85e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084212-j6sme85e/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: yb6f8nq9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084226-yb6f8nq9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-371
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yb6f8nq9
wandb:                                                                                
wandb: 🚀 View run noble-sweep-371 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yb6f8nq9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084226-yb6f8nq9/logs
wandb: Agent Starting Run: iurxzsha with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084233-iurxzsha
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-372
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/iurxzsha
wandb:                                                                                
wandb: 🚀 View run hearty-sweep-372 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/iurxzsha
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084233-iurxzsha/logs
wandb: Agent Starting Run: 1mnq58xi with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084238-1mnq58xi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-373
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1mnq58xi
wandb:                                                                                
wandb: 🚀 View run electric-sweep-373 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1mnq58xi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084238-1mnq58xi/logs
wandb: Agent Starting Run: 2sd1qvtn with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084243-2sd1qvtn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-374
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2sd1qvtn
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run electric-sweep-374 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2sd1qvtn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084243-2sd1qvtn/logs
Run 2sd1qvtn errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 2sd1qvtn errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: a7vuit19 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084249-a7vuit19
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-375
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a7vuit19
wandb:                                                                                
wandb: 🚀 View run dutiful-sweep-375 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a7vuit19
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084249-a7vuit19/logs
wandb: Agent Starting Run: df33g1qm with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084254-df33g1qm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-376
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/df33g1qm
wandb:                                                                                
wandb: 🚀 View run olive-sweep-376 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/df33g1qm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084254-df33g1qm/logs
wandb: Agent Starting Run: alfc313p with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084259-alfc313p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-377
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/alfc313p
wandb:                                                                                
wandb: 🚀 View run true-sweep-377 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/alfc313p
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084259-alfc313p/logs
wandb: Agent Starting Run: vdjervfu with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084305-vdjervfu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-378
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vdjervfu
wandb:                                                                                
wandb: 🚀 View run worthy-sweep-378 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vdjervfu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084305-vdjervfu/logs
wandb: Agent Starting Run: 2ipx56gy with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084310-2ipx56gy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-379
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2ipx56gy
wandb:                                                                                
wandb: 🚀 View run morning-sweep-379 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2ipx56gy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084310-2ipx56gy/logs
wandb: Agent Starting Run: yvg0shs5 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084315-yvg0shs5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-380
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yvg0shs5
wandb:                                                                                
wandb: 🚀 View run fragrant-sweep-380 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yvg0shs5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084315-yvg0shs5/logs
wandb: Agent Starting Run: u1oqhjpc with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_084321-u1oqhjpc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-381
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u1oqhjpc
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▅▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▅▃▂▂▂▁▁▂▁▁▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▂▁▁
wandb: validation_rmse █▆▄▃▂▂▂▂▂▂▂▁▃▃▂▂▁▁▁▁▁▃▁▁▁▂▁▁▃▁▁▁▁▂▁▁▂▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 1.3732
wandb:      train_rmse 1.17184
wandb: validation_loss 7.63563
wandb: validation_rmse 2.76326
wandb: 
wandb: 🚀 View run happy-sweep-381 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u1oqhjpc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_084321-u1oqhjpc/logs
wandb: Agent Starting Run: howndg79 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085131-howndg79
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-382
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/howndg79
wandb:                                                                                
wandb: 🚀 View run sandy-sweep-382 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/howndg79
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085131-howndg79/logs
wandb: Agent Starting Run: ricf3kms with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085136-ricf3kms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-383
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ricf3kms
wandb:                                                                                
wandb: 🚀 View run vibrant-sweep-383 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ricf3kms
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085136-ricf3kms/logs
wandb: Agent Starting Run: pmjwjrka with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085141-pmjwjrka
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-sweep-384
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pmjwjrka
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run splendid-sweep-384 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pmjwjrka
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085141-pmjwjrka/logs
wandb: Agent Starting Run: cquxwdw8 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085147-cquxwdw8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-385
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cquxwdw8
wandb:                                                                                
wandb: 🚀 View run lively-sweep-385 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cquxwdw8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085147-cquxwdw8/logs
wandb: Agent Starting Run: 8uq0a5qv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085152-8uq0a5qv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-386
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8uq0a5qv
wandb:                                                                                
wandb: 🚀 View run northern-sweep-386 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8uq0a5qv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085152-8uq0a5qv/logs
wandb: Agent Starting Run: l27b5d0n with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085157-l27b5d0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-387
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l27b5d0n
wandb:                                                                                
wandb: 🚀 View run firm-sweep-387 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l27b5d0n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085157-l27b5d0n/logs
wandb: Agent Starting Run: kjn2tcru with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085203-kjn2tcru
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-388
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kjn2tcru
wandb:                                                                                
wandb: 🚀 View run giddy-sweep-388 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kjn2tcru
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085203-kjn2tcru/logs
wandb: Agent Starting Run: lhwzrdr7 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085208-lhwzrdr7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-389
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lhwzrdr7
wandb:                                                                                
wandb: 🚀 View run treasured-sweep-389 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lhwzrdr7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085208-lhwzrdr7/logs
wandb: Agent Starting Run: t2ojx081 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085213-t2ojx081
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-390
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t2ojx081
wandb:                                                                                
wandb: 🚀 View run warm-sweep-390 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t2ojx081
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085213-t2ojx081/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 2hk7gnf8 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085247-2hk7gnf8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-391
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2hk7gnf8
wandb:                                                                                
wandb: 🚀 View run worthy-sweep-391 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2hk7gnf8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085247-2hk7gnf8/logs
wandb: Agent Starting Run: if6rbzda with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085256-if6rbzda
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-392
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/if6rbzda
wandb:                                                                                
wandb: 🚀 View run noble-sweep-392 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/if6rbzda
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085256-if6rbzda/logs
wandb: Agent Starting Run: ayt019n0 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085301-ayt019n0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-393
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ayt019n0
wandb:                                                                                
wandb: 🚀 View run summer-sweep-393 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ayt019n0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085301-ayt019n0/logs
wandb: Agent Starting Run: 5yba41og with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085307-5yba41og
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-394
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5yba41og
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run genial-sweep-394 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5yba41og
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085307-5yba41og/logs
wandb: Agent Starting Run: 8r6uezim with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085312-8r6uezim
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-395
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8r6uezim
wandb:                                                                                
wandb: 🚀 View run lucky-sweep-395 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8r6uezim
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085312-8r6uezim/logs
wandb: Agent Starting Run: gnm4mlh6 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085318-gnm4mlh6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-396
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gnm4mlh6
wandb:                                                                                
wandb: 🚀 View run iconic-sweep-396 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gnm4mlh6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085318-gnm4mlh6/logs
wandb: Agent Starting Run: e3dz0ujp with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085323-e3dz0ujp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-397
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e3dz0ujp
wandb:                                                                                
wandb: 🚀 View run zesty-sweep-397 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e3dz0ujp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085323-e3dz0ujp/logs
wandb: Agent Starting Run: doypcm24 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085328-doypcm24
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-398
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/doypcm24
wandb:                                                                                
wandb: 🚀 View run fiery-sweep-398 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/doypcm24
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085328-doypcm24/logs
wandb: Agent Starting Run: o8monqyv with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085333-o8monqyv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-399
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o8monqyv
wandb:                                                                                
wandb: 🚀 View run restful-sweep-399 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o8monqyv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085333-o8monqyv/logs
wandb: Agent Starting Run: 1zufdt2u with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085338-1zufdt2u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-400
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1zufdt2u
wandb:                                                                                
wandb: 🚀 View run confused-sweep-400 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1zufdt2u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085338-1zufdt2u/logs
wandb: Agent Starting Run: 7mvruyf8 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085344-7mvruyf8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-401
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7mvruyf8
wandb:                                                                                
wandb: 🚀 View run cosmic-sweep-401 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7mvruyf8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085344-7mvruyf8/logs
wandb: Agent Starting Run: 5bap38vo with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085349-5bap38vo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-402
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5bap38vo
wandb:                                                                                
wandb: 🚀 View run feasible-sweep-402 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5bap38vo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085349-5bap38vo/logs
wandb: Agent Starting Run: 2izirsnb with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085355-2izirsnb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-403
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2izirsnb
wandb:                                                                                
wandb: 🚀 View run honest-sweep-403 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2izirsnb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085355-2izirsnb/logs
wandb: Agent Starting Run: 6jve3deq with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085400-6jve3deq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-404
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6jve3deq
wandb:                                                                                
wandb: 🚀 View run peachy-sweep-404 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6jve3deq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085400-6jve3deq/logs
wandb: Agent Starting Run: 491pnjg6 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085405-491pnjg6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-405
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/491pnjg6
wandb:                                                                                
wandb: 🚀 View run driven-sweep-405 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/491pnjg6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085405-491pnjg6/logs
wandb: Agent Starting Run: a3p5etkb with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085411-a3p5etkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-406
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a3p5etkb
wandb:                                                                                
wandb: 🚀 View run good-sweep-406 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a3p5etkb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085411-a3p5etkb/logs
wandb: Agent Starting Run: 4s3vh2ou with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085416-4s3vh2ou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-407
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4s3vh2ou
wandb:                                                                                
wandb: 🚀 View run fiery-sweep-407 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4s3vh2ou
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085416-4s3vh2ou/logs
wandb: Agent Starting Run: fxcbxpq3 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085421-fxcbxpq3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-408
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fxcbxpq3
wandb:                                                                                
wandb: 🚀 View run crimson-sweep-408 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fxcbxpq3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085421-fxcbxpq3/logs
wandb: Agent Starting Run: f0yrvyez with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085426-f0yrvyez
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-409
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f0yrvyez
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-409 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f0yrvyez
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085426-f0yrvyez/logs
wandb: Agent Starting Run: 7s3wvijr with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085432-7s3wvijr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-410
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7s3wvijr
wandb:                                                                                
wandb: 🚀 View run glorious-sweep-410 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7s3wvijr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085432-7s3wvijr/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: qbabsvwz with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085507-qbabsvwz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-411
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qbabsvwz
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▆▄▄▃▂▁▁
wandb:      train_rmse █▇▆▆▄▄▃▂▁▁
wandb: validation_loss █▇▆▅▅▄▃▂▂▁
wandb: validation_rmse █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 808.51171
wandb:      train_rmse 28.43434
wandb: validation_loss 799.16815
wandb: validation_rmse 28.26956
wandb: 
wandb: 🚀 View run wandering-sweep-411 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qbabsvwz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085507-qbabsvwz/logs
wandb: Agent Starting Run: je1d0did with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085717-je1d0did
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-412
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/je1d0did
wandb:                                                                                
wandb: 🚀 View run curious-sweep-412 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/je1d0did
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085717-je1d0did/logs
wandb: Agent Starting Run: 62vs8pdd with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085723-62vs8pdd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-413
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/62vs8pdd
wandb:                                                                                
wandb: 🚀 View run silver-sweep-413 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/62vs8pdd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085723-62vs8pdd/logs
wandb: Agent Starting Run: 60v3olcv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085728-60v3olcv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-414
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/60v3olcv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▅▅▄▅▃▂▂▁
wandb:      train_rmse █▇▅▅▄▅▃▂▂▁
wandb: validation_loss █▆▃▃▂▂▂▂▁▁
wandb: validation_rmse █▆▃▃▂▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 876.56084
wandb:      train_rmse 29.60677
wandb: validation_loss 871.33987
wandb: validation_rmse 29.51847
wandb: 
wandb: 🚀 View run exalted-sweep-414 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/60v3olcv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085728-60v3olcv/logs
wandb: Agent Starting Run: bwat1i5s with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_085917-bwat1i5s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-415
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bwat1i5s
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▇▆▅▆▅▄▄▄▄▄▄▃▃▃▂▃▂▂▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁
wandb: validation_loss █▃▂▂▂▂▁▂▁▁▂▂▂▂▂▁▂▁▂▁▁▁▁▂▂▂▁▂▁▂▃▁▂▁▁▁▁▂▁▁
wandb: validation_rmse █▄▄▃▃▃▂▃▂▂▂▃▂▃▄▃▁▃▃▁▂▂▂▁▂▂▃▂▂▁▂▄▂▂▁▁▁▃▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 12.77589
wandb:      train_rmse 3.57434
wandb: validation_loss 6.50962
wandb: validation_rmse 2.5514
wandb: 
wandb: 🚀 View run expert-sweep-415 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bwat1i5s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_085917-bwat1i5s/logs
wandb: Agent Starting Run: 6dtzg5yd with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_090727-6dtzg5yd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-416
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6dtzg5yd
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▅▄▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss ██▃▃▃▂▂▂▂▁▁▂▂▁▁▁▂▁▁▁▂▁▁▁▁▂▃▁▁▁▁▂▁▁▁▁▁▁▁▂
wandb: validation_rmse ██▄▃▃▂▃▂▂▃▅▂▂▁▁▂▃▂▁▁▂▁▁▂▂▃▃▁▁▂▂▁▁▁▁▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:      train_loss 1.34415
wandb:      train_rmse 1.15937
wandb: validation_loss 13.75228
wandb: validation_rmse 3.70841
wandb: 
wandb: 🚀 View run fancy-sweep-416 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6dtzg5yd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_090727-6dtzg5yd/logs
wandb: Agent Starting Run: 990s4i24 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091533-990s4i24
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-417
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/990s4i24
wandb:                                                                                
wandb: 🚀 View run lucky-sweep-417 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/990s4i24
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091533-990s4i24/logs
wandb: Agent Starting Run: 7iaib7xy with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091538-7iaib7xy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-418
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7iaib7xy
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▅▅▄▃▂▂▁
wandb:      train_rmse █▇▆▅▅▄▃▂▂▁
wandb: validation_loss ██▇▆▅▄▄▂▁▁
wandb: validation_rmse ██▇▆▅▄▄▂▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 872.05967
wandb:      train_rmse 29.53066
wandb: validation_loss 880.81107
wandb: validation_rmse 29.67846
wandb: 
wandb: 🚀 View run spring-sweep-418 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7iaib7xy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091538-7iaib7xy/logs
wandb: Agent Starting Run: fvpes323 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091722-fvpes323
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-419
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fvpes323
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -128: [1, -128]
wandb:                                                                                
wandb: 🚀 View run ruby-sweep-419 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fvpes323
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091722-fvpes323/logs
Run fvpes323 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -128: [1, -128]

wandb: ERROR Run fvpes323 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -128: [1, -128]
wandb: ERROR 
wandb: Agent Starting Run: pkyxqwen with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091727-pkyxqwen
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-420
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pkyxqwen
wandb:                                                                                
wandb: 🚀 View run electric-sweep-420 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pkyxqwen
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091727-pkyxqwen/logs
wandb: Agent Starting Run: u8o5cuu9 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091732-u8o5cuu9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-421
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u8o5cuu9
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fragrant-sweep-421 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u8o5cuu9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091732-u8o5cuu9/logs
wandb: Agent Starting Run: 1s216k8p with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091738-1s216k8p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-422
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1s216k8p
wandb:                                                                                
wandb: 🚀 View run drawn-sweep-422 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1s216k8p
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091738-1s216k8p/logs
wandb: Agent Starting Run: 2yxmr12s with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091743-2yxmr12s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-423
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2yxmr12s
wandb:                                                                                
wandb: 🚀 View run woven-sweep-423 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2yxmr12s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091743-2yxmr12s/logs
wandb: Agent Starting Run: vef69kpx with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091748-vef69kpx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-424
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vef69kpx
wandb:                                                                                
wandb: 🚀 View run fine-sweep-424 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vef69kpx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091748-vef69kpx/logs
wandb: Agent Starting Run: xm0m32df with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091753-xm0m32df
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-425
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xm0m32df
wandb:                                                                                
wandb: 🚀 View run honest-sweep-425 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xm0m32df
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091753-xm0m32df/logs
wandb: Agent Starting Run: 4bl2z3bw with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091759-4bl2z3bw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-426
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4bl2z3bw
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run floral-sweep-426 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4bl2z3bw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091759-4bl2z3bw/logs
wandb: Agent Starting Run: gut8hibc with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091804-gut8hibc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-427
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gut8hibc
wandb:                                                                                
wandb: 🚀 View run generous-sweep-427 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gut8hibc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091804-gut8hibc/logs
wandb: Agent Starting Run: zmwoqtgj with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091809-zmwoqtgj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-428
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zmwoqtgj
wandb:                                                                                
wandb: 🚀 View run sleek-sweep-428 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zmwoqtgj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091809-zmwoqtgj/logs
wandb: Agent Starting Run: fl3mdps6 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091815-fl3mdps6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-429
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fl3mdps6
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-429 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fl3mdps6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091815-fl3mdps6/logs
wandb: Agent Starting Run: pbs9y4im with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091820-pbs9y4im
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-430
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pbs9y4im
wandb:                                                                                
wandb: 🚀 View run usual-sweep-430 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pbs9y4im
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091820-pbs9y4im/logs
wandb: Agent Starting Run: 2k97cgh8 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091825-2k97cgh8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-431
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2k97cgh8
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run easy-sweep-431 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2k97cgh8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091825-2k97cgh8/logs
wandb: Agent Starting Run: xopk533n with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091831-xopk533n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-432
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xopk533n
wandb:                                                                                
wandb: 🚀 View run solar-sweep-432 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xopk533n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091831-xopk533n/logs
wandb: Agent Starting Run: 771ek47x with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091836-771ek47x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-433
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/771ek47x
wandb:                                                                                
wandb: 🚀 View run robust-sweep-433 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/771ek47x
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091836-771ek47x/logs
wandb: Agent Starting Run: 7iw5g71z with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091842-7iw5g71z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-434
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7iw5g71z
wandb:                                                                                
wandb: 🚀 View run crimson-sweep-434 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7iw5g71z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091842-7iw5g71z/logs
wandb: Agent Starting Run: o2x7hzwj with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_091847-o2x7hzwj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-435
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o2x7hzwj
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▄▃▃▃▂▂▂▁▁
wandb: validation_rmse █▄▃▃▃▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 63.2733
wandb:      train_rmse 7.95445
wandb: validation_loss 67.83547
wandb: validation_rmse 8.23623
wandb: 
wandb: 🚀 View run fallen-sweep-435 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o2x7hzwj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_091847-o2x7hzwj/logs
wandb: Agent Starting Run: vke0e5p0 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092036-vke0e5p0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-436
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vke0e5p0
wandb:                                                                                
wandb: 🚀 View run cosmic-sweep-436 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vke0e5p0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092036-vke0e5p0/logs
wandb: Agent Starting Run: 0mdwch1u with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092041-0mdwch1u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-437
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0mdwch1u
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (64x2x7). Calculated output size: (64x0x1). Output size is too small
wandb:                                                                                
wandb: 🚀 View run sweet-sweep-437 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0mdwch1u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092041-0mdwch1u/logs
Run 0mdwch1u errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (64x2x7). Calculated output size: (64x0x1). Output size is too small

wandb: ERROR Run 0mdwch1u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (64x2x7). Calculated output size: (64x0x1). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: ls959hvs with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092046-ls959hvs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-438
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ls959hvs
wandb:                                                                                
wandb: 🚀 View run charmed-sweep-438 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ls959hvs
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092046-ls959hvs/logs
wandb: Agent Starting Run: xtev2yf7 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092052-xtev2yf7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-439
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xtev2yf7
wandb:                                                                                
wandb: 🚀 View run polished-sweep-439 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xtev2yf7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092052-xtev2yf7/logs
wandb: Agent Starting Run: 3k8p4r81 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092057-3k8p4r81
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-440
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3k8p4r81
wandb:                                                                                
wandb: 🚀 View run cool-sweep-440 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3k8p4r81
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092057-3k8p4r81/logs
wandb: Agent Starting Run: qpnwtcbi with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092103-qpnwtcbi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-441
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qpnwtcbi
wandb:                                                                                
wandb: 🚀 View run smart-sweep-441 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qpnwtcbi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092103-qpnwtcbi/logs
wandb: Agent Starting Run: lk3e9162 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092108-lk3e9162
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-442
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lk3e9162
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss ████▅▁▁▁▁▁
wandb:      train_rmse ████▆▁▁▁▁▁
wandb: validation_loss ███▇▂▁▁▁▁▁
wandb: validation_rmse ████▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 67.84457
wandb:      train_rmse 8.23678
wandb: validation_loss 69.00093
wandb: validation_rmse 8.30668
wandb: 
wandb: 🚀 View run silver-sweep-442 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lk3e9162
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092108-lk3e9162/logs
wandb: Agent Starting Run: 1kopzje3 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092251-1kopzje3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-443
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1kopzje3
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss ██▇▆▅▄▃▂▂▁
wandb:      train_rmse ██▇▆▅▄▃▂▂▁
wandb: validation_loss █▇▆▆▅▄▃▃▂▁
wandb: validation_rmse █▇▆▆▅▄▃▃▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 936.16047
wandb:      train_rmse 30.59674
wandb: validation_loss 932.34683
wandb: validation_rmse 30.53435
wandb: 
wandb: 🚀 View run northern-sweep-443 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1kopzje3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092251-1kopzje3/logs
wandb: Agent Starting Run: rvgmescp with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_092435-rvgmescp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-444
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rvgmescp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▅▄▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 4.62646
wandb:      train_rmse 2.15092
wandb: validation_loss 7.2708
wandb: validation_rmse 2.69644
wandb: 
wandb: 🚀 View run robust-sweep-444 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rvgmescp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_092435-rvgmescp/logs
wandb: Agent Starting Run: l677cuf3 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_093251-l677cuf3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-445
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l677cuf3
wandb:                                                                                
wandb: 🚀 View run glowing-sweep-445 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l677cuf3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_093251-l677cuf3/logs
wandb: Agent Starting Run: 5c153ipu with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_093301-5c153ipu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-446
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5c153ipu
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -832: [128, -832]
wandb:                                                                                
wandb: 🚀 View run quiet-sweep-446 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5c153ipu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_093301-5c153ipu/logs
Run 5c153ipu errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -832: [128, -832]

wandb: ERROR Run 5c153ipu errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -832: [128, -832]
wandb: ERROR 
wandb: Agent Starting Run: bi18xjnb with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_093307-bi18xjnb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-447
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bi18xjnb
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x1x10). Calculated output size: (32x0x5). Output size is too small
wandb:                                                                                
wandb: 🚀 View run dry-sweep-447 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bi18xjnb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_093307-bi18xjnb/logs
Run bi18xjnb errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x1x10). Calculated output size: (32x0x5). Output size is too small

wandb: ERROR Run bi18xjnb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (32x1x10). Calculated output size: (32x0x5). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: 42rxdfli with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_093312-42rxdfli
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-448
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/42rxdfli
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run effortless-sweep-448 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/42rxdfli
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_093312-42rxdfli/logs
wandb: Agent Starting Run: 9qs5q8pd with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_093318-9qs5q8pd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-449
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9qs5q8pd
wandb:                                                                                
wandb: 🚀 View run logical-sweep-449 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9qs5q8pd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_093318-9qs5q8pd/logs
wandb: Agent Starting Run: 1vem4wg8 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_093323-1vem4wg8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-450
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1vem4wg8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▃▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 10.05459
wandb:      train_rmse 3.1709
wandb: validation_loss 8.74293
wandb: validation_rmse 2.95684
wandb: 
wandb: 🚀 View run vibrant-sweep-450 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1vem4wg8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_093323-1vem4wg8/logs
wandb: Agent Starting Run: wycdjz78 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_094139-wycdjz78
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-451
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wycdjz78
wandb:                                                                                
wandb: 🚀 View run leafy-sweep-451 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wycdjz78
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_094139-wycdjz78/logs
wandb: Agent Starting Run: g0kwj55o with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_094144-g0kwj55o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-452
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g0kwj55o
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [64, -32]
wandb:                                                                                
wandb: 🚀 View run twilight-sweep-452 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g0kwj55o
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_094144-g0kwj55o/logs
Run g0kwj55o errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [64, -32]

wandb: ERROR Run g0kwj55o errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -32: [64, -32]
wandb: ERROR 
wandb: Agent Starting Run: 5oi2wse1 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_094150-5oi2wse1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-453
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5oi2wse1
wandb:                                                                                
wandb: 🚀 View run vocal-sweep-453 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5oi2wse1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_094150-5oi2wse1/logs
wandb: Agent Starting Run: vf92qjvj with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_094155-vf92qjvj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-454
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vf92qjvj
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▅▄▄▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▄▂▃▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▅▃▄▃▁▁▁▂▂▂▁▁▁▂▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 61.60018
wandb:      train_rmse 7.84858
wandb: validation_loss 40.8953
wandb: validation_rmse 6.39494
wandb: 
wandb: 🚀 View run pleasant-sweep-454 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vf92qjvj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_094155-vf92qjvj/logs
wandb: Agent Starting Run: wpnhwc1p with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095001-wpnhwc1p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-455
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wpnhwc1p
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 🚀 View run graceful-sweep-455 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wpnhwc1p
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095001-wpnhwc1p/logs
wandb: Agent Starting Run: i0y3dsnm with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095012-i0y3dsnm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-456
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i0y3dsnm
wandb:                                                                                
wandb: 🚀 View run devout-sweep-456 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i0y3dsnm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095012-i0y3dsnm/logs
wandb: Agent Starting Run: qqtx9cm3 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095017-qqtx9cm3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-457
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qqtx9cm3
wandb:                                                                                
wandb: 🚀 View run deft-sweep-457 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qqtx9cm3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095017-qqtx9cm3/logs
wandb: Agent Starting Run: j497uj9q with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095022-j497uj9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-458
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j497uj9q
wandb:                                                                                
wandb: 🚀 View run vibrant-sweep-458 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j497uj9q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095022-j497uj9q/logs
wandb: Agent Starting Run: s8xn11gp with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095028-s8xn11gp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-459
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s8xn11gp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss ▅█▆▇▇█▅▂▁▁
wandb:      train_rmse ▅█▆▇▇█▅▂▁▁
wandb: validation_loss ▂▆▆▄▂█▁▂▅▂
wandb: validation_rmse ▂▆▆▄▂█▁▂▅▂
wandb: 
wandb: Run summary:
wandb:      train_loss 939.6175
wandb:      train_rmse 30.65318
wandb: validation_loss 985.14288
wandb: validation_rmse 31.38699
wandb: 
wandb: 🚀 View run pleasant-sweep-459 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s8xn11gp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095028-s8xn11gp/logs
wandb: Agent Starting Run: 3dqxpdwu with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095211-3dqxpdwu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-460
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3dqxpdwu
wandb:                                                                                
wandb: 🚀 View run soft-sweep-460 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3dqxpdwu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095211-3dqxpdwu/logs
wandb: Agent Starting Run: 25tomsye with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095216-25tomsye
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-461
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/25tomsye
wandb:                                                                                
wandb: 🚀 View run resilient-sweep-461 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/25tomsye
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095216-25tomsye/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: ayz7af79 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095232-ayz7af79
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-462
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ayz7af79
wandb:                                                                                
wandb: 🚀 View run balmy-sweep-462 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ayz7af79
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095232-ayz7af79/logs
wandb: Agent Starting Run: 3svisvhq with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095237-3svisvhq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-463
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3svisvhq
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▂▁▁▁▁▁▁▁
wandb:      train_rmse █▅▃▁▁▁▁▁▁▁
wandb: validation_loss █▄▂▁▁▁▁▁▁▁
wandb: validation_rmse █▄▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 65.09709
wandb:      train_rmse 8.06828
wandb: validation_loss 68.85108
wandb: validation_rmse 8.29765
wandb: 
wandb: 🚀 View run dazzling-sweep-463 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3svisvhq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095237-3svisvhq/logs
wandb: Agent Starting Run: etjyhu91 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095421-etjyhu91
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-464
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/etjyhu91
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run noble-sweep-464 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/etjyhu91
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095421-etjyhu91/logs
wandb: Agent Starting Run: urx13qnz with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095426-urx13qnz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-465
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/urx13qnz
wandb:                                                                                
wandb: 🚀 View run floral-sweep-465 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/urx13qnz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095426-urx13qnz/logs
wandb: Agent Starting Run: vpfqez9x with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095431-vpfqez9x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-466
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vpfqez9x
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (5 x 19). Kernel size: (7 x 1). Kernel size can't be greater than actual input size
wandb:                                                                                
wandb: 🚀 View run earnest-sweep-466 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vpfqez9x
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095431-vpfqez9x/logs
Run vpfqez9x errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (5 x 19). Kernel size: (7 x 1). Kernel size can't be greater than actual input size

wandb: ERROR Run vpfqez9x errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
wandb: ERROR     return self._conv_forward(input, self.weight, self.bias)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
wandb: ERROR     return F.conv2d(input, weight, bias, self.stride,
wandb: ERROR RuntimeError: Calculated padded input size per channel: (5 x 19). Kernel size: (7 x 1). Kernel size can't be greater than actual input size
wandb: ERROR 
wandb: Agent Starting Run: yw7ozpfl with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095437-yw7ozpfl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-467
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yw7ozpfl
wandb:                                                                                
wandb: 🚀 View run good-sweep-467 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yw7ozpfl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095437-yw7ozpfl/logs
wandb: Agent Starting Run: qssxlis0 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095442-qssxlis0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-468
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qssxlis0
wandb:                                                                                
wandb: 🚀 View run vibrant-sweep-468 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qssxlis0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095442-qssxlis0/logs
wandb: Agent Starting Run: 7icbvs13 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095447-7icbvs13
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-469
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7icbvs13
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-469 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7icbvs13
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095447-7icbvs13/logs
wandb: Agent Starting Run: vmbpudum with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095453-vmbpudum
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-470
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vmbpudum
wandb:                                                                                
wandb: 🚀 View run youthful-sweep-470 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vmbpudum
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095453-vmbpudum/logs
wandb: Agent Starting Run: r2bhumc2 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095458-r2bhumc2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-471
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r2bhumc2
wandb:                                                                                
wandb: 🚀 View run sage-sweep-471 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r2bhumc2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095458-r2bhumc2/logs
wandb: Agent Starting Run: ssqyyqex with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095503-ssqyyqex
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-472
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ssqyyqex
wandb:                                                                                
wandb: 🚀 View run swept-sweep-472 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ssqyyqex
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095503-ssqyyqex/logs
wandb: Agent Starting Run: pbcpjo6n with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095509-pbcpjo6n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-473
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pbcpjo6n
wandb:                                                                                
wandb: 🚀 View run silver-sweep-473 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pbcpjo6n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095509-pbcpjo6n/logs
wandb: Agent Starting Run: 8kwpfa9g with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095514-8kwpfa9g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-474
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8kwpfa9g
wandb:                                                                                
wandb: 🚀 View run radiant-sweep-474 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8kwpfa9g
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095514-8kwpfa9g/logs
wandb: Agent Starting Run: f97xgdne with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095519-f97xgdne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-475
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f97xgdne
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -64: [1, -64]
wandb:                                                                                
wandb: 🚀 View run neat-sweep-475 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f97xgdne
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095519-f97xgdne/logs
Run f97xgdne errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -64: [1, -64]

wandb: ERROR Run f97xgdne errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -64: [1, -64]
wandb: ERROR 
wandb: Agent Starting Run: 4yplj0af with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095525-4yplj0af
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-476
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4yplj0af
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▇▆▅▅▄▃▂▂▁
wandb:      train_rmse █▇▆▅▅▄▃▂▂▁
wandb: validation_loss █▇▆▅▅▄▃▂▂▁
wandb: validation_rmse █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:      train_loss 868.21238
wandb:      train_rmse 29.46544
wandb: validation_loss 866.35495
wandb: validation_rmse 29.43391
wandb: 
wandb: 🚀 View run super-sweep-476 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4yplj0af
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095525-4yplj0af/logs
wandb: Agent Starting Run: ckan0ngv with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095723-ckan0ngv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-477
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ckan0ngv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▆▄▃▂▂▁▁▁▁
wandb:      train_rmse █▆▄▃▂▂▁▁▁▁
wandb: validation_loss █▅▄▃▂▂▂▁▁▁
wandb: validation_rmse █▆▄▃▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 800.87555
wandb:      train_rmse 28.29974
wandb: validation_loss 795.95972
wandb: validation_rmse 28.21276
wandb: 
wandb: 🚀 View run robust-sweep-477 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ckan0ngv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095723-ckan0ngv/logs
wandb: Agent Starting Run: gfv3zcl8 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095906-gfv3zcl8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-478
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gfv3zcl8
wandb:                                                                                
wandb: 🚀 View run eager-sweep-478 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gfv3zcl8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095906-gfv3zcl8/logs
wandb: Agent Starting Run: r4wjm24q with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095911-r4wjm24q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-479
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r4wjm24q
wandb:                                                                                
wandb: 🚀 View run twilight-sweep-479 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r4wjm24q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095911-r4wjm24q/logs
wandb: Agent Starting Run: r63h0xnp with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_095917-r63h0xnp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-480
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r63h0xnp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▂▁▁▁▁▁▁
wandb:      train_rmse █▅▄▂▂▁▁▁▁▁
wandb: validation_loss ▁▁█▁▅▃▂▁▁▂
wandb: validation_rmse ▁▂█▁▅▄▂▁▁▂
wandb: 
wandb: Run summary:
wandb:      train_loss 9.03491
wandb:      train_rmse 3.00581
wandb: validation_loss 131.66622
wandb: validation_rmse 11.47459
wandb: 
wandb: 🚀 View run confused-sweep-480 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r63h0xnp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_095917-r63h0xnp/logs
wandb: Agent Starting Run: w54nktty with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100100-w54nktty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-481
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w54nktty
wandb:                                                                                
wandb: 🚀 View run breezy-sweep-481 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w54nktty
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100100-w54nktty/logs
wandb: Agent Starting Run: zuiu3ae4 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100106-zuiu3ae4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-482
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zuiu3ae4
wandb:                                                                                
wandb: 🚀 View run young-sweep-482 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zuiu3ae4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100106-zuiu3ae4/logs
wandb: Agent Starting Run: u1ahlmp9 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100111-u1ahlmp9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-483
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u1ahlmp9
wandb:                                                                                
wandb: 🚀 View run devoted-sweep-483 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u1ahlmp9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100111-u1ahlmp9/logs
wandb: Agent Starting Run: 34kqsjg3 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100116-34kqsjg3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-484
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/34kqsjg3
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run iconic-sweep-484 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/34kqsjg3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100116-34kqsjg3/logs
wandb: Agent Starting Run: xzpta4xw with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100122-xzpta4xw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-485
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xzpta4xw
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▂▁▁▁▁▁▁▁▁
wandb: validation_loss ▂▃█▂▄▂▄▆▁▂
wandb: validation_rmse ▂▄█▂▄▂▄▆▁▂
wandb: 
wandb: Run summary:
wandb:      train_loss 94.59499
wandb:      train_rmse 9.726
wandb: validation_loss 70.25637
wandb: validation_rmse 8.38191
wandb: 
wandb: 🚀 View run sparkling-sweep-485 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xzpta4xw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100122-xzpta4xw/logs
wandb: Agent Starting Run: uzl1g8wa with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100305-uzl1g8wa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-486
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uzl1g8wa
wandb:                                                                                
wandb: 🚀 View run celestial-sweep-486 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uzl1g8wa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100305-uzl1g8wa/logs
wandb: Agent Starting Run: b4po5uf1 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100311-b4po5uf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-487
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b4po5uf1
wandb:                                                                                
wandb: 🚀 View run celestial-sweep-487 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b4po5uf1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100311-b4po5uf1/logs
wandb: Agent Starting Run: urgrk9zm with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100316-urgrk9zm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-488
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/urgrk9zm
wandb:                                                                                
wandb: 🚀 View run smooth-sweep-488 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/urgrk9zm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100316-urgrk9zm/logs
wandb: Agent Starting Run: 6fukyt1j with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100321-6fukyt1j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-489
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6fukyt1j
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 🚀 View run fine-sweep-489 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6fukyt1j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100321-6fukyt1j/logs
wandb: Agent Starting Run: jqpr9v41 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100332-jqpr9v41
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-490
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jqpr9v41
wandb:                                                                                
wandb: 🚀 View run dandy-sweep-490 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jqpr9v41
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100332-jqpr9v41/logs
wandb: Agent Starting Run: 5aaos805 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100337-5aaos805
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-491
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5aaos805
wandb:                                                                                
wandb: 🚀 View run wandering-sweep-491 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5aaos805
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100337-5aaos805/logs
wandb: Agent Starting Run: pq87lke2 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100343-pq87lke2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-492
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pq87lke2
wandb:                                                                                
wandb: 🚀 View run vivid-sweep-492 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pq87lke2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100343-pq87lke2/logs
wandb: Agent Starting Run: j5pnnm8l with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100348-j5pnnm8l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-493
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j5pnnm8l
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fragrant-sweep-493 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j5pnnm8l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100348-j5pnnm8l/logs
wandb: Agent Starting Run: ikouazvc with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100353-ikouazvc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-494
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ikouazvc
wandb:                                                                                
wandb: 🚀 View run icy-sweep-494 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ikouazvc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100353-ikouazvc/logs
wandb: Agent Starting Run: 7sffeu21 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100359-7sffeu21
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-495
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7sffeu21
wandb:                                                                                
wandb: 🚀 View run fresh-sweep-495 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7sffeu21
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100359-7sffeu21/logs
wandb: Agent Starting Run: 9fhgqteu with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100404-9fhgqteu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-496
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9fhgqteu
wandb:                                                                                
wandb: 🚀 View run summer-sweep-496 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9fhgqteu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100404-9fhgqteu/logs
wandb: Agent Starting Run: 14xr64lf with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_100409-14xr64lf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-497
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/14xr64lf
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_loss 35.45895
wandb:      train_rmse 5.95474
wandb: validation_loss 37.57491
wandb: validation_rmse 6.12984
wandb: 
wandb: 🚀 View run divine-sweep-497 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/14xr64lf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_100409-14xr64lf/logs
wandb: Agent Starting Run: z4xxkwqs with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101215-z4xxkwqs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-498
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z4xxkwqs
wandb:                                                                                
wandb: 🚀 View run distinctive-sweep-498 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z4xxkwqs
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101215-z4xxkwqs/logs
wandb: Agent Starting Run: wdjr1q6z with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101221-wdjr1q6z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-499
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wdjr1q6z
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.36 GiB. GPU 0 has a total capacity of 39.50 GiB of which 409.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 33.00 GiB is allocated by PyTorch, and 5.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run easy-sweep-499 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wdjr1q6z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101221-wdjr1q6z/logs
Run wdjr1q6z errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.36 GiB. GPU 0 has a total capacity of 39.50 GiB of which 409.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 33.00 GiB is allocated by PyTorch, and 5.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wdjr1q6z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
wandb: ERROR     return F.batch_norm(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
wandb: ERROR     return torch.batch_norm(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.36 GiB. GPU 0 has a total capacity of 39.50 GiB of which 409.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 33.00 GiB is allocated by PyTorch, and 5.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: mslvkdjd with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101236-mslvkdjd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-500
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mslvkdjd
wandb:                                                                                
wandb: 🚀 View run fanciful-sweep-500 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mslvkdjd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101236-mslvkdjd/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: n915k3kp with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101300-n915k3kp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-501
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n915k3kp
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run warm-sweep-501 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n915k3kp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101300-n915k3kp/logs
wandb: Agent Starting Run: 6t3dhah6 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101308-6t3dhah6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-502
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6t3dhah6
wandb:                                                                                
wandb: 🚀 View run magic-sweep-502 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6t3dhah6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101308-6t3dhah6/logs
wandb: Agent Starting Run: mrffytnj with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101313-mrffytnj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-503
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mrffytnj
wandb:                                                                                
wandb: 🚀 View run silver-sweep-503 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mrffytnj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101313-mrffytnj/logs
wandb: Agent Starting Run: mosrfybz with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101318-mosrfybz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-504
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mosrfybz
wandb:                                                                                
wandb: 🚀 View run swift-sweep-504 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mosrfybz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101318-mosrfybz/logs
wandb: Agent Starting Run: o6x14rfk with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101324-o6x14rfk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-505
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o6x14rfk
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [1, -32]
wandb:                                                                                
wandb: 🚀 View run divine-sweep-505 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o6x14rfk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101324-o6x14rfk/logs
Run o6x14rfk errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [1, -32]

wandb: ERROR Run o6x14rfk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -32: [1, -32]
wandb: ERROR 
wandb: Agent Starting Run: s43iyynu with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101329-s43iyynu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-506
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s43iyynu
wandb:                                                                                
wandb: 🚀 View run vital-sweep-506 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s43iyynu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101329-s43iyynu/logs
wandb: Agent Starting Run: xmrnma7q with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101334-xmrnma7q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-507
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xmrnma7q
wandb:                                                                                
wandb: 🚀 View run exalted-sweep-507 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xmrnma7q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101334-xmrnma7q/logs
wandb: Agent Starting Run: fep4xefj with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101340-fep4xefj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-508
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fep4xefj
wandb:                                                                                
wandb: 🚀 View run iconic-sweep-508 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fep4xefj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101340-fep4xefj/logs
wandb: Agent Starting Run: nymup3it with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101345-nymup3it
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-509
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nymup3it
wandb:                                                                                
wandb: 🚀 View run eager-sweep-509 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nymup3it
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101345-nymup3it/logs
wandb: Agent Starting Run: rlve4537 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101350-rlve4537
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-510
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rlve4537
wandb:                                                                                
wandb: 🚀 View run clean-sweep-510 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rlve4537
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101350-rlve4537/logs
wandb: Agent Starting Run: f29z7c5r with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101356-f29z7c5r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-511
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f29z7c5r
wandb:                                                                                
wandb: 🚀 View run vital-sweep-511 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f29z7c5r
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101356-f29z7c5r/logs
wandb: Agent Starting Run: qnnsj9gy with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101401-qnnsj9gy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-512
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qnnsj9gy
wandb:                                                                                
wandb: 🚀 View run fiery-sweep-512 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qnnsj9gy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101401-qnnsj9gy/logs
wandb: Agent Starting Run: nyceso3k with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101406-nyceso3k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-513
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nyceso3k
wandb:                                                                                
wandb: 🚀 View run winter-sweep-513 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nyceso3k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101406-nyceso3k/logs
wandb: Agent Starting Run: j93ruu9k with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101412-j93ruu9k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-514
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j93ruu9k
wandb:                                                                                
wandb: 🚀 View run cosmic-sweep-514 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j93ruu9k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101412-j93ruu9k/logs
wandb: Agent Starting Run: 1y5aaohn with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101417-1y5aaohn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-515
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1y5aaohn
wandb:                                                                                
wandb: 🚀 View run distinctive-sweep-515 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1y5aaohn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101417-1y5aaohn/logs
wandb: Agent Starting Run: rac7z8g0 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101422-rac7z8g0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-516
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rac7z8g0
wandb:                                                                                
wandb: 🚀 View run astral-sweep-516 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rac7z8g0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101422-rac7z8g0/logs
wandb: Agent Starting Run: 0haryirl with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101428-0haryirl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-517
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0haryirl
wandb:                                                                                
wandb: 🚀 View run fanciful-sweep-517 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0haryirl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101428-0haryirl/logs
wandb: Agent Starting Run: r7tdh3f3 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101433-r7tdh3f3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-518
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r7tdh3f3
wandb:                                                                                
wandb: 🚀 View run copper-sweep-518 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r7tdh3f3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101433-r7tdh3f3/logs
wandb: Agent Starting Run: fzul55ip with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101438-fzul55ip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-519
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fzul55ip
wandb:                                                                                
wandb: 🚀 View run clear-sweep-519 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fzul55ip
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101438-fzul55ip/logs
wandb: Agent Starting Run: i9wvt9gr with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_101444-i9wvt9gr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-520
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i9wvt9gr
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss █▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train_rmse █▅▄▃▃▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: validation_loss █▅▄▃▃▂▂▂▂▁▁▁▂▂▂▂▂▂▁▁▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb: validation_rmse █▆▅▄▄▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▂▂▁▁▁▁▁▂▁▂
wandb: 
wandb: Run summary:
wandb:      train_loss 49.31354
wandb:      train_rmse 7.02236
wandb: validation_loss 10.3357
wandb: validation_rmse 3.21492
wandb: 
wandb: 🚀 View run fearless-sweep-520 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i9wvt9gr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_101444-i9wvt9gr/logs
wandb: Agent Starting Run: qix1txo4 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102254-qix1txo4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-521
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qix1txo4
wandb:                                                                                
wandb: 🚀 View run youthful-sweep-521 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qix1txo4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102254-qix1txo4/logs
wandb: Agent Starting Run: 29vjy27m with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102300-29vjy27m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-522
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/29vjy27m
wandb:                                                                                
wandb: 🚀 View run morning-sweep-522 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/29vjy27m
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102300-29vjy27m/logs
wandb: Agent Starting Run: v1d4wqbo with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102305-v1d4wqbo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-523
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v1d4wqbo
wandb:                                                                                
wandb: 🚀 View run peachy-sweep-523 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v1d4wqbo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102305-v1d4wqbo/logs
wandb: Agent Starting Run: 4sfkmkj0 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102310-4sfkmkj0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-524
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4sfkmkj0
wandb:                                                                                
wandb: 🚀 View run golden-sweep-524 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4sfkmkj0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102310-4sfkmkj0/logs
wandb: Agent Starting Run: xqtqx14u with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102315-xqtqx14u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-525
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xqtqx14u
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x1x11). Calculated output size: (32x0x2). Output size is too small
wandb:                                                                                
wandb: 🚀 View run graceful-sweep-525 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xqtqx14u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102315-xqtqx14u/logs
Run xqtqx14u errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: Given input size: (32x1x11). Calculated output size: (32x0x2). Output size is too small

wandb: ERROR Run xqtqx14u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: Given input size: (32x1x11). Calculated output size: (32x0x2). Output size is too small
wandb: ERROR 
wandb: Agent Starting Run: jtp29xct with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102321-jtp29xct
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-526
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jtp29xct
wandb:                                                                                
wandb: 🚀 View run comfy-sweep-526 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jtp29xct
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102321-jtp29xct/logs
wandb: Agent Starting Run: wi181wrc with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102326-wi181wrc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-527
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wi181wrc
wandb:                                                                                
wandb: 🚀 View run fallen-sweep-527 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wi181wrc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102326-wi181wrc/logs
wandb: Agent Starting Run: wbbopw3h with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102331-wbbopw3h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-528
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wbbopw3h
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (4 x 27). Kernel size: (5 x 3). Kernel size can't be greater than actual input size
wandb:                                                                                
wandb: 🚀 View run warm-sweep-528 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wbbopw3h
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102331-wbbopw3h/logs
Run wbbopw3h errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (4 x 27). Kernel size: (5 x 3). Kernel size can't be greater than actual input size

wandb: ERROR Run wbbopw3h errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
wandb: ERROR     return self._conv_forward(input, self.weight, self.bias)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
wandb: ERROR     return F.conv2d(input, weight, bias, self.stride,
wandb: ERROR RuntimeError: Calculated padded input size per channel: (4 x 27). Kernel size: (5 x 3). Kernel size can't be greater than actual input size
wandb: ERROR 
wandb: Agent Starting Run: ye5bdiht with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102337-ye5bdiht
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-529
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ye5bdiht
wandb:                                                                                
wandb: 🚀 View run swift-sweep-529 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ye5bdiht
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102337-ye5bdiht/logs
wandb: Agent Starting Run: 7yzf4b4k with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102342-7yzf4b4k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-530
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7yzf4b4k
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fluent-sweep-530 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7yzf4b4k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102342-7yzf4b4k/logs
wandb: Agent Starting Run: me77003m with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102347-me77003m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-531
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/me77003m
wandb:                                                                                
wandb: 🚀 View run expert-sweep-531 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/me77003m
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102347-me77003m/logs
wandb: Agent Starting Run: 4ecv92hh with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102353-4ecv92hh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-532
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4ecv92hh
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-532 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4ecv92hh
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102353-4ecv92hh/logs
wandb: Agent Starting Run: 82u933bq with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102358-82u933bq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-533
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/82u933bq
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (5 x 76). Kernel size: (7 x 7). Kernel size can't be greater than actual input size
wandb:                                                                                
wandb: 🚀 View run unique-sweep-533 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/82u933bq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102358-82u933bq/logs
Run 82u933bq errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (5 x 76). Kernel size: (7 x 7). Kernel size can't be greater than actual input size

wandb: ERROR Run 82u933bq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
wandb: ERROR     return self._conv_forward(input, self.weight, self.bias)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
wandb: ERROR     return F.conv2d(input, weight, bias, self.stride,
wandb: ERROR RuntimeError: Calculated padded input size per channel: (5 x 76). Kernel size: (7 x 7). Kernel size can't be greater than actual input size
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: gg2ditu5 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102413-gg2ditu5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-534
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gg2ditu5
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.24 GiB is allocated by PyTorch, and 2.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run glowing-sweep-534 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gg2ditu5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102413-gg2ditu5/logs
Run gg2ditu5 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.24 GiB is allocated by PyTorch, and 2.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run gg2ditu5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.24 GiB is allocated by PyTorch, and 2.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: m4136szu with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102430-m4136szu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-535
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m4136szu
wandb:                                                                                
wandb: 🚀 View run dainty-sweep-535 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m4136szu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102430-m4136szu/logs
wandb: Agent Starting Run: dqo07lkb with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102435-dqo07lkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-536
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dqo07lkb
wandb:                                                                                
wandb: 🚀 View run quiet-sweep-536 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dqo07lkb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102435-dqo07lkb/logs
wandb: Agent Starting Run: qofnpdhd with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102440-qofnpdhd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-537
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qofnpdhd
wandb:                                                                                
wandb: 🚀 View run glad-sweep-537 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qofnpdhd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102440-qofnpdhd/logs
wandb: Agent Starting Run: x4slqmkg with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102446-x4slqmkg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-sweep-538
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x4slqmkg
wandb:                                                                                
wandb: 🚀 View run quiet-sweep-538 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x4slqmkg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102446-x4slqmkg/logs
wandb: Agent Starting Run: yulr2jb1 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102451-yulr2jb1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-539
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yulr2jb1
wandb:                                                                                
wandb: 🚀 View run magic-sweep-539 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yulr2jb1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102451-yulr2jb1/logs
wandb: Agent Starting Run: d0guqihu with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102456-d0guqihu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-540
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d0guqihu
wandb:                                                                                
wandb: 🚀 View run super-sweep-540 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d0guqihu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102456-d0guqihu/logs
wandb: Agent Starting Run: 9lq7c037 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102502-9lq7c037
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-541
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9lq7c037
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run kind-sweep-541 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9lq7c037
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102502-9lq7c037/logs
Run 9lq7c037 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 9lq7c037 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: gv912e6v with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102507-gv912e6v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-542
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gv912e6v
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      train_loss ▄▁▄▁▅▃█▁▂▆
wandb:      train_rmse ▄▁▄▁▅▃█▁▂▆
wandb: validation_loss ▄█▄▁▅▅▅▃▄▄
wandb: validation_rmse ▄█▄▁▅▅▅▃▄▄
wandb: 
wandb: Run summary:
wandb:      train_loss 988.2433
wandb:      train_rmse 31.43634
wandb: validation_loss 979.30719
wandb: validation_rmse 31.29388
wandb: 
wandb: 🚀 View run breezy-sweep-542 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gv912e6v
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102507-gv912e6v/logs
wandb: Agent Starting Run: pcj0diuc with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102650-pcj0diuc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-543
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pcj0diuc
wandb:                                                                                
wandb: 🚀 View run brisk-sweep-543 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pcj0diuc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102650-pcj0diuc/logs
wandb: Agent Starting Run: 0l60hzbp with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102656-0l60hzbp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-544
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0l60hzbp
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 37.10 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run glamorous-sweep-544 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0l60hzbp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102656-0l60hzbp/logs
Run 0l60hzbp errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 37.10 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0l60hzbp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 245, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 37.10 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: q3cficfv with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102711-q3cficfv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-545
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q3cficfv
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.70 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run jolly-sweep-545 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q3cficfv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102711-q3cficfv/logs
Run q3cficfv errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.70 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run q3cficfv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 405.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.70 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 73bxqbp0 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102727-73bxqbp0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-546
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/73bxqbp0
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-546 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/73bxqbp0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102727-73bxqbp0/logs
wandb: Agent Starting Run: o7nqbz6l with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102732-o7nqbz6l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-547
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o7nqbz6l
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 403.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.70 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run vague-sweep-547 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o7nqbz6l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102732-o7nqbz6l/logs
Run o7nqbz6l errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 403.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.70 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run o7nqbz6l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 403.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.70 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8e0dl464 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102748-8e0dl464
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-548
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8e0dl464
wandb:                                                                                
wandb: 🚀 View run dazzling-sweep-548 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8e0dl464
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102748-8e0dl464/logs
wandb: Agent Starting Run: 7tg4j3xw with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102753-7tg4j3xw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hearty-sweep-549
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7tg4j3xw
wandb:                                                                                
wandb: 🚀 View run hearty-sweep-549 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7tg4j3xw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102753-7tg4j3xw/logs
wandb: Agent Starting Run: h1tz5zp5 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102758-h1tz5zp5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-550
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h1tz5zp5
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run legendary-sweep-550 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h1tz5zp5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102758-h1tz5zp5/logs
wandb: Agent Starting Run: h8m6kny8 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102804-h8m6kny8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-551
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h8m6kny8
wandb:                                                                                
wandb: 🚀 View run cool-sweep-551 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h8m6kny8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102804-h8m6kny8/logs
wandb: Agent Starting Run: 6onz92a0 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102809-6onz92a0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-552
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6onz92a0
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 403.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run dry-sweep-552 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6onz92a0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102809-6onz92a0/logs
Run 6onz92a0 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 403.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6onz92a0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 403.38 MiB is free. Including non-PyTorch memory, this process has 39.09 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2bpc4g9n with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102825-2bpc4g9n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-553
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2bpc4g9n
wandb:                                                                                
wandb: 🚀 View run revived-sweep-553 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2bpc4g9n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102825-2bpc4g9n/logs
wandb: Agent Starting Run: s4fh86ua with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102830-s4fh86ua
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-554
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s4fh86ua
wandb:                                                                                
wandb: 🚀 View run sweet-sweep-554 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s4fh86ua
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102830-s4fh86ua/logs
wandb: Agent Starting Run: v73hpgm9 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102835-v73hpgm9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sweep-555
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v73hpgm9
wandb:                                                                                
wandb: 🚀 View run dauntless-sweep-555 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v73hpgm9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102835-v73hpgm9/logs
wandb: Agent Starting Run: yvomq5qs with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102841-yvomq5qs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-556
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yvomq5qs
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb:                                                                                
wandb: 🚀 View run expert-sweep-556 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yvomq5qs
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102841-yvomq5qs/logs
wandb: Agent Starting Run: lhyot8bo with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102851-lhyot8bo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-557
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lhyot8bo
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb:                                                                                
wandb: 🚀 View run gallant-sweep-557 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lhyot8bo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102851-lhyot8bo/logs
wandb: Agent Starting Run: 4um8sj8i with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102901-4um8sj8i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-558
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4um8sj8i
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run glamorous-sweep-558 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4um8sj8i
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102901-4um8sj8i/logs
Run 4um8sj8i errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4um8sj8i errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: vut2wjxi with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102917-vut2wjxi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-559
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vut2wjxi
wandb:                                                                                
wandb: 🚀 View run fine-sweep-559 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vut2wjxi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102917-vut2wjxi/logs
wandb: Agent Starting Run: 9vow9xf0 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102922-9vow9xf0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-560
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9vow9xf0
wandb:                                                                                
wandb: 🚀 View run stoic-sweep-560 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9vow9xf0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102922-9vow9xf0/logs
wandb: Agent Starting Run: 6bp5myu5 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102928-6bp5myu5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-561
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6bp5myu5
wandb:                                                                                
wandb: 🚀 View run ethereal-sweep-561 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6bp5myu5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102928-6bp5myu5/logs
wandb: Agent Starting Run: gxvrqcss with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102933-gxvrqcss
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-562
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gxvrqcss
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run easy-sweep-562 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gxvrqcss
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102933-gxvrqcss/logs
wandb: Agent Starting Run: xwtc9tts with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102938-xwtc9tts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-563
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xwtc9tts
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run royal-sweep-563 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xwtc9tts
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102938-xwtc9tts/logs
Run xwtc9tts errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run xwtc9tts errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: j4og5v20 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102954-j4og5v20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-564
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j4og5v20
wandb:                                                                                
wandb: 🚀 View run snowy-sweep-564 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j4og5v20
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102954-j4og5v20/logs
wandb: Agent Starting Run: 148qmf5m with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_102959-148qmf5m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-565
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/148qmf5m
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run flowing-sweep-565 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/148qmf5m
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_102959-148qmf5m/logs
wandb: Agent Starting Run: todqytwf with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103005-todqytwf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-566
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/todqytwf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run efficient-sweep-566 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/todqytwf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103005-todqytwf/logs
Run todqytwf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run todqytwf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 401.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f0ubk079 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103020-f0ubk079
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-567
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f0ubk079
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 🚀 View run sleek-sweep-567 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f0ubk079
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103020-f0ubk079/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: y8m4nqd6 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103036-y8m4nqd6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-568
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y8m4nqd6
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 397.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run leafy-sweep-568 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y8m4nqd6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103036-y8m4nqd6/logs
Run y8m4nqd6 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 397.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run y8m4nqd6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 397.38 MiB is free. Including non-PyTorch memory, this process has 39.10 GiB memory in use. Of the allocated memory 36.71 GiB is allocated by PyTorch, and 1.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5jy064ep with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103052-5jy064ep
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-569
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5jy064ep
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 🚀 View run valiant-sweep-569 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5jy064ep
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103052-5jy064ep/logs
wandb: Agent Starting Run: lqolmf5t with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103128-lqolmf5t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-570
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lqolmf5t
wandb:                                                                                
wandb: 🚀 View run amber-sweep-570 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lqolmf5t
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103128-lqolmf5t/logs
wandb: Agent Starting Run: 66e85kn0 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103133-66e85kn0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-571
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/66e85kn0
wandb:                                                                                
wandb: 🚀 View run upbeat-sweep-571 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/66e85kn0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103133-66e85kn0/logs
wandb: Agent Starting Run: ooeo7g39 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103139-ooeo7g39
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-572
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ooeo7g39
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
    loss.backward()
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 156.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.49 GiB is allocated by PyTorch, and 424.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run distinctive-sweep-572 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ooeo7g39
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103139-ooeo7g39/logs
Run ooeo7g39 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
    loss.backward()
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 156.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.49 GiB is allocated by PyTorch, and 424.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ooeo7g39 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
wandb: ERROR     loss.backward()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
wandb: ERROR     torch.autograd.backward(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
wandb: ERROR     _engine_run_backward(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
wandb: ERROR     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 156.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.49 GiB is allocated by PyTorch, and 424.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: wfhux0az with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103144-wfhux0az
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-sweep-573
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wfhux0az
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.45 GiB is allocated by PyTorch, and 461.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run silver-sweep-573 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wfhux0az
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103144-wfhux0az/logs
Run wfhux0az errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.45 GiB is allocated by PyTorch, and 461.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wfhux0az errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.45 GiB is allocated by PyTorch, and 461.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: sclo9y1i with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103200-sclo9y1i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-574
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sclo9y1i
wandb:                                                                                
wandb: 🚀 View run rosy-sweep-574 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sclo9y1i
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103200-sclo9y1i/logs
wandb: Agent Starting Run: w5v4kktn with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103205-w5v4kktn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-575
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w5v4kktn
wandb:                                                                                
wandb: 🚀 View run apricot-sweep-575 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w5v4kktn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103205-w5v4kktn/logs
wandb: Agent Starting Run: lah6m141 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103210-lah6m141
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-576
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lah6m141
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.45 GiB is allocated by PyTorch, and 461.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run cosmic-sweep-576 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lah6m141
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103210-lah6m141/logs
Run lah6m141 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.45 GiB is allocated by PyTorch, and 461.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run lah6m141 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.45 GiB is allocated by PyTorch, and 461.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 33uejvc7 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103226-33uejvc7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-577
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/33uejvc7
wandb:                                                                                
wandb: 🚀 View run logical-sweep-577 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/33uejvc7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103226-33uejvc7/logs
wandb: Agent Starting Run: 781olekp with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103231-781olekp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-578
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/781olekp
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 415.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run icy-sweep-578 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/781olekp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103231-781olekp/logs
Run 781olekp errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 415.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 781olekp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 415.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: y6sov9gb with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103247-y6sov9gb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-sweep-579
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y6sov9gb
wandb:                                                                                
wandb: 🚀 View run peachy-sweep-579 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y6sov9gb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103247-y6sov9gb/logs
wandb: Agent Starting Run: vknnqt2k with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103252-vknnqt2k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-580
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vknnqt2k
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 414.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run feasible-sweep-580 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vknnqt2k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103252-vknnqt2k/logs
Run vknnqt2k errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 414.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vknnqt2k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 414.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 34gg8jrs with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103308-34gg8jrs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-581
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/34gg8jrs
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 414.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run smart-sweep-581 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/34gg8jrs
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103308-34gg8jrs/logs
Run 34gg8jrs errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 414.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 34gg8jrs errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 39.50 GiB of which 85.38 MiB is free. Including non-PyTorch memory, this process has 39.40 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 414.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: wsyk4wo8 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103319-wsyk4wo8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-sweep-582
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wsyk4wo8
wandb:                                                                                
wandb: 🚀 View run woven-sweep-582 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wsyk4wo8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103319-wsyk4wo8/logs
wandb: Agent Starting Run: 0l4ev1qz with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103324-0l4ev1qz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-583
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0l4ev1qz
wandb:                                                                                
wandb: 🚀 View run absurd-sweep-583 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0l4ev1qz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103324-0l4ev1qz/logs
wandb: Agent Starting Run: mik602ay with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103329-mik602ay
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-584
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mik602ay
wandb:                                                                                
wandb: 🚀 View run toasty-sweep-584 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mik602ay
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103329-mik602ay/logs
wandb: Agent Starting Run: 74i73vyb with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103334-74i73vyb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-585
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/74i73vyb
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 416.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run northern-sweep-585 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/74i73vyb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103334-74i73vyb/logs
Run 74i73vyb errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 416.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 74i73vyb errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 416.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: egztfn1b with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103350-egztfn1b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-586
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/egztfn1b
wandb:                                                                                
wandb: 🚀 View run scarlet-sweep-586 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/egztfn1b
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103350-egztfn1b/logs
wandb: Agent Starting Run: umnkjdia with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103355-umnkjdia
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-587
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/umnkjdia
wandb:                                                                                
wandb: 🚀 View run volcanic-sweep-587 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/umnkjdia
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103355-umnkjdia/logs
wandb: Agent Starting Run: k79cs206 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103401-k79cs206
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-588
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k79cs206
wandb:                                                                                
wandb: 🚀 View run generous-sweep-588 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k79cs206
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103401-k79cs206/logs
wandb: Agent Starting Run: 6v5x93yf with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103406-6v5x93yf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-589
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6v5x93yf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 415.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run curious-sweep-589 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6v5x93yf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103406-6v5x93yf/logs
Run 6v5x93yf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 415.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6v5x93yf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 244, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.50 GiB is allocated by PyTorch, and 415.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f7la81eu with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103422-f7la81eu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-590
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f7la81eu
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run bright-sweep-590 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f7la81eu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103422-f7la81eu/logs
wandb: Agent Starting Run: x88o6bax with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103427-x88o6bax
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-591
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x88o6bax
wandb:                                                                                
wandb: 🚀 View run desert-sweep-591 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x88o6bax
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103427-x88o6bax/logs
wandb: Agent Starting Run: m3ijg60x with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103433-m3ijg60x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-592
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m3ijg60x
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run dandy-sweep-592 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m3ijg60x
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103433-m3ijg60x/logs
Run m3ijg60x errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run m3ijg60x errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: r8yg3u5s with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103438-r8yg3u5s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-593
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r8yg3u5s
wandb:                                                                                
wandb: 🚀 View run brisk-sweep-593 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r8yg3u5s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103438-r8yg3u5s/logs
wandb: Agent Starting Run: a0qvagml with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103443-a0qvagml
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-594
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a0qvagml
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run zany-sweep-594 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a0qvagml
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103443-a0qvagml/logs
wandb: Agent Starting Run: flz2wxl4 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103448-flz2wxl4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-595
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/flz2wxl4
wandb:                                                                                
wandb: 🚀 View run swift-sweep-595 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/flz2wxl4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103448-flz2wxl4/logs
wandb: Agent Starting Run: j0kvq6wd with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103454-j0kvq6wd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-596
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j0kvq6wd
wandb:                                                                                
wandb: 🚀 View run ethereal-sweep-596 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j0kvq6wd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103454-j0kvq6wd/logs
wandb: Agent Starting Run: y01vzhyr with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103459-y01vzhyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-597
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y01vzhyr
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
    loss.backward()
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.73 GiB is allocated by PyTorch, and 245.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run unique-sweep-597 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y01vzhyr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103459-y01vzhyr/logs
Run y01vzhyr errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
    loss.backward()
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.73 GiB is allocated by PyTorch, and 245.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run y01vzhyr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
wandb: ERROR     loss.backward()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
wandb: ERROR     torch.autograd.backward(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
wandb: ERROR     _engine_run_backward(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
wandb: ERROR     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.73 GiB is allocated by PyTorch, and 245.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: qtcn9cxw with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103505-qtcn9cxw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-598
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qtcn9cxw
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.43 GiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.67 GiB is allocated by PyTorch, and 245.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run likely-sweep-598 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qtcn9cxw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103505-qtcn9cxw/logs
Run qtcn9cxw errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.43 GiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.67 GiB is allocated by PyTorch, and 245.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qtcn9cxw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.43 GiB. GPU 0 has a total capacity of 39.50 GiB of which 81.38 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.67 GiB is allocated by PyTorch, and 245.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cx0rhjf2 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103515-cx0rhjf2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-599
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cx0rhjf2
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run misunderstood-sweep-599 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cx0rhjf2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103515-cx0rhjf2/logs
wandb: Agent Starting Run: t118ma6q with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103520-t118ma6q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-600
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t118ma6q
wandb:                                                                                
wandb: 🚀 View run pious-sweep-600 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t118ma6q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103520-t118ma6q/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: n0g67bij with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103534-n0g67bij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-601
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n0g67bij
wandb:                                                                                
wandb: 🚀 View run light-sweep-601 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n0g67bij
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103534-n0g67bij/logs
wandb: Agent Starting Run: u3dfsx21 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103542-u3dfsx21
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-602
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u3dfsx21
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fragrant-sweep-602 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u3dfsx21
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103542-u3dfsx21/logs
wandb: Agent Starting Run: qlxb612k with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103547-qlxb612k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-603
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qlxb612k
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.81 GiB is allocated by PyTorch, and 163.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run devout-sweep-603 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qlxb612k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103547-qlxb612k/logs
Run qlxb612k errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.81 GiB is allocated by PyTorch, and 163.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qlxb612k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.81 GiB is allocated by PyTorch, and 163.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 0pdssd2b with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103553-0pdssd2b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-604
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0pdssd2b
wandb:                                                                                
wandb: 🚀 View run crimson-sweep-604 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0pdssd2b
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103553-0pdssd2b/logs
wandb: Agent Starting Run: dj61apdz with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103558-dj61apdz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-605
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dj61apdz
wandb:                                                                                
wandb: 🚀 View run cosmic-sweep-605 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dj61apdz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103558-dj61apdz/logs
wandb: Agent Starting Run: qs3zn7yi with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103603-qs3zn7yi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-606
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qs3zn7yi
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -448: [128, -448]
wandb:                                                                                
wandb: 🚀 View run generous-sweep-606 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qs3zn7yi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103603-qs3zn7yi/logs
Run qs3zn7yi errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -448: [128, -448]

wandb: ERROR Run qs3zn7yi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -448: [128, -448]
wandb: ERROR 
wandb: Agent Starting Run: l2a6lfft with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103609-l2a6lfft
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-607
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l2a6lfft
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 196.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run devoted-sweep-607 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l2a6lfft
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103609-l2a6lfft/logs
Run l2a6lfft errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 196.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run l2a6lfft errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 196.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cd1gvwvg with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103614-cd1gvwvg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-608
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cd1gvwvg
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 196.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run devout-sweep-608 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cd1gvwvg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103614-cd1gvwvg/logs
Run cd1gvwvg errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 196.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run cd1gvwvg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.78 GiB is allocated by PyTorch, and 196.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ov2xb190 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103619-ov2xb190
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-609
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ov2xb190
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run kind-sweep-609 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ov2xb190
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103619-ov2xb190/logs
Run ov2xb190 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ov2xb190 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
wandb: ERROR     return self._conv_forward(input, self.weight, self.bias)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
wandb: ERROR     return F.conv2d(input, weight, bias, self.stride,
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: hjuvuuu3 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103625-hjuvuuu3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-610
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hjuvuuu3
wandb:                                                                                
wandb: 🚀 View run iconic-sweep-610 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hjuvuuu3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103625-hjuvuuu3/logs
wandb: Agent Starting Run: 519zy0aq with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103630-519zy0aq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-611
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/519zy0aq
wandb:                                                                                
wandb: 🚀 View run expert-sweep-611 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/519zy0aq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103630-519zy0aq/logs
wandb: Agent Starting Run: nb9s7a41 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103636-nb9s7a41
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-612
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nb9s7a41
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run fanciful-sweep-612 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nb9s7a41
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103636-nb9s7a41/logs
Run nb9s7a41 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run nb9s7a41 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: rjko2ok3 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103651-rjko2ok3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-613
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rjko2ok3
wandb:                                                                                
wandb: 🚀 View run floral-sweep-613 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rjko2ok3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103651-rjko2ok3/logs
wandb: Agent Starting Run: tthf53nh with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103657-tthf53nh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-614
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tthf53nh
wandb:                                                                                
wandb: 🚀 View run dutiful-sweep-614 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tthf53nh
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103657-tthf53nh/logs
wandb: Agent Starting Run: 4qmxu014 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103702-4qmxu014
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-615
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4qmxu014
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run ethereal-sweep-615 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4qmxu014
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103702-4qmxu014/logs
Run 4qmxu014 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4qmxu014 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 2wuybn14 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103717-2wuybn14
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-616
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2wuybn14
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run deep-sweep-616 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2wuybn14
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103717-2wuybn14/logs
Run 2wuybn14 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2wuybn14 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 42.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: qkkgrykr with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103723-qkkgrykr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-617
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qkkgrykr
wandb:                                                                                
wandb: 🚀 View run exalted-sweep-617 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qkkgrykr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103723-qkkgrykr/logs
wandb: Agent Starting Run: qcvhykqp with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103728-qcvhykqp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-618
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qcvhykqp
wandb:                                                                                
wandb: 🚀 View run amber-sweep-618 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qcvhykqp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103728-qcvhykqp/logs
wandb: Agent Starting Run: bytmpl1z with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103733-bytmpl1z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-619
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bytmpl1z
wandb:                                                                                
wandb: 🚀 View run revived-sweep-619 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bytmpl1z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103733-bytmpl1z/logs
wandb: Agent Starting Run: pm3q2kza with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103739-pm3q2kza
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-620
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pm3q2kza
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run playful-sweep-620 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pm3q2kza
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103739-pm3q2kza/logs
wandb: Agent Starting Run: iv03rx9e with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103744-iv03rx9e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-621
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/iv03rx9e
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run tough-sweep-621 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/iv03rx9e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103744-iv03rx9e/logs
wandb: Agent Starting Run: l9jqy9a2 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103750-l9jqy9a2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-622
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l9jqy9a2
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 272.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run upbeat-sweep-622 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l9jqy9a2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103750-l9jqy9a2/logs
Run l9jqy9a2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 272.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run l9jqy9a2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 272.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.83 GiB is allocated by PyTorch, and 143.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2a8u2anb with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103755-2a8u2anb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-623
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2a8u2anb
wandb:                                                                                
wandb: 🚀 View run eternal-sweep-623 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2a8u2anb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103755-2a8u2anb/logs
wandb: Agent Starting Run: qkz40090 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103801-qkz40090
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-624
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qkz40090
wandb:                                                                                
wandb: 🚀 View run winter-sweep-624 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qkz40090
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103801-qkz40090/logs
wandb: Agent Starting Run: m7hb8z1c with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103806-m7hb8z1c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-625
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m7hb8z1c
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run grateful-sweep-625 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m7hb8z1c
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103806-m7hb8z1c/logs
Run m7hb8z1c errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run m7hb8z1c errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ulxlc4ml with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103811-ulxlc4ml
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-626
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ulxlc4ml
wandb:                                                                                
wandb: 🚀 View run rare-sweep-626 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ulxlc4ml
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103811-ulxlc4ml/logs
wandb: Agent Starting Run: tr5ozc6w with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103817-tr5ozc6w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-627
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tr5ozc6w
wandb:                                                                                
wandb: 🚀 View run crisp-sweep-627 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tr5ozc6w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103817-tr5ozc6w/logs
wandb: Agent Starting Run: mari1m0n with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103822-mari1m0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-628
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mari1m0n
wandb:                                                                                
wandb: 🚀 View run light-sweep-628 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mari1m0n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103822-mari1m0n/logs
wandb: Agent Starting Run: 22xtzald with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103827-22xtzald
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-629
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/22xtzald
wandb:                                                                                
wandb: 🚀 View run upbeat-sweep-629 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/22xtzald
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103827-22xtzald/logs
wandb: Agent Starting Run: ses2yuv5 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103833-ses2yuv5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-630
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ses2yuv5
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run serene-sweep-630 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ses2yuv5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103833-ses2yuv5/logs
wandb: Agent Starting Run: 5lxydvwi with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103838-5lxydvwi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-631
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5lxydvwi
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run cosmic-sweep-631 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5lxydvwi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103838-5lxydvwi/logs
wandb: Agent Starting Run: c5vadvhl with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103844-c5vadvhl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-632
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c5vadvhl
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run snowy-sweep-632 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c5vadvhl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103844-c5vadvhl/logs
Run c5vadvhl errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run c5vadvhl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: z4ej7jne with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103849-z4ej7jne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-633
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z4ej7jne
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run fine-sweep-633 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z4ej7jne
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103849-z4ej7jne/logs
Run z4ej7jne errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run z4ej7jne errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gksmi4bj with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103854-gksmi4bj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-634
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gksmi4bj
wandb:                                                                                
wandb: 🚀 View run sparkling-sweep-634 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gksmi4bj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103854-gksmi4bj/logs
wandb: Agent Starting Run: 440yvucg with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103859-440yvucg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sweep-635
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/440yvucg
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run upbeat-sweep-635 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/440yvucg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103859-440yvucg/logs
Run 440yvucg errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 440yvucg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.84 GiB is allocated by PyTorch, and 135.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: jof6vl9v with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103905-jof6vl9v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-636
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jof6vl9v
wandb:                                                                                
wandb: 🚀 View run expert-sweep-636 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jof6vl9v
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103905-jof6vl9v/logs
wandb: Agent Starting Run: mkcxh5aj with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103910-mkcxh5aj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-637
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mkcxh5aj
wandb:                                                                                
wandb: 🚀 View run jolly-sweep-637 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mkcxh5aj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103910-mkcxh5aj/logs
wandb: Agent Starting Run: 5wzat0bl with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103915-5wzat0bl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-638
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5wzat0bl
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 119.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run sweepy-sweep-638 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5wzat0bl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103915-5wzat0bl/logs
Run 5wzat0bl errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 119.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5wzat0bl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.86 GiB is allocated by PyTorch, and 119.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: tdwzvsfw with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103921-tdwzvsfw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-639
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tdwzvsfw
wandb:                                                                                
wandb: 🚀 View run divine-sweep-639 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tdwzvsfw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103921-tdwzvsfw/logs
wandb: Agent Starting Run: xpb8jr59 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103926-xpb8jr59
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-640
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xpb8jr59
wandb:                                                                                
wandb: 🚀 View run treasured-sweep-640 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xpb8jr59
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103926-xpb8jr59/logs
wandb: Agent Starting Run: vol94ww6 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103932-vol94ww6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-641
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vol94ww6
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run jumping-sweep-641 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vol94ww6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103932-vol94ww6/logs
Run vol94ww6 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vol94ww6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lgun07s5 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103937-lgun07s5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-642
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lgun07s5
wandb:                                                                                
wandb: 🚀 View run graceful-sweep-642 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lgun07s5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103937-lgun07s5/logs
wandb: Agent Starting Run: jcc518f9 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103942-jcc518f9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-643
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jcc518f9
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-643 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jcc518f9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103942-jcc518f9/logs
wandb: Agent Starting Run: rq380w1c with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103948-rq380w1c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-644
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rq380w1c
wandb:                                                                                
wandb: 🚀 View run decent-sweep-644 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rq380w1c
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103948-rq380w1c/logs
wandb: Agent Starting Run: 1lgnnx4o with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103953-1lgnnx4o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-645
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1lgnnx4o
wandb:                                                                                
wandb: 🚀 View run solar-sweep-645 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1lgnnx4o
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103953-1lgnnx4o/logs
wandb: Agent Starting Run: y5gwmr0p with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_103958-y5gwmr0p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-646
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y5gwmr0p
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run glowing-sweep-646 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y5gwmr0p
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_103958-y5gwmr0p/logs
Run y5gwmr0p errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run y5gwmr0p errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8xrsym6y with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104004-8xrsym6y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-647
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8xrsym6y
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run ethereal-sweep-647 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8xrsym6y
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104004-8xrsym6y/logs
Run 8xrsym6y errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8xrsym6y errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 4f8uau0j with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104019-4f8uau0j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-648
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4f8uau0j
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb:                                                                                
wandb: 🚀 View run glamorous-sweep-648 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4f8uau0j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104019-4f8uau0j/logs
wandb: Agent Starting Run: k1ws0rb4 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104025-k1ws0rb4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-649
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k1ws0rb4
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run golden-sweep-649 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k1ws0rb4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104025-k1ws0rb4/logs
Run k1ws0rb4 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run k1ws0rb4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: i8ggoi99 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104030-i8ggoi99
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-650
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i8ggoi99
wandb:                                                                                
wandb: 🚀 View run fallen-sweep-650 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i8ggoi99
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104030-i8ggoi99/logs
wandb: Agent Starting Run: kbdtj7hb with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104035-kbdtj7hb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-651
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kbdtj7hb
wandb:                                                                                
wandb: 🚀 View run faithful-sweep-651 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kbdtj7hb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104035-kbdtj7hb/logs
wandb: Agent Starting Run: 6bayhr8d with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104041-6bayhr8d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-652
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6bayhr8d
wandb:                                                                                
wandb: 🚀 View run radiant-sweep-652 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6bayhr8d
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104041-6bayhr8d/logs
wandb: Agent Starting Run: 6zax9i05 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104046-6zax9i05
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-653
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6zax9i05
wandb:                                                                                
wandb: 🚀 View run glamorous-sweep-653 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6zax9i05
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104046-6zax9i05/logs
wandb: Agent Starting Run: icwktchp with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104051-icwktchp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-654
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/icwktchp
wandb:                                                                                
wandb: 🚀 View run good-sweep-654 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/icwktchp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104051-icwktchp/logs
wandb: Agent Starting Run: sko4z5zb with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104057-sko4z5zb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-655
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sko4z5zb
wandb:                                                                                
wandb: 🚀 View run iconic-sweep-655 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sko4z5zb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104057-sko4z5zb/logs
wandb: Agent Starting Run: pafyrwmf with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104102-pafyrwmf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-656
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pafyrwmf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run divine-sweep-656 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pafyrwmf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104102-pafyrwmf/logs
Run pafyrwmf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run pafyrwmf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 90.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: zbfl5gtt with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104107-zbfl5gtt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-657
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zbfl5gtt
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run light-sweep-657 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zbfl5gtt
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104107-zbfl5gtt/logs
Run zbfl5gtt errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run zbfl5gtt errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5du6p21v with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104113-5du6p21v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-sweep-658
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5du6p21v
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb:                                                                                
wandb: 🚀 View run smart-sweep-658 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5du6p21v
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104113-5du6p21v/logs
Run 5du6p21v errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 5du6p21v errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: yhha8vce with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104118-yhha8vce
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-659
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yhha8vce
wandb:                                                                                
wandb: 🚀 View run misty-sweep-659 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yhha8vce
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104118-yhha8vce/logs
wandb: Agent Starting Run: huiqgrc3 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104123-huiqgrc3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-660
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/huiqgrc3
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run glorious-sweep-660 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/huiqgrc3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104123-huiqgrc3/logs
Run huiqgrc3 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run huiqgrc3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: pu1beoom with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104129-pu1beoom
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-661
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pu1beoom
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run balmy-sweep-661 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pu1beoom
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104129-pu1beoom/logs
wandb: Agent Starting Run: v7qd23y8 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104134-v7qd23y8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-662
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v7qd23y8
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run worldly-sweep-662 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/v7qd23y8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104134-v7qd23y8/logs
Run v7qd23y8 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run v7qd23y8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 89.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 6f3dp5ri with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104149-6f3dp5ri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-663
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6f3dp5ri
wandb:                                                                                
wandb: 🚀 View run robust-sweep-663 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6f3dp5ri
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104149-6f3dp5ri/logs
wandb: Agent Starting Run: gp6sjycv with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104155-gp6sjycv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-664
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gp6sjycv
wandb:                                                                                
wandb: 🚀 View run azure-sweep-664 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gp6sjycv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104155-gp6sjycv/logs
wandb: Agent Starting Run: ngz61ohf with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104201-ngz61ohf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-665
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ngz61ohf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 79.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run polished-sweep-665 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ngz61ohf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104201-ngz61ohf/logs
Run ngz61ohf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 79.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ngz61ohf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.89 GiB is allocated by PyTorch, and 79.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 73io4zk7 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104206-73io4zk7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-666
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/73io4zk7
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run earnest-sweep-666 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/73io4zk7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104206-73io4zk7/logs
Run 73io4zk7 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 73io4zk7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 19kkbygc with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104211-19kkbygc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-667
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/19kkbygc
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run neat-sweep-667 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/19kkbygc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104211-19kkbygc/logs
Run 19kkbygc errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 19kkbygc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8jyuzco3 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104217-8jyuzco3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-668
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8jyuzco3
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run young-sweep-668 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8jyuzco3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104217-8jyuzco3/logs
Run 8jyuzco3 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8jyuzco3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: nakh23nk with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104222-nakh23nk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-669
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nakh23nk
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run graceful-sweep-669 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nakh23nk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104222-nakh23nk/logs
Run nakh23nk errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run nakh23nk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: x40vv4yn with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104227-x40vv4yn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-670
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x40vv4yn
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-670 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x40vv4yn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104227-x40vv4yn/logs
wandb: Agent Starting Run: aaodp38n with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104233-aaodp38n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-671
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aaodp38n
wandb:                                                                                
wandb: 🚀 View run glad-sweep-671 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aaodp38n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104233-aaodp38n/logs
wandb: Agent Starting Run: 7nnrc6ww with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104238-7nnrc6ww
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-672
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7nnrc6ww
wandb:                                                                                
wandb: 🚀 View run northern-sweep-672 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7nnrc6ww
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104238-7nnrc6ww/logs
wandb: Agent Starting Run: ez9xsyxg with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104244-ez9xsyxg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-673
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ez9xsyxg
wandb:                                                                                
wandb: 🚀 View run misty-sweep-673 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ez9xsyxg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104244-ez9xsyxg/logs
wandb: Agent Starting Run: sl9zjn7u with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104249-sl9zjn7u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-674
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sl9zjn7u
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 544.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-674 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/sl9zjn7u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104249-sl9zjn7u/logs
Run sl9zjn7u errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 544.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run sl9zjn7u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 544.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: d62prwkk with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104254-d62prwkk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-675
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d62prwkk
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run tough-sweep-675 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d62prwkk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104254-d62prwkk/logs
Run d62prwkk errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run d62prwkk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: n2b4rrfw with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104300-n2b4rrfw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-676
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n2b4rrfw
wandb:                                                                                
wandb: 🚀 View run sunny-sweep-676 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n2b4rrfw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104300-n2b4rrfw/logs
wandb: Agent Starting Run: voh3lt52 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104305-voh3lt52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-677
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/voh3lt52
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run drawn-sweep-677 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/voh3lt52
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104305-voh3lt52/logs
Run voh3lt52 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run voh3lt52 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 13.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: eq8l9s37 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104310-eq8l9s37
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-sweep-678
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eq8l9s37
wandb:                                                                                
wandb: 🚀 View run sandy-sweep-678 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eq8l9s37
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104310-eq8l9s37/logs
wandb: Agent Starting Run: xjrlbjtf with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104316-xjrlbjtf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-679
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xjrlbjtf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run floral-sweep-679 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xjrlbjtf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104316-xjrlbjtf/logs
Run xjrlbjtf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run xjrlbjtf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: epcxeoqq with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104321-epcxeoqq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-680
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/epcxeoqq
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run fresh-sweep-680 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/epcxeoqq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104321-epcxeoqq/logs
Run epcxeoqq errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run epcxeoqq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: dlzzm5rf with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104327-dlzzm5rf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-681
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dlzzm5rf
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run gentle-sweep-681 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dlzzm5rf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104327-dlzzm5rf/logs
Run dlzzm5rf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run dlzzm5rf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: wxxfqmim with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104332-wxxfqmim
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-682
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wxxfqmim
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run fluent-sweep-682 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wxxfqmim
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104332-wxxfqmim/logs
Run wxxfqmim errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wxxfqmim errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 74x4hf5k with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104337-74x4hf5k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-683
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/74x4hf5k
wandb:                                                                                
wandb: 🚀 View run skilled-sweep-683 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/74x4hf5k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104337-74x4hf5k/logs
wandb: Agent Starting Run: lyf3y93h with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104343-lyf3y93h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-684
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lyf3y93h
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run revived-sweep-684 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lyf3y93h
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104343-lyf3y93h/logs
Run lyf3y93h errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run lyf3y93h errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ybtw4von with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104348-ybtw4von
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-685
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ybtw4von
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run clear-sweep-685 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ybtw4von
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104348-ybtw4von/logs
Run ybtw4von errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ybtw4von errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7hqns57p with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104353-7hqns57p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-686
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7hqns57p
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run chocolate-sweep-686 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7hqns57p
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104353-7hqns57p/logs
wandb: Agent Starting Run: khn5mwq0 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104359-khn5mwq0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-687
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/khn5mwq0
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run zesty-sweep-687 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/khn5mwq0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104359-khn5mwq0/logs
wandb: Agent Starting Run: q1cdacnv with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104404-q1cdacnv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-688
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q1cdacnv
wandb:                                                                                
wandb: 🚀 View run lyric-sweep-688 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q1cdacnv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104404-q1cdacnv/logs
wandb: Agent Starting Run: k1c24ayt with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104409-k1c24ayt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-689
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k1c24ayt
wandb:                                                                                
wandb: 🚀 View run astral-sweep-689 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k1c24ayt
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104409-k1c24ayt/logs
wandb: Agent Starting Run: 1wkynylx with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104415-1wkynylx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-690
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1wkynylx
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run easy-sweep-690 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1wkynylx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104415-1wkynylx/logs
Run 1wkynylx errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1wkynylx errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: th2ak574 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104420-th2ak574
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-691
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/th2ak574
wandb:                                                                                
wandb: 🚀 View run graceful-sweep-691 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/th2ak574
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104420-th2ak574/logs
wandb: Agent Starting Run: jyiju2ry with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104425-jyiju2ry
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-692
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jyiju2ry
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run icy-sweep-692 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jyiju2ry
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104425-jyiju2ry/logs
wandb: Agent Starting Run: m21iq37r with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104431-m21iq37r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-693
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m21iq37r
wandb:                                                                                
wandb: 🚀 View run laced-sweep-693 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m21iq37r
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104431-m21iq37r/logs
wandb: Agent Starting Run: 6w1chen7 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104436-6w1chen7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-694
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6w1chen7
wandb:                                                                                
wandb: 🚀 View run atomic-sweep-694 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6w1chen7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104436-6w1chen7/logs
wandb: Agent Starting Run: zrghrqjp with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104441-zrghrqjp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-695
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zrghrqjp
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run sweepy-sweep-695 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zrghrqjp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104441-zrghrqjp/logs
wandb: Agent Starting Run: vbidsei2 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104447-vbidsei2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-696
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vbidsei2
wandb:                                                                                
wandb: 🚀 View run whole-sweep-696 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vbidsei2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104447-vbidsei2/logs
wandb: Agent Starting Run: 47tilflu with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104452-47tilflu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-697
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/47tilflu
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run wobbly-sweep-697 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/47tilflu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104452-47tilflu/logs
Run 47tilflu errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 47tilflu errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: eifvjiyc with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104458-eifvjiyc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-698
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eifvjiyc
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run kind-sweep-698 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eifvjiyc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104458-eifvjiyc/logs
Run eifvjiyc errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run eifvjiyc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7wv52pqz with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104503-7wv52pqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-699
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7wv52pqz
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run crisp-sweep-699 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7wv52pqz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104503-7wv52pqz/logs
Run 7wv52pqz errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7wv52pqz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: w3mw72pv with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104518-w3mw72pv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-700
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w3mw72pv
wandb:                                                                                
wandb: 🚀 View run prime-sweep-700 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w3mw72pv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104518-w3mw72pv/logs
wandb: Agent Starting Run: l8u2d81n with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104524-l8u2d81n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-701
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l8u2d81n
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb: / 0.003 MB of 0.003 MB uploadedwandb: - 0.003 MB of 0.003 MB uploadedwandb:                                                                                
wandb: 🚀 View run efficient-sweep-701 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l8u2d81n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104524-l8u2d81n/logs
wandb: Agent Starting Run: 74vmiqco with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104535-74vmiqco
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-702
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/74vmiqco
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run wandering-sweep-702 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/74vmiqco
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104535-74vmiqco/logs
Run 74vmiqco errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 74vmiqco errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: n1fr3wky with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104540-n1fr3wky
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-703
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n1fr3wky
wandb:                                                                                
wandb: 🚀 View run royal-sweep-703 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n1fr3wky
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104540-n1fr3wky/logs
wandb: Agent Starting Run: y2aczkgd with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104545-y2aczkgd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-704
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y2aczkgd
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run curious-sweep-704 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y2aczkgd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104545-y2aczkgd/logs
Run y2aczkgd errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run y2aczkgd errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: w0atccxq with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104551-w0atccxq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-705
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w0atccxq
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-705 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w0atccxq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104551-w0atccxq/logs
Run w0atccxq errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run w0atccxq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 78.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ryxebw9n with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104556-ryxebw9n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-706
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ryxebw9n
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run summer-sweep-706 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ryxebw9n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104556-ryxebw9n/logs
Run ryxebw9n errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ryxebw9n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 11.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 77.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: awfcujpq with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104601-awfcujpq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-707
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/awfcujpq
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -256: [1, -256]
wandb:                                                                                
wandb: 🚀 View run cerulean-sweep-707 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/awfcujpq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104601-awfcujpq/logs
Run awfcujpq errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -256: [1, -256]

wandb: ERROR Run awfcujpq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -256: [1, -256]
wandb: ERROR 
wandb: Agent Starting Run: b77nfdmi with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104607-b77nfdmi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-708
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b77nfdmi
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run celestial-sweep-708 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/b77nfdmi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104607-b77nfdmi/logs
Run b77nfdmi errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run b77nfdmi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: dz8pj6hi with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104622-dz8pj6hi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-709
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dz8pj6hi
wandb:                                                                                
wandb: 🚀 View run still-sweep-709 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dz8pj6hi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104622-dz8pj6hi/logs
wandb: Agent Starting Run: iw7smp7b with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104628-iw7smp7b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-710
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/iw7smp7b
wandb:                                                                                
wandb: 🚀 View run vocal-sweep-710 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/iw7smp7b
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104628-iw7smp7b/logs
wandb: Agent Starting Run: f4qqd010 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104633-f4qqd010
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-711
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f4qqd010
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -416: [64, -416]
wandb:                                                                                
wandb: 🚀 View run vague-sweep-711 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f4qqd010
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104633-f4qqd010/logs
Run f4qqd010 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -416: [64, -416]

wandb: ERROR Run f4qqd010 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -416: [64, -416]
wandb: ERROR 
wandb: Agent Starting Run: 4243s2vg with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104638-4243s2vg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-712
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4243s2vg
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-712 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4243s2vg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104638-4243s2vg/logs
wandb: Agent Starting Run: ffpi5ji9 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104644-ffpi5ji9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-713
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ffpi5ji9
wandb:                                                                                
wandb: 🚀 View run gentle-sweep-713 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ffpi5ji9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104644-ffpi5ji9/logs
wandb: Agent Starting Run: ic57sx5z with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104649-ic57sx5z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-714
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ic57sx5z
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -832: [1, -832]
wandb:                                                                                
wandb: 🚀 View run breezy-sweep-714 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ic57sx5z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104649-ic57sx5z/logs
Run ic57sx5z errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -832: [1, -832]

wandb: ERROR Run ic57sx5z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -832: [1, -832]
wandb: ERROR 
wandb: Agent Starting Run: a5f5c99n with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104654-a5f5c99n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-715
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a5f5c99n
wandb:                                                                                
wandb: 🚀 View run warm-sweep-715 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a5f5c99n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104654-a5f5c99n/logs
wandb: Agent Starting Run: earzfq1w with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104659-earzfq1w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-716
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/earzfq1w
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run sage-sweep-716 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/earzfq1w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104659-earzfq1w/logs
Run earzfq1w errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run earzfq1w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.90 GiB is allocated by PyTorch, and 79.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: jl1q34uw with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104705-jl1q34uw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-717
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jl1q34uw
wandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb: | 0.004 MB of 0.004 MB uploadedwandb: / 0.004 MB of 0.004 MB uploadedwandb: - 0.004 MB of 0.004 MB uploadedwandb: \ 0.004 MB of 0.004 MB uploadedwandb:                                                                                
wandb: 🚀 View run sleek-sweep-717 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jl1q34uw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104705-jl1q34uw/logs
wandb: Agent Starting Run: 65j3ma3f with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104715-65j3ma3f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-718
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/65j3ma3f
wandb:                                                                                
wandb: 🚀 View run faithful-sweep-718 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/65j3ma3f
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104715-65j3ma3f/logs
wandb: Agent Starting Run: 9511cm91 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104721-9511cm91
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-719
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9511cm91
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run sweet-sweep-719 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9511cm91
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104721-9511cm91/logs
wandb: Agent Starting Run: 0j7rlpoa with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104726-0j7rlpoa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-720
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0j7rlpoa
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 71.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run rosy-sweep-720 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0j7rlpoa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104726-0j7rlpoa/logs
Run 0j7rlpoa errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 71.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0j7rlpoa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 71.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: zdto8k5g with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104731-zdto8k5g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-721
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zdto8k5g
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 68.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run cerulean-sweep-721 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zdto8k5g
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104731-zdto8k5g/logs
Run zdto8k5g errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 68.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run zdto8k5g errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 68.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 56jb5g75 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104737-56jb5g75
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-722
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/56jb5g75
wandb:                                                                                
wandb: 🚀 View run driven-sweep-722 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/56jb5g75
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104737-56jb5g75/logs
wandb: Agent Starting Run: 1hav93jo with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104742-1hav93jo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-sweep-723
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1hav93jo
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 67.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run gallant-sweep-723 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1hav93jo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104742-1hav93jo/logs
Run 1hav93jo errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 67.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1hav93jo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 67.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: a8s86zpn with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104747-a8s86zpn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-724
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a8s86zpn
wandb:                                                                                
wandb: 🚀 View run whole-sweep-724 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a8s86zpn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104747-a8s86zpn/logs
wandb: Agent Starting Run: qsxum9bj with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104753-qsxum9bj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-725
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qsxum9bj
wandb:                                                                                
wandb: 🚀 View run worldly-sweep-725 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qsxum9bj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104753-qsxum9bj/logs
wandb: Agent Starting Run: ujbjgdbj with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104758-ujbjgdbj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-726
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ujbjgdbj
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run mild-sweep-726 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ujbjgdbj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104758-ujbjgdbj/logs
Run ujbjgdbj errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run ujbjgdbj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: j5vlykoa with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104804-j5vlykoa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-727
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j5vlykoa
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 67.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run glorious-sweep-727 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j5vlykoa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104804-j5vlykoa/logs
Run j5vlykoa errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 67.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run j5vlykoa errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 67.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: qjf11s66 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104809-qjf11s66
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-728
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qjf11s66
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.000 MB of 0.000 MB uploadedwandb: \ 0.000 MB of 0.000 MB uploadedwandb: | 0.000 MB of 0.000 MB uploadedwandb: / 0.000 MB of 0.000 MB uploadedwandb: - 0.000 MB of 0.000 MB uploadedwandb:                                                                                
wandb: 🚀 View run valiant-sweep-728 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qjf11s66
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104809-qjf11s66/logs
Run qjf11s66 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qjf11s66 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 3ga9p0ti with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104819-3ga9p0ti
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-729
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3ga9p0ti
wandb:                                                                                
wandb: 🚀 View run deft-sweep-729 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3ga9p0ti
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104819-3ga9p0ti/logs
wandb: Agent Starting Run: qugcyl9j with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104825-qugcyl9j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-730
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qugcyl9j
wandb:                                                                                
wandb: 🚀 View run leafy-sweep-730 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qugcyl9j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104825-qugcyl9j/logs
wandb: Agent Starting Run: 6m1e9ery with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104830-6m1e9ery
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-731
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6m1e9ery
wandb:                                                                                
wandb: 🚀 View run feasible-sweep-731 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6m1e9ery
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104830-6m1e9ery/logs
wandb: Agent Starting Run: 8q22v6wj with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104836-8q22v6wj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-732
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8q22v6wj
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run leafy-sweep-732 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8q22v6wj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104836-8q22v6wj/logs
wandb: Agent Starting Run: bawry08a with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104841-bawry08a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-733
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bawry08a
wandb:                                                                                
wandb: 🚀 View run efficient-sweep-733 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bawry08a
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104841-bawry08a/logs
wandb: Agent Starting Run: nbao1ed4 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104846-nbao1ed4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-sweep-734
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nbao1ed4
wandb:                                                                                
wandb: 🚀 View run vivid-sweep-734 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nbao1ed4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104846-nbao1ed4/logs
wandb: Agent Starting Run: x4l56d7a with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104852-x4l56d7a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-735
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x4l56d7a
wandb:                                                                                
wandb: 🚀 View run easy-sweep-735 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x4l56d7a
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104852-x4l56d7a/logs
wandb: Agent Starting Run: 8db3jhi3 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104857-8db3jhi3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-736
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8db3jhi3
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 280.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run jolly-sweep-736 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8db3jhi3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104857-8db3jhi3/logs
Run 8db3jhi3 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 280.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8db3jhi3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 280.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: yd59730m with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104902-yd59730m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-737
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yd59730m
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run deft-sweep-737 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yd59730m
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104902-yd59730m/logs
Run yd59730m errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run yd59730m errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 1du1rsdg with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104908-1du1rsdg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-738
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1du1rsdg
wandb:                                                                                
wandb: 🚀 View run smooth-sweep-738 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1du1rsdg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104908-1du1rsdg/logs
wandb: Agent Starting Run: 5k850gn0 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104913-5k850gn0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-739
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5k850gn0
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run electric-sweep-739 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5k850gn0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104913-5k850gn0/logs
Run 5k850gn0 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5k850gn0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 02d8fy4e with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104918-02d8fy4e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-740
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/02d8fy4e
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run olive-sweep-740 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/02d8fy4e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104918-02d8fy4e/logs
Run 02d8fy4e errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 02d8fy4e errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: l9y2sg4w with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104924-l9y2sg4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-741
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l9y2sg4w
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run leafy-sweep-741 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l9y2sg4w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104924-l9y2sg4w/logs
Run l9y2sg4w errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run l9y2sg4w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: a8tphb7k with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104929-a8tphb7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-742
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a8tphb7k
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run trim-sweep-742 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a8tphb7k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104929-a8tphb7k/logs
wandb: Agent Starting Run: zi3zokp0 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104934-zi3zokp0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-743
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zi3zokp0
wandb:                                                                                
wandb: 🚀 View run cool-sweep-743 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zi3zokp0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104934-zi3zokp0/logs
wandb: Agent Starting Run: epzh3poi with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104940-epzh3poi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-744
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/epzh3poi
wandb:                                                                                
wandb: 🚀 View run misty-sweep-744 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/epzh3poi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104940-epzh3poi/logs
wandb: Agent Starting Run: bauhpqjg with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104945-bauhpqjg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-745
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bauhpqjg
wandb:                                                                                
wandb: 🚀 View run daily-sweep-745 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bauhpqjg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104945-bauhpqjg/logs
wandb: Agent Starting Run: ilu6nga2 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104950-ilu6nga2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-746
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ilu6nga2
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run volcanic-sweep-746 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ilu6nga2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104950-ilu6nga2/logs
Run ilu6nga2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ilu6nga2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 09rqud6j with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_104956-09rqud6j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-747
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/09rqud6j
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run vocal-sweep-747 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/09rqud6j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_104956-09rqud6j/logs
Run 09rqud6j errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 09rqud6j errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.91 GiB is allocated by PyTorch, and 63.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: yhg2nebj with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105001-yhg2nebj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-748
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yhg2nebj
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run skilled-sweep-748 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yhg2nebj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105001-yhg2nebj/logs
wandb: Agent Starting Run: fjlcxwwe with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105006-fjlcxwwe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-749
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fjlcxwwe
wandb:                                                                                
wandb: 🚀 View run dainty-sweep-749 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fjlcxwwe
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105006-fjlcxwwe/logs
wandb: Agent Starting Run: l9gf0vh2 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105012-l9gf0vh2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-750
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l9gf0vh2
wandb:                                                                                
wandb: 🚀 View run dainty-sweep-750 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l9gf0vh2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105012-l9gf0vh2/logs
wandb: Agent Starting Run: xhu9aa42 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105017-xhu9aa42
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-751
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xhu9aa42
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run graceful-sweep-751 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xhu9aa42
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105017-xhu9aa42/logs
Run xhu9aa42 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run xhu9aa42 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ia1uxx1k with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105022-ia1uxx1k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-752
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ia1uxx1k
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run polar-sweep-752 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ia1uxx1k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105022-ia1uxx1k/logs
Run ia1uxx1k errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ia1uxx1k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: yvzbtaz5 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105028-yvzbtaz5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-753
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yvzbtaz5
wandb:                                                                                
wandb: 🚀 View run cool-sweep-753 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yvzbtaz5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105028-yvzbtaz5/logs
wandb: Agent Starting Run: ld7p8hl5 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105033-ld7p8hl5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-754
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ld7p8hl5
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run ruby-sweep-754 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ld7p8hl5
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105033-ld7p8hl5/logs
Run ld7p8hl5 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ld7p8hl5 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: uowje369 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105038-uowje369
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-755
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uowje369
wandb:                                                                                
wandb: 🚀 View run magic-sweep-755 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uowje369
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105038-uowje369/logs
wandb: Agent Starting Run: 1n0egtuo with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105043-1n0egtuo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run morning-sweep-756
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1n0egtuo
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run morning-sweep-756 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1n0egtuo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105043-1n0egtuo/logs
Run 1n0egtuo errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1n0egtuo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 1b1hdsnt with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105049-1b1hdsnt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-757
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1b1hdsnt
wandb:                                                                                
wandb: 🚀 View run exalted-sweep-757 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1b1hdsnt
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105049-1b1hdsnt/logs
wandb: Agent Starting Run: psk3cpip with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105054-psk3cpip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-758
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/psk3cpip
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-758 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/psk3cpip
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105054-psk3cpip/logs
Run psk3cpip errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run psk3cpip errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: 5oqa4ay2 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105059-5oqa4ay2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-759
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5oqa4ay2
wandb:                                                                                
wandb: 🚀 View run lemon-sweep-759 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5oqa4ay2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105059-5oqa4ay2/logs
wandb: Agent Starting Run: 5ieer74b with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105105-5ieer74b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-760
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5ieer74b
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run eager-sweep-760 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5ieer74b
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105105-5ieer74b/logs
wandb: Agent Starting Run: q75w9lit with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105110-q75w9lit
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-761
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q75w9lit
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run trim-sweep-761 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q75w9lit
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105110-q75w9lit/logs
Run q75w9lit errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run q75w9lit errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: cnhwzep0 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105115-cnhwzep0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-762
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cnhwzep0
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 470.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run amber-sweep-762 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cnhwzep0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105115-cnhwzep0/logs
Run cnhwzep0 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 470.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run cnhwzep0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 470.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: wy5z1ov8 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105121-wy5z1ov8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-763
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wy5z1ov8
wandb:                                                                                
wandb: 🚀 View run fiery-sweep-763 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wy5z1ov8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105121-wy5z1ov8/logs
wandb: Agent Starting Run: 4unpf3lx with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105126-4unpf3lx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-764
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4unpf3lx
wandb:                                                                                
wandb: 🚀 View run soft-sweep-764 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4unpf3lx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105126-4unpf3lx/logs
wandb: Agent Starting Run: co64t8fa with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105131-co64t8fa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-765
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/co64t8fa
wandb:                                                                                
wandb: 🚀 View run flowing-sweep-765 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/co64t8fa
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105131-co64t8fa/logs
wandb: Agent Starting Run: n2xja191 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105137-n2xja191
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-766
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n2xja191
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 570.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run lunar-sweep-766 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n2xja191
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105137-n2xja191/logs
Run n2xja191 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 570.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run n2xja191 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 570.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.92 GiB is allocated by PyTorch, and 59.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: wdiuiswm with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105142-wdiuiswm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-767
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wdiuiswm
wandb:                                                                                
wandb: 🚀 View run lunar-sweep-767 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wdiuiswm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105142-wdiuiswm/logs
wandb: Agent Starting Run: akyuskcm with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105147-akyuskcm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-768
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/akyuskcm
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run denim-sweep-768 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/akyuskcm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105147-akyuskcm/logs
Run akyuskcm errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run akyuskcm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: eko2yii2 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105153-eko2yii2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-769
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eko2yii2
wandb:                                                                                
wandb: 🚀 View run daily-sweep-769 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eko2yii2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105153-eko2yii2/logs
wandb: Agent Starting Run: byo6hprg with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105158-byo6hprg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-770
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/byo6hprg
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run true-sweep-770 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/byo6hprg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105158-byo6hprg/logs
Run byo6hprg errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run byo6hprg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ek3xhhv7 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105203-ek3xhhv7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-771
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ek3xhhv7
wandb:                                                                                
wandb: 🚀 View run fallen-sweep-771 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ek3xhhv7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105203-ek3xhhv7/logs
wandb: Agent Starting Run: d0lyw795 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105209-d0lyw795
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-772
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d0lyw795
wandb:                                                                                
wandb: 🚀 View run classic-sweep-772 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d0lyw795
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105209-d0lyw795/logs
wandb: Agent Starting Run: a8g06d35 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105214-a8g06d35
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-773
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a8g06d35
wandb:                                                                                
wandb: 🚀 View run worldly-sweep-773 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/a8g06d35
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105214-a8g06d35/logs
wandb: Agent Starting Run: bl1b7dap with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105220-bl1b7dap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-774
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bl1b7dap
wandb:                                                                                
wandb: 🚀 View run different-sweep-774 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bl1b7dap
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105220-bl1b7dap/logs
wandb: Agent Starting Run: 84snqlfq with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105225-84snqlfq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-775
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/84snqlfq
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-775 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/84snqlfq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105225-84snqlfq/logs
Run 84snqlfq errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 84snqlfq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 9.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 51.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: fxcdhe7e with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105230-fxcdhe7e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-776
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fxcdhe7e
wandb:                                                                                
wandb: 🚀 View run deft-sweep-776 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fxcdhe7e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105230-fxcdhe7e/logs
wandb: Agent Starting Run: dvvh7ty2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105235-dvvh7ty2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-777
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dvvh7ty2
wandb:                                                                                
wandb: 🚀 View run expert-sweep-777 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dvvh7ty2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105235-dvvh7ty2/logs
wandb: Agent Starting Run: npkvc5tq with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105241-npkvc5tq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sweep-778
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/npkvc5tq
wandb:                                                                                
wandb: 🚀 View run serene-sweep-778 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/npkvc5tq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105241-npkvc5tq/logs
wandb: Agent Starting Run: lez0727l with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105246-lez0727l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-779
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lez0727l
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run logical-sweep-779 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lez0727l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105246-lez0727l/logs
Run lez0727l errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run lez0727l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: x9drpphm with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105251-x9drpphm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-780
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x9drpphm
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run wobbly-sweep-780 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x9drpphm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105251-x9drpphm/logs
Run x9drpphm errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run x9drpphm errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: w09nwbjk with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105257-w09nwbjk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-781
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w09nwbjk
wandb:                                                                                
wandb: 🚀 View run fancy-sweep-781 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w09nwbjk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105257-w09nwbjk/logs
wandb: Agent Starting Run: 7sr3c1au with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105302-7sr3c1au
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-782
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7sr3c1au
wandb:                                                                                
wandb: 🚀 View run sleek-sweep-782 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7sr3c1au
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105302-7sr3c1au/logs
wandb: Agent Starting Run: 55azzyou with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105307-55azzyou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-783
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/55azzyou
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fresh-sweep-783 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/55azzyou
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105307-55azzyou/logs
wandb: Agent Starting Run: m1qqcbr0 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105313-m1qqcbr0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-784
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m1qqcbr0
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run devoted-sweep-784 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/m1qqcbr0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105313-m1qqcbr0/logs
Run m1qqcbr0 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run m1qqcbr0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 53.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5i48658p with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105318-5i48658p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-785
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5i48658p
wandb:                                                                                
wandb: 🚀 View run comic-sweep-785 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5i48658p
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105318-5i48658p/logs
wandb: Agent Starting Run: 355hif2o with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105323-355hif2o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-786
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/355hif2o
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run classic-sweep-786 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/355hif2o
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105323-355hif2o/logs
Run 355hif2o errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 355hif2o errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f0dbp8in with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105329-f0dbp8in
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-787
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f0dbp8in
wandb:                                                                                
wandb: 🚀 View run restful-sweep-787 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f0dbp8in
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105329-f0dbp8in/logs
wandb: Agent Starting Run: i2swpfne with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105334-i2swpfne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-788
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i2swpfne
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run rural-sweep-788 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i2swpfne
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105334-i2swpfne/logs
Run i2swpfne errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run i2swpfne errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 3ibniry1 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105340-3ibniry1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-789
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3ibniry1
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run solar-sweep-789 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3ibniry1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105340-3ibniry1/logs
Run 3ibniry1 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 3ibniry1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 09r1g5id with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105355-09r1g5id
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-790
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/09r1g5id
wandb:                                                                                
wandb: 🚀 View run laced-sweep-790 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/09r1g5id
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105355-09r1g5id/logs
wandb: Agent Starting Run: ey8sydi1 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105401-ey8sydi1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-791
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ey8sydi1
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run driven-sweep-791 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ey8sydi1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105401-ey8sydi1/logs
Run ey8sydi1 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ey8sydi1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 93sq9ycj with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105406-93sq9ycj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-792
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/93sq9ycj
wandb:                                                                                
wandb: 🚀 View run vague-sweep-792 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/93sq9ycj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105406-93sq9ycj/logs
wandb: Agent Starting Run: i6yhgagp with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105411-i6yhgagp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-793
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i6yhgagp
wandb:                                                                                
wandb: 🚀 View run wild-sweep-793 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i6yhgagp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105411-i6yhgagp/logs
wandb: Agent Starting Run: ylhx02o1 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105417-ylhx02o1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-794
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ylhx02o1
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.21 GiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run stilted-sweep-794 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ylhx02o1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105417-ylhx02o1/logs
Run ylhx02o1 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.21 GiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ylhx02o1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.21 GiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 3wjbfz3q with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105432-3wjbfz3q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-795
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3wjbfz3q
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 522.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run devout-sweep-795 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/3wjbfz3q
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105432-3wjbfz3q/logs
Run 3wjbfz3q errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 522.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 3wjbfz3q errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 522.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: bkea3jy0 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105437-bkea3jy0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-796
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bkea3jy0
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run iconic-sweep-796 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bkea3jy0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105437-bkea3jy0/logs
Run bkea3jy0 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bkea3jy0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: h0zwztah with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105443-h0zwztah
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-797
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h0zwztah
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run olive-sweep-797 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/h0zwztah
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105443-h0zwztah/logs
Run h0zwztah errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run h0zwztah errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: qjkj4nbf with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105448-qjkj4nbf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-798
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qjkj4nbf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run twilight-sweep-798 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qjkj4nbf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105448-qjkj4nbf/logs
Run qjkj4nbf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qjkj4nbf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 4mve90xt with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105453-4mve90xt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-799
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4mve90xt
wandb:                                                                                
wandb: 🚀 View run mild-sweep-799 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4mve90xt
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105453-4mve90xt/logs
wandb: Agent Starting Run: qcz9t8s4 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105459-qcz9t8s4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-800
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qcz9t8s4
wandb:                                                                                
wandb: 🚀 View run visionary-sweep-800 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qcz9t8s4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105459-qcz9t8s4/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: lz3q8kr8 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105512-lz3q8kr8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-801
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lz3q8kr8
wandb:                                                                                
wandb: 🚀 View run lively-sweep-801 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lz3q8kr8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105512-lz3q8kr8/logs
wandb: Agent Starting Run: oriiws8o with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105521-oriiws8o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-802
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oriiws8o
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run fresh-sweep-802 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oriiws8o
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105521-oriiws8o/logs
Run oriiws8o errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run oriiws8o errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: n1x2kin9 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105526-n1x2kin9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-803
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n1x2kin9
wandb:                                                                                
wandb: 🚀 View run robust-sweep-803 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n1x2kin9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105526-n1x2kin9/logs
wandb: Agent Starting Run: 1qbyy9h9 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105531-1qbyy9h9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-804
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1qbyy9h9
wandb:                                                                                
wandb: 🚀 View run tough-sweep-804 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1qbyy9h9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105531-1qbyy9h9/logs
wandb: Agent Starting Run: 6mk59nnf with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105537-6mk59nnf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-805
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6mk59nnf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run jolly-sweep-805 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6mk59nnf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105537-6mk59nnf/logs
Run 6mk59nnf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6mk59nnf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 530.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.93 GiB is allocated by PyTorch, and 52.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 1iocub8p with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105542-1iocub8p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-sweep-806
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1iocub8p
wandb:                                                                                
wandb: 🚀 View run cerulean-sweep-806 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1iocub8p
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105542-1iocub8p/logs
wandb: Agent Starting Run: 7gasqzkk with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105547-7gasqzkk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-sweep-807
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7gasqzkk
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 40.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run giddy-sweep-807 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7gasqzkk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105547-7gasqzkk/logs
Run 7gasqzkk errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 40.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7gasqzkk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 40.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: w6kqv7zm with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105603-w6kqv7zm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-808
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w6kqv7zm
wandb:                                                                                
wandb: 🚀 View run spring-sweep-808 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w6kqv7zm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105603-w6kqv7zm/logs
wandb: Agent Starting Run: kry8wofg with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105608-kry8wofg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-809
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kry8wofg
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run volcanic-sweep-809 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kry8wofg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105608-kry8wofg/logs
Run kry8wofg errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run kry8wofg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: etclu11n with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105613-etclu11n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-810
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/etclu11n
wandb:                                                                                
wandb: 🚀 View run floral-sweep-810 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/etclu11n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105613-etclu11n/logs
wandb: Agent Starting Run: unwf78rb with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105619-unwf78rb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-811
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/unwf78rb
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run lilac-sweep-811 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/unwf78rb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105619-unwf78rb/logs
wandb: Agent Starting Run: g7c8gpxw with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105624-g7c8gpxw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-812
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g7c8gpxw
wandb:                                                                                
wandb: 🚀 View run sunny-sweep-812 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g7c8gpxw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105624-g7c8gpxw/logs
wandb: Agent Starting Run: s7ugfqmn with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105629-s7ugfqmn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-813
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s7ugfqmn
wandb:                                                                                
wandb: 🚀 View run whole-sweep-813 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s7ugfqmn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105629-s7ugfqmn/logs
wandb: Agent Starting Run: 0e8oyh4w with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105635-0e8oyh4w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-814
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0e8oyh4w
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 40.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run spring-sweep-814 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0e8oyh4w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105635-0e8oyh4w/logs
Run 0e8oyh4w errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 40.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0e8oyh4w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 40.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ugsg851x with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105640-ugsg851x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-815
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ugsg851x
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run deep-sweep-815 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ugsg851x
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105640-ugsg851x/logs
wandb: Agent Starting Run: 102sbdl2 with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105645-102sbdl2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-816
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/102sbdl2
wandb:                                                                                
wandb: 🚀 View run icy-sweep-816 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/102sbdl2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105645-102sbdl2/logs
wandb: Agent Starting Run: erf67b40 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105651-erf67b40
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-817
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/erf67b40
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.95 GiB is allocated by PyTorch, and 34.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run drawn-sweep-817 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/erf67b40
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105651-erf67b40/logs
Run erf67b40 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.95 GiB is allocated by PyTorch, and 34.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run erf67b40 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.95 GiB is allocated by PyTorch, and 34.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: s81r6kom with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105656-s81r6kom
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-818
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s81r6kom
wandb:                                                                                
wandb: 🚀 View run breezy-sweep-818 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/s81r6kom
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105656-s81r6kom/logs
wandb: Agent Starting Run: i4fb9kc9 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105701-i4fb9kc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-819
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i4fb9kc9
wandb:                                                                                
wandb: 🚀 View run eager-sweep-819 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i4fb9kc9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105701-i4fb9kc9/logs
wandb: Agent Starting Run: 66t70yoy with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105707-66t70yoy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-820
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/66t70yoy
wandb:                                                                                
wandb: 🚀 View run rural-sweep-820 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/66t70yoy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105707-66t70yoy/logs
wandb: Agent Starting Run: 1lo73xin with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105712-1lo73xin
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-821
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1lo73xin
wandb:                                                                                
wandb: 🚀 View run royal-sweep-821 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1lo73xin
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105712-1lo73xin/logs
wandb: Agent Starting Run: 9k5ipqqx with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105718-9k5ipqqx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-822
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9k5ipqqx
wandb:                                                                                
wandb: 🚀 View run sweepy-sweep-822 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9k5ipqqx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105718-9k5ipqqx/logs
wandb: Agent Starting Run: 95itwurs with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105723-95itwurs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-823
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/95itwurs
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [32, -32]
wandb:                                                                                
wandb: 🚀 View run feasible-sweep-823 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/95itwurs
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105723-95itwurs/logs
Run 95itwurs errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -32: [32, -32]

wandb: ERROR Run 95itwurs errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -32: [32, -32]
wandb: ERROR 
wandb: Agent Starting Run: lpfuiicx with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105728-lpfuiicx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-824
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lpfuiicx
wandb:                                                                                
wandb: 🚀 View run happy-sweep-824 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lpfuiicx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105728-lpfuiicx/logs
wandb: Agent Starting Run: zv2ufbe7 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105734-zv2ufbe7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-825
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zv2ufbe7
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.95 GiB is allocated by PyTorch, and 34.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run twilight-sweep-825 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/zv2ufbe7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105734-zv2ufbe7/logs
Run zv2ufbe7 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.95 GiB is allocated by PyTorch, and 34.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run zv2ufbe7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.95 GiB is allocated by PyTorch, and 34.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: n5n46of2 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105739-n5n46of2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-826
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n5n46of2
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 21.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run summer-sweep-826 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n5n46of2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105739-n5n46of2/logs
Run n5n46of2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 21.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run n5n46of2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 21.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 96b3x5vq with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105744-96b3x5vq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-827
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/96b3x5vq
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run flowing-sweep-827 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/96b3x5vq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105744-96b3x5vq/logs
Run 96b3x5vq errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 96b3x5vq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2y264svy with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105749-2y264svy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-828
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2y264svy
wandb:                                                                                
wandb: 🚀 View run lemon-sweep-828 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2y264svy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105749-2y264svy/logs
wandb: Agent Starting Run: r93pvscf with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105755-r93pvscf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-829
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r93pvscf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run driven-sweep-829 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r93pvscf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105755-r93pvscf/logs
Run r93pvscf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run r93pvscf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: hyceu73j with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105800-hyceu73j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-830
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hyceu73j
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run astral-sweep-830 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hyceu73j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105800-hyceu73j/logs
Run hyceu73j errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run hyceu73j errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: vfd5k31u with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105806-vfd5k31u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-831
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vfd5k31u
wandb:                                                                                
wandb: 🚀 View run laced-sweep-831 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vfd5k31u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105806-vfd5k31u/logs
wandb: Agent Starting Run: p9jxrh0e with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105811-p9jxrh0e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-832
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p9jxrh0e
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -96: [64, -96]
wandb:                                                                                
wandb: 🚀 View run sparkling-sweep-832 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p9jxrh0e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105811-p9jxrh0e/logs
Run p9jxrh0e errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -96: [64, -96]

wandb: ERROR Run p9jxrh0e errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -96: [64, -96]
wandb: ERROR 
wandb: Agent Starting Run: oeiayg1j with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105816-oeiayg1j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-833
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oeiayg1j
wandb:                                                                                
wandb: 🚀 View run ruby-sweep-833 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/oeiayg1j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105816-oeiayg1j/logs
wandb: Agent Starting Run: mudf45es with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105822-mudf45es
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-834
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mudf45es
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run happy-sweep-834 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mudf45es
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105822-mudf45es/logs
Run mudf45es errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run mudf45es errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5puzp57a with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105827-5puzp57a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stilted-sweep-835
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5puzp57a
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run stilted-sweep-835 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5puzp57a
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105827-5puzp57a/logs
wandb: Agent Starting Run: l87rvek6 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105832-l87rvek6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-836
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l87rvek6
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run glad-sweep-836 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l87rvek6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105832-l87rvek6/logs
wandb: Agent Starting Run: ez0pogdu with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105838-ez0pogdu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-837
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ez0pogdu
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 268.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run kind-sweep-837 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ez0pogdu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105838-ez0pogdu/logs
Run ez0pogdu errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 268.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ez0pogdu errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 268.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 17.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: d9yag6f2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105843-d9yag6f2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-838
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d9yag6f2
wandb:                                                                                
wandb: 🚀 View run icy-sweep-838 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/d9yag6f2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105843-d9yag6f2/logs
wandb: Agent Starting Run: skw98j0v with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105848-skw98j0v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-839
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/skw98j0v
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run fresh-sweep-839 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/skw98j0v
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105848-skw98j0v/logs
wandb: Agent Starting Run: j0aiotjk with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105853-j0aiotjk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-840
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j0aiotjk
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run stellar-sweep-840 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j0aiotjk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105853-j0aiotjk/logs
Run j0aiotjk errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run j0aiotjk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: 167dzlyw with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105859-167dzlyw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-841
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/167dzlyw
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 16.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run noble-sweep-841 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/167dzlyw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105859-167dzlyw/logs
Run 167dzlyw errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 16.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 167dzlyw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 16.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: w8o1nxzb with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105904-w8o1nxzb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-842
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w8o1nxzb
wandb:                                                                                
wandb: 🚀 View run dandy-sweep-842 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w8o1nxzb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105904-w8o1nxzb/logs
wandb: Agent Starting Run: u3bwawco with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105909-u3bwawco
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-843
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u3bwawco
wandb:                                                                                
wandb: 🚀 View run celestial-sweep-843 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/u3bwawco
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105909-u3bwawco/logs
wandb: Agent Starting Run: p52yi13s with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105915-p52yi13s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-sweep-844
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p52yi13s
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 14.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run breezy-sweep-844 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p52yi13s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105915-p52yi13s/logs
Run p52yi13s errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 14.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run p52yi13s errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 14.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 32gsgcnp with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105925-32gsgcnp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-sweep-845
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/32gsgcnp
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 14.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run kind-sweep-845 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/32gsgcnp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105925-32gsgcnp/logs
Run 32gsgcnp errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 14.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 32gsgcnp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 14.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ah70xuy9 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105931-ah70xuy9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-846
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ah70xuy9
wandb:                                                                                
wandb: 🚀 View run dandy-sweep-846 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ah70xuy9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105931-ah70xuy9/logs
wandb: Agent Starting Run: y5pywwxc with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105936-y5pywwxc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-847
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y5pywwxc
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run zesty-sweep-847 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y5pywwxc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105936-y5pywwxc/logs
Run y5pywwxc errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run y5pywwxc errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 7.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7k36orec with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105941-7k36orec
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-848
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7k36orec
wandb:                                                                                
wandb: 🚀 View run magic-sweep-848 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7k36orec
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105941-7k36orec/logs
wandb: Agent Starting Run: 1lyl5llq with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105946-1lyl5llq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-849
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1lyl5llq
wandb:                                                                                
wandb: 🚀 View run swept-sweep-849 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1lyl5llq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105946-1lyl5llq/logs
wandb: Agent Starting Run: g3nnrgp3 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_105952-g3nnrgp3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-850
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g3nnrgp3
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 15.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run deft-sweep-850 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g3nnrgp3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_105952-g3nnrgp3/logs
Run g3nnrgp3 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 15.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run g3nnrgp3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 15.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: aw5larxq with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110002-aw5larxq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-851
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aw5larxq
wandb:                                                                                
wandb: 🚀 View run swift-sweep-851 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aw5larxq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110002-aw5larxq/logs
wandb: Agent Starting Run: mtfixooh with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110008-mtfixooh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-852
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mtfixooh
wandb:                                                                                
wandb: 🚀 View run sunny-sweep-852 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mtfixooh
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110008-mtfixooh/logs
wandb: Agent Starting Run: x9wf4j11 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110013-x9wf4j11
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-853
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x9wf4j11
wandb:                                                                                
wandb: 🚀 View run apricot-sweep-853 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x9wf4j11
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110013-x9wf4j11/logs
wandb: Agent Starting Run: exeboyqr with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110018-exeboyqr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-854
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/exeboyqr
wandb:                                                                                
wandb: 🚀 View run spring-sweep-854 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/exeboyqr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110018-exeboyqr/logs
wandb: Agent Starting Run: 0dsygbfq with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110024-0dsygbfq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-855
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0dsygbfq
wandb:                                                                                
wandb: 🚀 View run pretty-sweep-855 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0dsygbfq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110024-0dsygbfq/logs
wandb: Agent Starting Run: l733wnbd with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110029-l733wnbd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-856
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l733wnbd
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 15.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run polished-sweep-856 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l733wnbd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110029-l733wnbd/logs
Run l733wnbd errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 15.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run l733wnbd errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 15.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: rdgj5d4k with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110044-rdgj5d4k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-857
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rdgj5d4k
wandb:                                                                                
wandb: 🚀 View run proud-sweep-857 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rdgj5d4k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110044-rdgj5d4k/logs
wandb: Agent Starting Run: 1vy4dm4z with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110050-1vy4dm4z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-858
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1vy4dm4z
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run swift-sweep-858 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1vy4dm4z
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110050-1vy4dm4z/logs
Run 1vy4dm4z errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1vy4dm4z errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: grqsgp7n with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110100-grqsgp7n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-859
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/grqsgp7n
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run polar-sweep-859 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/grqsgp7n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110100-grqsgp7n/logs
Run grqsgp7n errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run grqsgp7n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 4e2a9b9g with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110111-4e2a9b9g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-860
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4e2a9b9g
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -96: [1, -96]
wandb:                                                                                
wandb: 🚀 View run fancy-sweep-860 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4e2a9b9g
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110111-4e2a9b9g/logs
Run 4e2a9b9g errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
    fc_layers.append(nn.Linear(input_dim, 1))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -96: [1, -96]

wandb: ERROR Run 4e2a9b9g errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 72, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, 1))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -96: [1, -96]
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: pjwrypbe with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110124-pjwrypbe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-sweep-861
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pjwrypbe
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 660.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run spring-sweep-861 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pjwrypbe
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110124-pjwrypbe/logs
Run pjwrypbe errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 660.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run pjwrypbe errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 660.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: dq1r5rh0 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110139-dq1r5rh0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-862
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dq1r5rh0
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run major-sweep-862 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dq1r5rh0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110139-dq1r5rh0/logs
wandb: Agent Starting Run: xdbxjz72 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110144-xdbxjz72
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sweep-863
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xdbxjz72
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run sage-sweep-863 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xdbxjz72
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110144-xdbxjz72/logs
wandb: Agent Starting Run: isaqo8ot with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110150-isaqo8ot
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-864
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/isaqo8ot
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run deep-sweep-864 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/isaqo8ot
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110150-isaqo8ot/logs
Run isaqo8ot errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run isaqo8ot errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: bkhqjspk with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110205-bkhqjspk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-865
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bkhqjspk
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run proud-sweep-865 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bkhqjspk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110205-bkhqjspk/logs
Run bkhqjspk errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bkhqjspk errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ak094sbq with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110211-ak094sbq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-866
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ak094sbq
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run chocolate-sweep-866 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ak094sbq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110211-ak094sbq/logs
Run ak094sbq errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ak094sbq errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: dgunpve6 with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110216-dgunpve6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-867
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dgunpve6
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run lilac-sweep-867 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dgunpve6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110216-dgunpve6/logs
Run dgunpve6 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run dgunpve6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: xvu7lb7s with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110222-xvu7lb7s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-868
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xvu7lb7s
wandb:                                                                                
wandb: 🚀 View run dandy-sweep-868 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xvu7lb7s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110222-xvu7lb7s/logs
wandb: Agent Starting Run: 1xdaio36 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110227-1xdaio36
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-869
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1xdaio36
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:499: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
wandb:                                                                                
wandb: 🚀 View run earthy-sweep-869 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1xdaio36
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110227-1xdaio36/logs
wandb: Agent Starting Run: gguh53st with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110232-gguh53st
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-870
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gguh53st
wandb:                                                                                
wandb: 🚀 View run radiant-sweep-870 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gguh53st
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110232-gguh53st/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: k1ptyhi7 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110306-k1ptyhi7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-871
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k1ptyhi7
wandb:                                                                                
wandb: 🚀 View run efficient-sweep-871 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k1ptyhi7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110306-k1ptyhi7/logs
wandb: Agent Starting Run: 6p7q3598 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110314-6p7q3598
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-872
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6p7q3598
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run daily-sweep-872 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6p7q3598
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110314-6p7q3598/logs
Run 6p7q3598 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 6p7q3598 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: pqr8rmdu with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110320-pqr8rmdu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-873
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pqr8rmdu
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 268.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run dainty-sweep-873 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pqr8rmdu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110320-pqr8rmdu/logs
Run pqr8rmdu errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 268.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run pqr8rmdu errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 268.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: rhbf3099 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110325-rhbf3099
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-874
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rhbf3099
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run dark-sweep-874 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rhbf3099
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110325-rhbf3099/logs
Run rhbf3099 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run rhbf3099 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: dehgyl8n with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110330-dehgyl8n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-875
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dehgyl8n
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run twilight-sweep-875 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dehgyl8n
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110330-dehgyl8n/logs
Run dehgyl8n errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run dehgyl8n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: t2p8v8dc with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110336-t2p8v8dc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sweep-876
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t2p8v8dc
wandb:                                                                                
wandb: 🚀 View run blooming-sweep-876 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t2p8v8dc
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110336-t2p8v8dc/logs
wandb: Agent Starting Run: tdccch2m with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110341-tdccch2m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-877
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tdccch2m
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run chocolate-sweep-877 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tdccch2m
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110341-tdccch2m/logs
Run tdccch2m errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run tdccch2m errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.09 GiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: gg7m1xju with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110351-gg7m1xju
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-sweep-878
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gg7m1xju
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb:                                                                                
wandb: 🚀 View run revived-sweep-878 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gg7m1xju
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110351-gg7m1xju/logs
Run gg7m1xju errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run gg7m1xju errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: er0sxqx2 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110402-er0sxqx2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-879
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/er0sxqx2
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run restful-sweep-879 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/er0sxqx2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110402-er0sxqx2/logs
Run er0sxqx2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run er0sxqx2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 62929r4r with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 5x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110407-62929r4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-sweep-880
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/62929r4r
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run glad-sweep-880 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/62929r4r
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110407-62929r4r/logs
Run 62929r4r errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 62929r4r errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 8gdnthiw with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110422-8gdnthiw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-881
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8gdnthiw
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.003 MB of 0.003 MB uploadedwandb: \ 0.003 MB of 0.003 MB uploadedwandb: | 0.003 MB of 0.003 MB uploadedwandb:                                                                                
wandb: 🚀 View run dainty-sweep-881 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8gdnthiw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110422-8gdnthiw/logs
Run 8gdnthiw errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8gdnthiw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: eeeqo4fx with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110437-eeeqo4fx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-882
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eeeqo4fx
wandb:                                                                                
wandb: 🚀 View run fancy-sweep-882 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/eeeqo4fx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110437-eeeqo4fx/logs
wandb: Agent Starting Run: io8fnv4v with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 2
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110443-io8fnv4v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-883
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/io8fnv4v
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run deep-sweep-883 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/io8fnv4v
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110443-io8fnv4v/logs
Run io8fnv4v errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run io8fnv4v errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 455rvky0 with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110459-455rvky0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-884
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/455rvky0
wandb:                                                                                
wandb: 🚀 View run happy-sweep-884 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/455rvky0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110459-455rvky0/logs
wandb: Agent Starting Run: wyz1r492 with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x1
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110504-wyz1r492
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-sweep-885
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wyz1r492
wandb:                                                                                
wandb: 🚀 View run misty-sweep-885 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wyz1r492
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110504-wyz1r492/logs
wandb: Agent Starting Run: 6u2va3di with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x1
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110510-6u2va3di
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-886
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6u2va3di
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run confused-sweep-886 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6u2va3di
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110510-6u2va3di/logs
Run 6u2va3di errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6u2va3di errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 11.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: z2ws580k with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110515-z2ws580k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-887
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z2ws580k
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run rosy-sweep-887 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/z2ws580k
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110515-z2ws580k/logs
Run z2ws580k errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run z2ws580k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: j0vozvmf with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 3x7
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110520-j0vozvmf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-888
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j0vozvmf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run warm-sweep-888 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/j0vozvmf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110520-j0vozvmf/logs
Run j0vozvmf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run j0vozvmf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 13.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: bylkxma8 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110531-bylkxma8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-889
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bylkxma8
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run fluent-sweep-889 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bylkxma8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110531-bylkxma8/logs
Run bylkxma8 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bylkxma8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2w7fkvg9 with config:
wandb: 	activation_fn: Swish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 1x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: Adam
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110536-2w7fkvg9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-890
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2w7fkvg9
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 618.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run different-sweep-890 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2w7fkvg9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110536-2w7fkvg9/logs
Run 2w7fkvg9 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 618.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2w7fkvg9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 618.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: y3mhqqdn with config:
wandb: 	activation_fn: Sigmoid
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.5
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdamW
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110542-y3mhqqdn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-891
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y3mhqqdn
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-891 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/y3mhqqdn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110542-y3mhqqdn/logs
wandb: Agent Starting Run: 7as0s83m with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.5
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: Adam
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110547-7as0s83m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-892
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7as0s83m
wandb:                                                                                
wandb: 🚀 View run ancient-sweep-892 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7as0s83m
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110547-7as0s83m/logs
wandb: Agent Starting Run: 6va45xcw with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 2
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110552-6va45xcw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-sweep-893
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6va45xcw
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run jumping-sweep-893 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6va45xcw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110552-6va45xcw/logs
Run 6va45xcw errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6va45xcw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: plffdv4i with config:
wandb: 	activation_fn: Mish
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 1x7
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 8
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110557-plffdv4i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-894
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/plffdv4i
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run fallen-sweep-894 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/plffdv4i
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110557-plffdv4i/logs
Run plffdv4i errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run plffdv4i errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ga6fykmz with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.8
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 4
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110603-ga6fykmz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-895
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ga6fykmz
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run magic-sweep-895 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ga6fykmz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110603-ga6fykmz/logs
Run ga6fykmz errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ga6fykmz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 235, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: mn03xrho with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 32
wandb: 	kernel_size: 5x5
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0.0001
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110613-mn03xrho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-896
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mn03xrho
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run exalted-sweep-896 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mn03xrho
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110613-mn03xrho/logs
Run mn03xrho errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run mn03xrho errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 3.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.97 GiB is allocated by PyTorch, and 12.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 6j8gr83c with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.4
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Xavier_normal
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110618-6j8gr83c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-sweep-897
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6j8gr83c
wandb:                                                                                
wandb: 🚀 View run efficient-sweep-897 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6j8gr83c
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110618-6j8gr83c/logs
wandb: Agent Starting Run: ygxbjrxx with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.1
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.2
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: AdaGrad
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 2
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.001
wandb: 	weights_init: Kaiming_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110624-ygxbjrxx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-sweep-898
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ygxbjrxx
wandb:                                                                                
wandb: 🚀 View run golden-sweep-898 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ygxbjrxx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110624-ygxbjrxx/logs
wandb: Agent Starting Run: 9qnbl84b with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 5x3
wandb: 	learning_rate: 1e-06
wandb: 	linear_dropout: 0.4
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110629-9qnbl84b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-899
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9qnbl84b
wandb:                                                                                
wandb: 🚀 View run valiant-sweep-899 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9qnbl84b
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110629-9qnbl84b/logs
wandb: Agent Starting Run: nfaog6ye with config:
wandb: 	activation_fn: Tanh
wandb: 	batch_size: 64
wandb: 	conv_dropout: 0.3
wandb: 	epochs: 50
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x5
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0.3
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 1
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 2
wandb: 	pooling_size: 4
wandb: 	stride: 3
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: False
wandb: 	weight_decay: 0.01
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110635-nfaog6ye
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-900
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nfaog6ye
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 🚀 View run distinctive-sweep-900 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nfaog6ye
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_110635-nfaog6ye/logs
Run nfaog6ye errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run nfaog6ye errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 144, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 126, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: tinrni4o with config:
wandb: 	activation_fn: LeakyReLU
wandb: 	batch_size: 16
wandb: 	conv_dropout: 0.2
wandb: 	epochs: 50
wandb: 	hidden_units: 128
wandb: 	kernel_size: 7x7
wandb: 	learning_rate: 1e-05
wandb: 	linear_dropout: 0.1
wandb: 	momentum: 0.7
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 3
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: False
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 0
wandb: 	weights_init: Xavier_uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241122_110640-tinrni4o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-901
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/deyj4pby
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tinrni4o
/zhome/fb/0/212723/.lsbatch/1732228434.23241213.shell: line 21: 841067 Killed                  python3 ~/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py > log/modular_train_test$(date +"%d-%m-%y")_$(date +'%H:%M:%S').log
