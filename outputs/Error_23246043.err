Loaded module: cuda/11.8
Loaded dependency [python3/3.10.13]: sqlite3/3.45.1
Loaded dependency [python3/3.10.13]: gcc/12.3.0-binutils-2.40
Loaded module: python3/3.10.13

Loading python3/3.10.13
  Loading requirement: sqlite3/3.45.1 gcc/12.3.0-binutils-2.40
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: raimo-sieber (raimo-sieber-technical-university-of-munich). Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: 2zmftz6j with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_193458-2zmftz6j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-1
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2zmftz6j
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 33770249
wandb:       train_loss 1.6356
wandb:       train_rmse 1.27891
wandb:  validation_loss 5.96887
wandb:  validation_rmse 2.44313
wandb: 
wandb: 🚀 View run pleasant-sweep-1 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2zmftz6j
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_193458-2zmftz6j/logs
wandb: Agent Starting Run: hgo25for with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_194024-hgo25for
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-2
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hgo25for
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8444681
wandb:       train_loss 1.77016
wandb:       train_rmse 1.33047
wandb:  validation_loss 5.43266
wandb:  validation_rmse 2.33081
wandb: 
wandb: 🚀 View run earthy-sweep-2 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/hgo25for
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_194024-hgo25for/logs
wandb: Agent Starting Run: kv8vnl71 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_194443-kv8vnl71
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-sweep-3
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kv8vnl71
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8444681
wandb:       train_loss 1.66281
wandb:       train_rmse 1.2895
wandb:  validation_loss 4.69083
wandb:  validation_rmse 2.16583
wandb: 
wandb: 🚀 View run fast-sweep-3 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kv8vnl71
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_194443-kv8vnl71/logs
wandb: Agent Starting Run: gaseiuoq with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_194901-gaseiuoq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-sweep-4
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gaseiuoq
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 2113289
wandb:       train_loss 2.08616
wandb:       train_rmse 1.44436
wandb:  validation_loss 5.27396
wandb:  validation_rmse 2.29651
wandb: 
wandb: 🚀 View run volcanic-sweep-4 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gaseiuoq
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_194901-gaseiuoq/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9mj0eqs6 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_195324-9mj0eqs6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-5
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9mj0eqs6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 34784009
wandb:       train_loss 1.64705
wandb:       train_rmse 1.28338
wandb:  validation_loss 4.9268
wandb:  validation_rmse 2.21964
wandb: 
wandb: 🚀 View run peach-sweep-5 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9mj0eqs6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_195324-9mj0eqs6/logs
wandb: Agent Starting Run: xl9glxas with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_195756-xl9glxas
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-sweep-6
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xl9glxas
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8698121
wandb:       train_loss 1.4335
wandb:       train_rmse 1.19729
wandb:  validation_loss 4.32968
wandb:  validation_rmse 2.08079
wandb: 
wandb: 🚀 View run likely-sweep-6 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xl9glxas
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_195756-xl9glxas/logs
wandb: Agent Starting Run: 4du99px0 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_200214-4du99px0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-7
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4du99px0
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8698121
wandb:       train_loss 1.73915
wandb:       train_rmse 1.31877
wandb:  validation_loss 5.32885
wandb:  validation_rmse 2.30843
wandb: 
wandb: 🚀 View run grateful-sweep-7 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4du99px0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_200214-4du99px0/logs
wandb: Agent Starting Run: an6m1qfv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_200633-an6m1qfv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-8
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/an6m1qfv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▄▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 2113289
wandb:       train_loss 1.58134
wandb:       train_rmse 1.25751
wandb:  validation_loss 4.25289
wandb:  validation_rmse 2.06225
wandb: 
wandb: 🚀 View run good-sweep-8 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/an6m1qfv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_200633-an6m1qfv/logs
wandb: Agent Starting Run: 1b2q6zb6 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_201046-1b2q6zb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-9
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1b2q6zb6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 67538129
wandb:       train_loss 2.09837
wandb:       train_rmse 1.44858
wandb:  validation_loss 5.76092
wandb:  validation_rmse 2.40019
wandb: 
wandb: 🚀 View run deep-sweep-9 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/1b2q6zb6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_201046-1b2q6zb6/logs
wandb: Agent Starting Run: e7nbju20 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_201536-e7nbju20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-10
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e7nbju20
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 16886993
wandb:       train_loss 2.45718
wandb:       train_rmse 1.56754
wandb:  validation_loss 4.60454
wandb:  validation_rmse 2.14582
wandb: 
wandb: 🚀 View run lemon-sweep-10 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e7nbju20
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_201536-e7nbju20/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: o7wtt1mk with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_202001-o7wtt1mk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-11
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o7wtt1mk
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 16886993
wandb:       train_loss 2.08333
wandb:       train_rmse 1.44338
wandb:  validation_loss 4.35497
wandb:  validation_rmse 2.08686
wandb: 
wandb: 🚀 View run dulcet-sweep-11 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o7wtt1mk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_202001-o7wtt1mk/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: p8efwvyr with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_202430-p8efwvyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-12
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p8efwvyr
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▄▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 4224209
wandb:       train_loss 2.17755
wandb:       train_rmse 1.47565
wandb:  validation_loss 4.58906
wandb:  validation_rmse 2.14221
wandb: 
wandb: 🚀 View run earthy-sweep-12 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p8efwvyr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_202430-p8efwvyr/logs
wandb: Agent Starting Run: aso0b64e with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_202844-aso0b64e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-13
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aso0b64e
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 69565649
wandb:       train_loss 2.23148
wandb:       train_rmse 1.49381
wandb:  validation_loss 4.62904
wandb:  validation_rmse 2.15152
wandb: 
wandb: 🚀 View run pleasant-sweep-13 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aso0b64e
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_202844-aso0b64e/logs
wandb: Agent Starting Run: fidw3ikv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_203334-fidw3ikv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-14
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fidw3ikv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 17393873
wandb:       train_loss 1.91099
wandb:       train_rmse 1.38238
wandb:  validation_loss 5.14935
wandb:  validation_rmse 2.26922
wandb: 
wandb: 🚀 View run sweet-sweep-14 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/fidw3ikv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_203334-fidw3ikv/logs
wandb: Agent Starting Run: aexqvnve with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_203752-aexqvnve
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-sweep-15
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aexqvnve
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 17393873
wandb:       train_loss 1.79981
wandb:       train_rmse 1.34157
wandb:  validation_loss 5.03869
wandb:  validation_rmse 2.2447
wandb: 
wandb: 🚀 View run effortless-sweep-15 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/aexqvnve
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_203752-aexqvnve/logs
wandb: Agent Starting Run: nox63opu with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_204217-nox63opu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-16
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nox63opu
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 4224209
wandb:       train_loss 1.91486
wandb:       train_rmse 1.38378
wandb:  validation_loss 4.58118
wandb:  validation_rmse 2.14037
wandb: 
wandb: 🚀 View run divine-sweep-16 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nox63opu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_204217-nox63opu/logs
wandb: Agent Starting Run: odbyxzoe with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_204630-odbyxzoe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-sweep-17
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/odbyxzoe
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 135073889
wandb:       train_loss 1.6114
wandb:       train_rmse 1.26941
wandb:  validation_loss 5.71109
wandb:  validation_rmse 2.38979
wandb: 
wandb: 🚀 View run wobbly-sweep-17 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/odbyxzoe
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_204630-odbyxzoe/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 10143bxg with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_205146-10143bxg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-18
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/10143bxg
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 33771617
wandb:       train_loss 1.85283
wandb:       train_rmse 1.36119
wandb:  validation_loss 6.24417
wandb:  validation_rmse 2.49883
wandb: 
wandb: 🚀 View run lilac-sweep-18 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/10143bxg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_205146-10143bxg/logs
wandb: Agent Starting Run: bf0gfttx with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_205618-bf0gfttx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sweep-19
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bf0gfttx
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 33771617
wandb:       train_loss 1.99643
wandb:       train_rmse 1.41295
wandb:  validation_loss 5.65745
wandb:  validation_rmse 2.37854
wandb: 
wandb: 🚀 View run glorious-sweep-19 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bf0gfttx
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_205618-bf0gfttx/logs
wandb: Agent Starting Run: tt897pxd with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_210049-tt897pxd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-20
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tt897pxd
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8446049
wandb:       train_loss 1.95462
wandb:       train_rmse 1.39808
wandb:  validation_loss 4.73772
wandb:  validation_rmse 2.17663
wandb: 
wandb: 🚀 View run stellar-sweep-20 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tt897pxd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_210049-tt897pxd/logs
wandb: Agent Starting Run: cjlg63q6 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_210508-cjlg63q6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-sweep-21
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cjlg63q6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 139128929
wandb:       train_loss 2.42368
wandb:       train_rmse 1.55682
wandb:  validation_loss 5.40062
wandb:  validation_rmse 2.32392
wandb: 
wandb: 🚀 View run eternal-sweep-21 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cjlg63q6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_210508-cjlg63q6/logs
wandb: Agent Starting Run: evq02clw with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_211046-evq02clw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-22
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/evq02clw
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 34785377
wandb:       train_loss 1.77386
wandb:       train_rmse 1.33186
wandb:  validation_loss 5.9794
wandb:  validation_rmse 2.44528
wandb: 
wandb: 🚀 View run light-sweep-22 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/evq02clw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_211046-evq02clw/logs
wandb: Agent Starting Run: ph3fuquk with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_211523-ph3fuquk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-23
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ph3fuquk
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 34785377
wandb:       train_loss 2.5428
wandb:       train_rmse 1.59462
wandb:  validation_loss 4.81001
wandb:  validation_rmse 2.19317
wandb: 
wandb: 🚀 View run zany-sweep-23 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ph3fuquk
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_211523-ph3fuquk/logs
wandb: Agent Starting Run: 4lttl3is with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_211954-4lttl3is
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-24
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4lttl3is
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8446049
wandb:       train_loss 2.03269
wandb:       train_rmse 1.42572
wandb:  validation_loss 4.25431
wandb:  validation_rmse 2.0626
wandb: 
wandb: 🚀 View run frosty-sweep-24 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4lttl3is
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_211954-4lttl3is/logs
wandb: Agent Starting Run: qbw9r2q7 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_212413-qbw9r2q7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-25
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qbw9r2q7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 270145409
wandb:       train_loss 2.39794
wandb:       train_rmse 1.54853
wandb:  validation_loss 5.86389
wandb:  validation_rmse 2.42155
wandb: 
wandb: 🚀 View run rich-sweep-25 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/qbw9r2q7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_212413-qbw9r2q7/logs
wandb: Agent Starting Run: 9hau1wkp with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_213106-9hau1wkp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-26
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9hau1wkp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 67540865
wandb:       train_loss 1.73939
wandb:       train_rmse 1.31886
wandb:  validation_loss 6.25451
wandb:  validation_rmse 2.5009
wandb: 
wandb: 🚀 View run mild-sweep-26 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9hau1wkp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_213106-9hau1wkp/logs
wandb: Agent Starting Run: r4gfv21t with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_213557-r4gfv21t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-sweep-27
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r4gfv21t
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 67540865
wandb:       train_loss 2.11569
wandb:       train_rmse 1.45454
wandb:  validation_loss 5.24373
wandb:  validation_rmse 2.28992
wandb: 
wandb: 🚀 View run deft-sweep-27 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/r4gfv21t
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_213557-r4gfv21t/logs
wandb: Agent Starting Run: 5tv6oq6h with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_214047-5tv6oq6h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-28
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5tv6oq6h
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 16889729
wandb:       train_loss 2.20831
wandb:       train_rmse 1.48604
wandb:  validation_loss 4.98207
wandb:  validation_rmse 2.23205
wandb: 
wandb: 🚀 View run dutiful-sweep-28 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5tv6oq6h
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_214047-5tv6oq6h/logs
wandb: Agent Starting Run: f788nsz1 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_214505-f788nsz1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-29
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f788nsz1
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: integer out of range
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 278255489
wandb: 
wandb: 🚀 View run legendary-sweep-29 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/f788nsz1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_214505-f788nsz1/logs
Run f788nsz1 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: integer out of range

wandb: ERROR Run f788nsz1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: integer out of range
wandb: ERROR 
wandb: Agent Starting Run: e0ag9875 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_214521-e0ag9875
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-30
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e0ag9875
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 69568385
wandb:       train_loss 3.0164
wandb:       train_rmse 1.73678
wandb:  validation_loss 5.07566
wandb:  validation_rmse 2.25292
wandb: 
wandb: 🚀 View run lunar-sweep-30 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/e0ag9875
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_214521-e0ag9875/logs
wandb: Agent Starting Run: g1mfdbmp with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_215011-g1mfdbmp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-31
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g1mfdbmp
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 69568385
wandb:       train_loss 3.11097
wandb:       train_rmse 1.76379
wandb:  validation_loss 5.1054
wandb:  validation_rmse 2.25951
wandb: 
wandb: 🚀 View run winter-sweep-31 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g1mfdbmp
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_215011-g1mfdbmp/logs
wandb: Agent Starting Run: ai21ft0w with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 1
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_215509-ai21ft0w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-32
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ai21ft0w
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 16889729
wandb:       train_loss 1.99872
wandb:       train_rmse 1.41376
wandb:  validation_loss 5.13138
wandb:  validation_rmse 2.26525
wandb: 
wandb: 🚀 View run dutiful-sweep-32 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ai21ft0w
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_215509-ai21ft0w/logs
wandb: Agent Starting Run: 9nd5dnrs with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_215933-9nd5dnrs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-33
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9nd5dnrs
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 65519545
wandb:       train_loss 2.34653
wandb:       train_rmse 1.53184
wandb:  validation_loss 4.71368
wandb:  validation_rmse 2.1711
wandb: 
wandb: 🚀 View run sweepy-sweep-33 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9nd5dnrs
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_215933-9nd5dnrs/logs
wandb: Agent Starting Run: wb47t2wm with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_220426-wb47t2wm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-34
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wb47t2wm
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 3973049
wandb:       train_loss 1.96426
wandb:       train_rmse 1.40152
wandb:  validation_loss 4.2212
wandb:  validation_rmse 2.05456
wandb: 
wandb: 🚀 View run snowy-sweep-34 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wb47t2wm
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_220426-wb47t2wm/logs
wandb: Agent Starting Run: kgbue5ku with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_220844-kgbue5ku
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-sweep-35
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kgbue5ku
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 3973049
wandb:       train_loss 1.64931
wandb:       train_rmse 1.28425
wandb:  validation_loss 3.88002
wandb:  validation_rmse 1.96978
wandb: 
wandb: 🚀 View run prime-sweep-35 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kgbue5ku
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_220844-kgbue5ku/logs
wandb: Agent Starting Run: t48tdsd4 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_221303-t48tdsd4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-36
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t48tdsd4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▂▂▄▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 237497
wandb:       train_loss 2.12357
wandb:       train_rmse 1.45725
wandb:  validation_loss 3.53122
wandb:  validation_rmse 1.87915
wandb: 
wandb: 🚀 View run fearless-sweep-36 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t48tdsd4
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_221303-t48tdsd4/logs
wandb: Agent Starting Run: lllrpljr with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_221721-lllrpljr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-37
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lllrpljr
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 69566393
wandb:       train_loss 1.99671
wandb:       train_rmse 1.41305
wandb:  validation_loss 5.39978
wandb:  validation_rmse 2.32374
wandb: 
wandb: 🚀 View run vital-sweep-37 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lllrpljr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_221721-lllrpljr/logs
wandb: Agent Starting Run: br74m8c9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_222210-br74m8c9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-38
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/br74m8c9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 4478905
wandb:       train_loss 1.61976
wandb:       train_rmse 1.2727
wandb:  validation_loss 4.13984
wandb:  validation_rmse 2.03466
wandb: 
wandb: 🚀 View run northern-sweep-38 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/br74m8c9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_222210-br74m8c9/logs
wandb: Agent Starting Run: mlfx4rb3 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_222628-mlfx4rb3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-39
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mlfx4rb3
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 4224953
wandb:       train_loss 2.15828
wandb:       train_rmse 1.46911
wandb:  validation_loss 4.30943
wandb:  validation_rmse 2.07592
wandb: 
wandb: 🚀 View run different-sweep-39 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mlfx4rb3
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_222628-mlfx4rb3/logs
wandb: Agent Starting Run: g0bau4kg with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_223102-g0bau4kg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-40
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g0bau4kg
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▄▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 237497
wandb:       train_loss 1.84236
wandb:       train_rmse 1.35734
wandb:  validation_loss 3.93145
wandb:  validation_rmse 1.98279
wandb: 
wandb: 🚀 View run fearless-sweep-40 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/g0bau4kg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_223102-g0bau4kg/logs
wandb: Agent Starting Run: 6tiprp0s with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_223515-6tiprp0s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-sweep-41
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6tiprp0s
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 131039025
wandb:       train_loss 2.37826
wandb:       train_rmse 1.54216
wandb:  validation_loss 4.27861
wandb:  validation_rmse 2.06848
wandb: 
wandb: 🚀 View run graceful-sweep-41 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6tiprp0s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_223515-6tiprp0s/logs
wandb: Agent Starting Run: o628m8cj with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_224036-o628m8cj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-42
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o628m8cj
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 7946033
wandb:       train_loss 2.19205
wandb:       train_rmse 1.48056
wandb:  validation_loss 4.36571
wandb:  validation_rmse 2.08943
wandb: 
wandb: 🚀 View run young-sweep-42 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/o628m8cj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_224036-o628m8cj/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 9q1t7acz with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_224504-9q1t7acz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-43
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9q1t7acz
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 7946033
wandb:       train_loss 2.01994
wandb:       train_rmse 1.42125
wandb:  validation_loss 3.90261
wandb:  validation_rmse 1.9755
wandb: 
wandb: 🚀 View run zesty-sweep-43 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9q1t7acz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_224504-9q1t7acz/logs
wandb: Agent Starting Run: 4do8wsi9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_224928-4do8wsi9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-44
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4do8wsi9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 474929
wandb:       train_loss 2.01613
wandb:       train_rmse 1.4199
wandb:  validation_loss 3.79149
wandb:  validation_rmse 1.94718
wandb: 
wandb: 🚀 View run logical-sweep-44 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4do8wsi9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_224928-4do8wsi9/logs
wandb: Agent Starting Run: scuqdi3y with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_225346-scuqdi3y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-45
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/scuqdi3y
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 139132721
wandb:       train_loss 2.36878
wandb:       train_rmse 1.53908
wandb:  validation_loss 5.1322
wandb:  validation_rmse 2.26544
wandb: 
wandb: 🚀 View run dazzling-sweep-45 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/scuqdi3y
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_225346-scuqdi3y/logs
wandb: Agent Starting Run: tllmwzfb with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_225921-tllmwzfb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-sweep-46
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tllmwzfb
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8957745
wandb:       train_loss 1.92173
wandb:       train_rmse 1.38627
wandb:  validation_loss 4.34488
wandb:  validation_rmse 2.08444
wandb: 
wandb: 🚀 View run visionary-sweep-46 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/tllmwzfb
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_225921-tllmwzfb/logs
wandb: Agent Starting Run: ubulypm2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_230345-ubulypm2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-47
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ubulypm2
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8449841
wandb:       train_loss 1.99588
wandb:       train_rmse 1.41276
wandb:  validation_loss 3.65296
wandb:  validation_rmse 1.91127
wandb: 
wandb: 🚀 View run lively-sweep-47 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ubulypm2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_230345-ubulypm2/logs
wandb: Agent Starting Run: djxvbxsn with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_230809-djxvbxsn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-48
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/djxvbxsn
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 474929
wandb:       train_loss 2.14055
wandb:       train_rmse 1.46306
wandb:  validation_loss 3.36971
wandb:  validation_rmse 1.83568
wandb: 
wandb: 🚀 View run earnest-sweep-48 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/djxvbxsn
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_230809-djxvbxsn/logs
wandb: Agent Starting Run: 91ijclzr with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_231227-91ijclzr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-49
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/91ijclzr
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 262084897
wandb:       train_loss 1.73497
wandb:       train_rmse 1.31718
wandb:  validation_loss 4.66101
wandb:  validation_rmse 2.15894
wandb: 
wandb: 🚀 View run bright-sweep-49 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/91ijclzr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_231227-91ijclzr/logs
wandb: Agent Starting Run: 130kpyfv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_231912-130kpyfv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-50
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/130kpyfv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 15898913
wandb:       train_loss 2.47932
wandb:       train_rmse 1.57459
wandb:  validation_loss 4.91312
wandb:  validation_rmse 2.21656
wandb: 
wandb: 🚀 View run vague-sweep-50 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/130kpyfv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_231912-130kpyfv/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: axt2t7jv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_232340-axt2t7jv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-51
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/axt2t7jv
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 15898913
wandb:       train_loss 1.87853
wandb:       train_rmse 1.37059
wandb:  validation_loss 3.64725
wandb:  validation_rmse 1.90978
wandb: 
wandb: 🚀 View run jolly-sweep-51 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/axt2t7jv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_232340-axt2t7jv/logs
wandb: Agent Starting Run: pqbiv6zd with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_232810-pqbiv6zd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-sweep-52
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pqbiv6zd
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 956705
wandb:       train_loss 1.63711
wandb:       train_rmse 1.2795
wandb:  validation_loss 3.75314
wandb:  validation_rmse 1.9373
wandb: 
wandb: 🚀 View run cool-sweep-52 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/pqbiv6zd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_232810-pqbiv6zd/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: 8xx0jp6o with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_233234-8xx0jp6o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-53
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8xx0jp6o
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: integer out of range
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 278272289
wandb: 
wandb: 🚀 View run denim-sweep-53 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8xx0jp6o
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_233234-8xx0jp6o/logs
Run 8xx0jp6o errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
RuntimeError: integer out of range

wandb: ERROR Run 8xx0jp6o errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR RuntimeError: integer out of range
wandb: ERROR 
wandb: Agent Starting Run: w52gcfmu with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_233249-w52gcfmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-54
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w52gcfmu
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 17922337
wandb:       train_loss 2.08187
wandb:       train_rmse 1.44287
wandb:  validation_loss 4.43771
wandb:  validation_rmse 2.10659
wandb: 
wandb: 🚀 View run celestial-sweep-54 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/w52gcfmu
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_233249-w52gcfmu/logs
wandb: Agent Starting Run: wh6jvhs9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_233718-wh6jvhs9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-55
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wh6jvhs9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 16906529
wandb:       train_loss 2.73189
wandb:       train_rmse 1.65284
wandb:  validation_loss 4.06067
wandb:  validation_rmse 2.01511
wandb: 
wandb: 🚀 View run worthy-sweep-55 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/wh6jvhs9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_233718-wh6jvhs9/logs
wandb: Agent Starting Run: cnqo8k0r with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_234152-cnqo8k0r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-56
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cnqo8k0r
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 956705
wandb:       train_loss 2.80798
wandb:       train_rmse 1.6757
wandb:  validation_loss 3.79688
wandb:  validation_rmse 1.94856
wandb: 
wandb: 🚀 View run earnest-sweep-56 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/cnqo8k0r
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_234152-cnqo8k0r/logs
wandb: Agent Starting Run: l7tae3ip with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_234611-l7tae3ip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-57
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l7tae3ip
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.51 GiB. GPU 0 has a total capacity of 79.25 GiB of which 11.80 GiB is free. Including non-PyTorch memory, this process has 67.45 GiB memory in use. Of the allocated memory 61.08 GiB is allocated by PyTorch, and 5.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 524204289
wandb: 
wandb: 🚀 View run ancient-sweep-57 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l7tae3ip
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_234611-l7tae3ip/logs
Run l7tae3ip errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.51 GiB. GPU 0 has a total capacity of 79.25 GiB of which 11.80 GiB is free. Including non-PyTorch memory, this process has 67.45 GiB memory in use. Of the allocated memory 61.08 GiB is allocated by PyTorch, and 5.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run l7tae3ip errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.51 GiB. GPU 0 has a total capacity of 79.25 GiB of which 11.80 GiB is free. Including non-PyTorch memory, this process has 67.45 GiB memory in use. Of the allocated memory 61.08 GiB is allocated by PyTorch, and 5.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: af6zcbk0 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_234632-af6zcbk0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-58
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/af6zcbk0
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 31832321
wandb:       train_loss 1.82471
wandb:       train_rmse 1.35082
wandb:  validation_loss 4.42768
wandb:  validation_rmse 2.1042
wandb: 
wandb: 🚀 View run atomic-sweep-58 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/af6zcbk0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_234632-af6zcbk0/logs
wandb: Agent Starting Run: bcqrwaol with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_235102-bcqrwaol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-59
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bcqrwaol
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 31832321
wandb:       train_loss 1.93829
wandb:       train_rmse 1.39223
wandb:  validation_loss 4.21604
wandb:  validation_rmse 2.0533
wandb: 
wandb: 🚀 View run laced-sweep-59 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bcqrwaol
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_235102-bcqrwaol/logs
wandb: Agent Starting Run: ezn4g74y with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241123_235537-ezn4g74y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-60
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ezn4g74y
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 1947905
wandb:       train_loss 2.01435
wandb:       train_rmse 1.41928
wandb:  validation_loss 4.16002
wandb:  validation_rmse 2.03961
wandb: 
wandb: 🚀 View run cosmic-sweep-60 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ezn4g74y
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241123_235537-ezn4g74y/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: bfb5bltd with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000001-bfb5bltd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-61
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bfb5bltd
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.20 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.55 GiB memory in use. Of the allocated memory 69.00 GiB is allocated by PyTorch, and 6.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 556579073
wandb: 
wandb: 🚀 View run lyric-sweep-61 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bfb5bltd
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000001-bfb5bltd/logs
Run bfb5bltd errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
    return F.max_pool2d(input, self.kernel_size, self.stride,
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
    return if_false(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.20 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.55 GiB memory in use. Of the allocated memory 69.00 GiB is allocated by PyTorch, and 6.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bfb5bltd errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/pooling.py", line 164, in forward
wandb: ERROR     return F.max_pool2d(input, self.kernel_size, self.stride,
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_jit_internal.py", line 503, in fn
wandb: ERROR     return if_false(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 796, in _max_pool2d
wandb: ERROR     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.20 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.55 GiB memory in use. Of the allocated memory 69.00 GiB is allocated by PyTorch, and 6.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: q8ubsnmy with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000022-q8ubsnmy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-sweep-62
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q8ubsnmy
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb:       train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:  validation_rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: total_parameters 35879169
wandb:       train_loss 2.16927
wandb:       train_rmse 1.47284
wandb:  validation_loss 5.23588
wandb:  validation_rmse 2.28821
wandb: 
wandb: 🚀 View run deep-sweep-62 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/q8ubsnmy
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000022-q8ubsnmy/logs
wandb: Agent Starting Run: dxfwch33 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000448-dxfwch33
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sweep-63
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dxfwch33
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.10 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.55 GiB memory in use. Of the allocated memory 70.14 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 33847553
wandb: 
wandb: 🚀 View run toasty-sweep-63 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dxfwch33
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000448-dxfwch33/logs
Run dxfwch33 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.10 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.55 GiB memory in use. Of the allocated memory 70.14 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run dxfwch33 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
wandb: ERROR     return F.batch_norm(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
wandb: ERROR     return torch.batch_norm(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.10 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.55 GiB memory in use. Of the allocated memory 70.14 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 80wtj4lj with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 2
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000504-80wtj4lj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-64
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/80wtj4lj
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.66 GiB is free. Including non-PyTorch memory, this process has 77.58 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 4.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 1947905
wandb: 
wandb: 🚀 View run polished-sweep-64 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/80wtj4lj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000504-80wtj4lj/logs
Run 80wtj4lj errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.66 GiB is free. Including non-PyTorch memory, this process has 77.58 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 4.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 80wtj4lj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
wandb: ERROR     return F.batch_norm(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
wandb: ERROR     return torch.batch_norm(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 79.25 GiB of which 1.66 GiB is free. Including non-PyTorch memory, this process has 77.58 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 4.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5wx30k7s with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000520-5wx30k7s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-65
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5wx30k7s
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.10 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 127017497
wandb: 
wandb: 🚀 View run celestial-sweep-65 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5wx30k7s
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000520-5wx30k7s/logs
Run 5wx30k7s errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
    test_outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.10 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5wx30k7s errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 251, in train
wandb: ERROR     test_outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
wandb: ERROR     return F.batch_norm(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
wandb: ERROR     return torch.batch_norm(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.10 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 0fal0fs6 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000535-0fal0fs6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-66
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0fal0fs6
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.12 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 1860121
wandb: 
wandb: 🚀 View run daily-sweep-66 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/0fal0fs6
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000535-0fal0fs6/logs
Run 0fal0fs6 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.12 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0fal0fs6 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.12 GiB is allocated by PyTorch, and 1.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5jsbt8jl with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000551-5jsbt8jl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-67
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5jsbt8jl
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.14 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 1628697
wandb: 
wandb: 🚀 View run balmy-sweep-67 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5jsbt8jl
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000551-5jsbt8jl/logs
Run 5jsbt8jl errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.14 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5jsbt8jl errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 696.75 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 76.14 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: rs7qf0ic with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000606-rs7qf0ic
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-68
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rs7qf0ic
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 8729
wandb: 
wandb: 🚀 View run fiery-sweep-68 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rs7qf0ic
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000606-rs7qf0ic/logs
Run rs7qf0ic errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run rs7qf0ic errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: 80kzx6y2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000612-80kzx6y2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-69
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/80kzx6y2
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 266.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.68 GiB is allocated by PyTorch, and 921.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 139133465
wandb: 
wandb: 🚀 View run dark-sweep-69 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/80kzx6y2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000612-80kzx6y2/logs
Run 80kzx6y2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 266.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.68 GiB is allocated by PyTorch, and 921.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 80kzx6y2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 458, in forward
wandb: ERROR     return self._conv_forward(input, self.weight, self.bias)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
wandb: ERROR     return F.conv2d(input, weight, bias, self.stride,
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 266.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.68 GiB is allocated by PyTorch, and 921.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: n4noty05 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000622-n4noty05
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-sweep-70
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n4noty05
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.71 GiB is allocated by PyTorch, and 894.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 2363929
wandb: 
wandb: 🚀 View run proud-sweep-70 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n4noty05
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000622-n4noty05/logs
Run n4noty05 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.71 GiB is allocated by PyTorch, and 894.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run n4noty05 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.71 GiB is allocated by PyTorch, and 894.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5j6h42aj with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000638-5j6h42aj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-sweep-71
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5j6h42aj
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 870.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 2109977
wandb: 
wandb: 🚀 View run valiant-sweep-71 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5j6h42aj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000638-5j6h42aj/logs
Run 5j6h42aj errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 870.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5j6h42aj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 164.75 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 870.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8w7f8twe with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000654-8w7f8twe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-72
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8w7f8twe
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 872.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 37401
wandb: 
wandb: 🚀 View run fiery-sweep-72 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8w7f8twe
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000654-8w7f8twe/logs
Run 8w7f8twe errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 872.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8w7f8twe errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 872.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 5d25iaf8 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000709-5d25iaf8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-73
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5d25iaf8
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 970.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 871.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run trim-sweep-73 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/5d25iaf8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000709-5d25iaf8/logs
Run 5d25iaf8 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 970.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 871.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5d25iaf8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 970.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.73 GiB is allocated by PyTorch, and 871.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: bj4xlsq2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000715-bj4xlsq2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-sweep-74
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bj4xlsq2
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 829.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 3729393
wandb: 
wandb: 🚀 View run avid-sweep-74 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bj4xlsq2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000715-bj4xlsq2/logs
Run bj4xlsq2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 829.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bj4xlsq2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 162.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 829.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: n60mx7ee with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000730-n60mx7ee
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-75
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n60mx7ee
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
    loss.backward()
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.22 GiB is allocated by PyTorch, and 498.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 3266545
wandb: 
wandb: 🚀 View run dandy-sweep-75 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/n60mx7ee
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000730-n60mx7ee/logs
Run n60mx7ee errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
    loss.backward()
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.22 GiB is allocated by PyTorch, and 498.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run n60mx7ee errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 60, in train_one_epoch
wandb: ERROR     loss.backward()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/_tensor.py", line 521, in backward
wandb: ERROR     torch.autograd.backward(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
wandb: ERROR     _engine_run_backward(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
wandb: ERROR     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.22 GiB is allocated by PyTorch, and 498.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: greul6p9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000746-greul6p9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sweep-76
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/greul6p9
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 26609
wandb: 
wandb: 🚀 View run distinctive-sweep-76 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/greul6p9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000746-greul6p9/logs
Run greul6p9 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run greul6p9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: yal258gg with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000751-yal258gg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-77
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yal258gg
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.10 GiB is allocated by PyTorch, and 626.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run balmy-sweep-77 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/yal258gg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000751-yal258gg/logs
Run yal258gg errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.10 GiB is allocated by PyTorch, and 626.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run yal258gg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.10 GiB is allocated by PyTorch, and 626.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: uh9wedl9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000806-uh9wedl9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-sweep-78
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uh9wedl9
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.15 GiB is allocated by PyTorch, and 572.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 4737009
wandb: 
wandb: 🚀 View run logical-sweep-78 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uh9wedl9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000806-uh9wedl9/logs
Run uh9wedl9 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.15 GiB is allocated by PyTorch, and 572.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run uh9wedl9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.15 GiB is allocated by PyTorch, and 572.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 9eug2tza with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000823-9eug2tza
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-79
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9eug2tza
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 373.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 4229105
wandb: 
wandb: 🚀 View run lemon-sweep-79 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9eug2tza
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000823-9eug2tza/logs
Run 9eug2tza errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
    return F.batch_norm(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
    return torch.batch_norm(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 373.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 9eug2tza errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 176, in forward
wandb: ERROR     return F.batch_norm(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 2512, in batch_norm
wandb: ERROR     return torch.batch_norm(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 32.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 373.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2ucfhi8t with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000828-2ucfhi8t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-80
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2ucfhi8t
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 374.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 83953
wandb: 
wandb: 🚀 View run pious-sweep-80 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2ucfhi8t
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000828-2ucfhi8t/logs
Run 2ucfhi8t errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
    spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 374.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2ucfhi8t errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 250, in train
wandb: ERROR     spectrogram, target = vdata["spectrogram"].to(DEVICE), vdata["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 374.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: p5nwaxip with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000844-p5nwaxip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-81
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p5nwaxip
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 373.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run polished-sweep-81 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/p5nwaxip
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000844-p5nwaxip/logs
Run p5nwaxip errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 373.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run p5nwaxip errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.89 GiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 373.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: i97df19i with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000854-i97df19i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-82
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i97df19i
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.55 GiB is allocated by PyTorch, and 166.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 7502497
wandb: 
wandb: 🚀 View run snowy-sweep-82 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/i97df19i
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000854-i97df19i/logs
Run i97df19i errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
    outputs = model(spectrogram)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
    x = self.conv_layers(input_data)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/activation.py", line 104, in forward
    return F.relu(input, inplace=self.inplace)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.55 GiB is allocated by PyTorch, and 166.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run i97df19i errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 56, in train_one_epoch
wandb: ERROR     outputs = model(spectrogram)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 76, in forward
wandb: ERROR     x = self.conv_layers(input_data)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
wandb: ERROR     input = module(input)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/activation.py", line 104, in forward
wandb: ERROR     return F.relu(input, inplace=self.inplace)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/functional.py", line 1500, in relu
wandb: ERROR     result = torch.relu(input)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.55 GiB is allocated by PyTorch, and 166.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: l56b7kmg with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000859-l56b7kmg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-sweep-83
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l56b7kmg
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.57 GiB is allocated by PyTorch, and 141.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 6576801
wandb: 
wandb: 🚀 View run devoted-sweep-83 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/l56b7kmg
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000859-l56b7kmg/logs
Run l56b7kmg errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.57 GiB is allocated by PyTorch, and 141.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run l56b7kmg errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.57 GiB is allocated by PyTorch, and 141.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 6ahrswa8 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000905-6ahrswa8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-84
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6ahrswa8
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 96929
wandb: 
wandb: 🚀 View run restful-sweep-84 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6ahrswa8
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000905-6ahrswa8/logs
Run 6ahrswa8 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 6ahrswa8 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: gx16pw7l with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000910-gx16pw7l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-sweep-85
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gx16pw7l
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.07 GiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.58 GiB is allocated by PyTorch, and 140.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run still-sweep-85 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/gx16pw7l
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000910-gx16pw7l/logs
Run gx16pw7l errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.07 GiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.58 GiB is allocated by PyTorch, and 140.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run gx16pw7l errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.07 GiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.58 GiB is allocated by PyTorch, and 140.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 71fx9mvz with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000921-71fx9mvz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-86
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/71fx9mvz
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 104.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 9517729
wandb: 
wandb: 🚀 View run floral-sweep-86 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/71fx9mvz
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000921-71fx9mvz/logs
Run 71fx9mvz errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 104.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 71fx9mvz errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 30.75 MiB is free. Including non-PyTorch memory, this process has 79.21 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 104.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: nr739b4u with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000927-nr739b4u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-87
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nr739b4u
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 106.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run vital-sweep-87 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/nr739b4u
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000927-nr739b4u/logs
Run nr739b4u errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 106.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run nr739b4u errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 106.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: mggdyvw0 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000932-mggdyvw0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-88
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mggdyvw0
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 105.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 211617
wandb: 
wandb: 🚀 View run confused-sweep-88 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mggdyvw0
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000932-mggdyvw0/logs
Run mggdyvw0 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 105.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run mggdyvw0 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 105.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: mikxzhod with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000937-mikxzhod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-89
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mikxzhod
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.79 GiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 103.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run laced-sweep-89 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mikxzhod
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000937-mikxzhod/logs
Run mikxzhod errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.79 GiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 103.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run mikxzhod errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.79 GiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 103.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 8rzov1v9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000948-8rzov1v9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-90
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8rzov1v9
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 102.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run good-sweep-90 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/8rzov1v9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000948-8rzov1v9/logs
Run 8rzov1v9 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 102.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8rzov1v9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.61 GiB is allocated by PyTorch, and 102.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: lokyegdf with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000953-lokyegdf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-sweep-91
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lokyegdf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 100.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run earnest-sweep-91 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/lokyegdf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000953-lokyegdf/logs
Run lokyegdf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 100.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run lokyegdf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 28.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 100.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7pk7f9va with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_000959-7pk7f9va
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-92
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7pk7f9va
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 375809
wandb: 
wandb: 🚀 View run pleasant-sweep-92 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7pk7f9va
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_000959-7pk7f9va/logs
Run 7pk7f9va errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 7pk7f9va errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: bw44ekjj with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001004-bw44ekjj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-sweep-93
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bw44ekjj
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.15 GiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 99.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run leafy-sweep-93 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bw44ekjj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001004-bw44ekjj/logs
Run bw44ekjj errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.15 GiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 99.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bw44ekjj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.15 GiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 99.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 6fgc33wi with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001014-6fgc33wi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-94
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6fgc33wi
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 97.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run scarlet-sweep-94 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/6fgc33wi
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001014-6fgc33wi/logs
Run 6fgc33wi errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 97.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6fgc33wi errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 97.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: vbc16kcf with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001021-vbc16kcf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-95
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vbc16kcf
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 95.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run divine-sweep-95 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/vbc16kcf
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001021-vbc16kcf/logs
Run vbc16kcf errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 95.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vbc16kcf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 26.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.62 GiB is allocated by PyTorch, and 95.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 7nbzx0it with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 3
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 64
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001026-7nbzx0it
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-96
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7nbzx0it
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 94.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 605185
wandb: 
wandb: 🚀 View run vague-sweep-96 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/7nbzx0it
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001026-7nbzx0it/logs
Run 7nbzx0it errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 94.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7nbzx0it errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 94.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ah79pggw with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001032-ah79pggw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-97
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ah79pggw
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 940.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 94.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run rose-sweep-97 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ah79pggw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001032-ah79pggw/logs
Run ah79pggw errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 940.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 94.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ah79pggw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 940.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 94.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: jz19sfe7 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001047-jz19sfe7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-98
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jz19sfe7
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 91.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 715481
wandb: 
wandb: 🚀 View run silvery-sweep-98 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/jz19sfe7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001047-jz19sfe7/logs
Run jz19sfe7 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 91.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run jz19sfe7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 91.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 2ytsgkb1 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001052-2ytsgkb1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-99
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2ytsgkb1
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 89.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 477913
wandb: 
wandb: 🚀 View run exalted-sweep-99 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/2ytsgkb1
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001052-2ytsgkb1/logs
Run 2ytsgkb1 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 89.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2ytsgkb1 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 89.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: t9ws07z2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001058-t9ws07z2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-100
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t9ws07z2
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -192: [64, -192]
wandb:                                                                                
wandb: 🚀 View run lyric-sweep-100 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/t9ws07z2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001058-t9ws07z2/logs
Run t9ws07z2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -192: [64, -192]

wandb: ERROR Run t9ws07z2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -192: [64, -192]
wandb: ERROR 
wandb: Agent Starting Run: k61x4yg9 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001103-k61x4yg9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-101
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k61x4yg9
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 89.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run lunar-sweep-101 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/k61x4yg9
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001103-k61x4yg9/logs
Run k61x4yg9 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 89.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run k61x4yg9 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.63 GiB is allocated by PyTorch, and 89.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: bnbtu3s7 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001119-bnbtu3s7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-102
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bnbtu3s7
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 83.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 1215193
wandb: 
wandb: 🚀 View run soft-sweep-102 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/bnbtu3s7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001119-bnbtu3s7/logs
Run bnbtu3s7 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 83.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bnbtu3s7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 83.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: rbr5yu2o with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001125-rbr5yu2o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-103
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rbr5yu2o
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 80.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 961241
wandb: 
wandb: 🚀 View run eager-sweep-103 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/rbr5yu2o
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001125-rbr5yu2o/logs
Run rbr5yu2o errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 80.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run rbr5yu2o errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 80.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: psvfq7bj with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 8
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001130-psvfq7bj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-sweep-104
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/psvfq7bj
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 27353
wandb: 
wandb: 🚀 View run desert-sweep-104 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/psvfq7bj
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001130-psvfq7bj/logs
Run psvfq7bj errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run psvfq7bj errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: 614vf8u2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001135-614vf8u2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-sweep-105
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/614vf8u2
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.83 GiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 79.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run stoic-sweep-105 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/614vf8u2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001135-614vf8u2/logs
Run 614vf8u2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.83 GiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 79.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 614vf8u2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.83 GiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 79.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: c27yh3b2 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001146-c27yh3b2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-sweep-106
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c27yh3b2
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 73.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 1476977
wandb: 
wandb: 🚀 View run frosty-sweep-106 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/c27yh3b2
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001146-c27yh3b2/logs
Run c27yh3b2 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 73.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run c27yh3b2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 24.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 73.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: mu6il0lw with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001156-mu6il0lw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-107
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mu6il0lw
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 71.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 1001841
wandb: 
wandb: 🚀 View run fearless-sweep-107 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/mu6il0lw
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001156-mu6il0lw/logs
Run mu6il0lw errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 71.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run mu6il0lw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 71.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: uvdy5wgo with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001211-uvdy5wgo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-sweep-108
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uvdy5wgo
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -384: [64, -384]
wandb:                                                                                
wandb: 🚀 View run amber-sweep-108 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/uvdy5wgo
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001211-uvdy5wgo/logs
Run uvdy5wgo errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -384: [64, -384]

wandb: ERROR Run uvdy5wgo errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -384: [64, -384]
wandb: ERROR 
wandb: Agent Starting Run: 9veluc3y with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001218-9veluc3y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zany-sweep-109
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9veluc3y
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.07 GiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 71.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run zany-sweep-109 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9veluc3y
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001218-9veluc3y/logs
Run 9veluc3y errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.07 GiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 71.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 9veluc3y errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.07 GiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.65 GiB is allocated by PyTorch, and 71.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 4m7v0dzv with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001228-4m7v0dzv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-sweep-110
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4m7v0dzv
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.66 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 2476401
wandb: 
wandb: 🚀 View run worldly-sweep-110 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/4m7v0dzv
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001228-4m7v0dzv/logs
Run 4m7v0dzv errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.66 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4m7v0dzv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.66 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: kvaerlkh with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001243-kvaerlkh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-111
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kvaerlkh
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.67 GiB is allocated by PyTorch, and 53.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 1968497
wandb: 
wandb: 🚀 View run glowing-sweep-111 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/kvaerlkh
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001243-kvaerlkh/logs
Run kvaerlkh errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.67 GiB is allocated by PyTorch, and 53.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run kvaerlkh errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.67 GiB is allocated by PyTorch, and 53.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: 9z2vdgfr with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 16
wandb: 	padding: 1
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001249-9z2vdgfr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-112
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9z2vdgfr
/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/init.py:453: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 100721
wandb: 
wandb: 🚀 View run dainty-sweep-112 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/9z2vdgfr
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001249-9z2vdgfr/logs
Run 9z2vdgfr errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
    model.apply(weights_init_map[config.weights_init])
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
    module.apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
    fn(self)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
    'Uniform': lambda m: weights_init_uniform_rule(m),
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
    y = 1.0/n**.5
ZeroDivisionError: float division by zero

wandb: ERROR Run 9z2vdgfr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 150, in train
wandb: ERROR     model.apply(weights_init_map[config.weights_init])
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 895, in apply
wandb: ERROR     module.apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 896, in apply
wandb: ERROR     fn(self)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 132, in <lambda>
wandb: ERROR     'Uniform': lambda m: weights_init_uniform_rule(m),
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 91, in weights_init_uniform_rule
wandb: ERROR     y = 1.0/n**.5
wandb: ERROR ZeroDivisionError: float division by zero
wandb: ERROR 
wandb: Agent Starting Run: erw0f89a with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001255-erw0f89a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-113
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/erw0f89a
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.67 GiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.67 GiB is allocated by PyTorch, and 53.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 🚀 View run colorful-sweep-113 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/erw0f89a
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001255-erw0f89a/logs
Run erw0f89a errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.67 GiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.67 GiB is allocated by PyTorch, and 53.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run erw0f89a errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.67 GiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.67 GiB is allocated by PyTorch, and 53.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: dkkchek7 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 1
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001315-dkkchek7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-sweep-114
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dkkchek7
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.68 GiB is allocated by PyTorch, and 40.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 3145121
wandb: 
wandb: 🚀 View run restful-sweep-114 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/dkkchek7
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001315-dkkchek7/logs
Run dkkchek7 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.68 GiB is allocated by PyTorch, and 40.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run dkkchek7 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.68 GiB is allocated by PyTorch, and 40.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: xh7piw09 with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001326-xh7piw09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-115
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xh7piw09
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.69 GiB is allocated by PyTorch, and 32.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: 
wandb: Run history:
wandb: total_parameters ▁
wandb: 
wandb: Run summary:
wandb: total_parameters 2194849
wandb: 
wandb: 🚀 View run wandering-sweep-115 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/xh7piw09
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001326-xh7piw09/logs
Run xh7piw09 errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
    avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
    spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.69 GiB is allocated by PyTorch, and 32.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run xh7piw09 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 241, in train
wandb: ERROR     avg_loss = train_one_epoch(loss_fn, model, train_data_loader, optimizer)
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 50, in train_one_epoch
wandb: ERROR     spectrogram, target = data["spectrogram"].to(DEVICE), data["target"].to(DEVICE)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 20.75 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 78.69 GiB is allocated by PyTorch, and 32.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ceviky4d with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 0
wandb: 	pooling_size: 2
wandb: 	stride: 2
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001337-ceviky4d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-116
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ceviky4d
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -768: [64, -768]
wandb:                                                                                
wandb: 🚀 View run neat-sweep-116 at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/ceviky4d
wandb: ⭐️ View project at: https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241124_001337-ceviky4d/logs
Run ceviky4d errored:
Traceback (most recent call last):
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
    model = CNN_97(
  File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
    fc_layers.append(nn.Linear(input_dim, hidden_units))
  File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
RuntimeError: Trying to create tensor with negative dimension -768: [64, -768]

wandb: ERROR Run ceviky4d errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py", line 108, in train
wandb: ERROR     model = CNN_97(
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/scripts/models.py", line 64, in __init__
wandb: ERROR     fc_layers.append(nn.Linear(input_dim, hidden_units))
wandb: ERROR   File "/zhome/fb/0/212723/DL/p4_velocity/DL_97_venv/lib64/python3.9/site-packages/torch/nn/modules/linear.py", line 99, in __init__
wandb: ERROR     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
wandb: ERROR RuntimeError: Trying to create tensor with negative dimension -768: [64, -768]
wandb: ERROR 
wandb: Agent Starting Run: x93v1z5v with config:
wandb: 	activation_fn: ReLU
wandb: 	batch_size: 32
wandb: 	conv_dropout: 0
wandb: 	epochs: 30
wandb: 	hidden_units: 64
wandb: 	kernel_size: 3x3
wandb: 	learning_rate: 0.0001
wandb: 	linear_dropout: 0
wandb: 	momentum: 0.9
wandb: 	num_conv_layers: 4
wandb: 	num_fc_layers: 3
wandb: 	optimizer: SGD
wandb: 	out_channels: 32
wandb: 	padding: 1
wandb: 	pooling_size: 1
wandb: 	stride: 1
wandb: 	use_cnn_batchnorm: True
wandb: 	use_fc_batchnorm: True
wandb: 	weight_decay: 1e-05
wandb: 	weights_init: Uniform
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /zhome/fb/0/212723/DL/p4_velocity/git/DeepLearning97/wandb/run-20241124_001342-x93v1z5v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-sweep-117
wandb: ⭐️ View project at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097
wandb: 🧹 View sweep at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/sweeps/nmuvd013
wandb: 🚀 View run at https://wandb.ai/raimo-sieber-technical-university-of-munich/Deep%20Learning%20Project%20Group%2097/runs/x93v1z5v
/zhome/fb/0/212723/.lsbatch/1732386830.23246043.shell: line 21: 345904 Killed                  python3 ~/DL/p4_velocity/git/DeepLearning97/scripts/modular_train_test.py > log/modular_train_test$(date +"%d-%m-%y")_$(date +'%H:%M:%S').log
