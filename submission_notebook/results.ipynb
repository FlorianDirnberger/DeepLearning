{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Based Velocity Estimation From Doppler-Radar\n",
    "\n",
    "This notebook contains all trained models discussed in the submitted report. Since we were not allowed to use the actual data from Trackman, synthetic dummy data (random generated in the required size) are used for testing. That also explains the extremly high test rmse in each case. (Using dummy data has been agreed with our supervisor Mark Henney)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import mse_loss as torch_mse_loss\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import time\n",
    "\n",
    "\n",
    "# Constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ROOT = Path().resolve()\n",
    "DATA_ROOT = ROOT / \"synthetic_data\"\n",
    "MODEL_DIR = ROOT / \"models\"\n",
    "\n",
    "SEQ_LEN = 1600  # Fixed sequence length\n",
    "INPUT_SIZE = 8  # Number of features in your time-series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrVelCNNRegr(nn.Module):\n",
    "    \"\"\"Baseline model for regression to the velocity\n",
    "\n",
    "    Use this to benchmark your model performance.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6,\n",
    "                      out_channels=16,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear1=nn.Linear(in_features=37120,out_features=1024)\n",
    "        self.linear2=nn.Linear(in_features=1024,out_features=256)\n",
    "        self.linear3=nn.Linear(in_features=256,out_features=1)\n",
    "    \n",
    "    def _input_layer(self, input_data):\n",
    "        return self.conv1(input_data)\n",
    "\n",
    "    def _hidden_layer(self, x):\n",
    "        x=self.conv2(x)\n",
    "        x=self.conv3(x)\n",
    "        x=self.conv4(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.linear1(x)\n",
    "        return self.linear2(x)\n",
    "\n",
    "    def _output_layer(self, x):\n",
    "        return self.linear3(x)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self._input_layer(input_data)\n",
    "        x = self._hidden_layer(x)\n",
    "        return self._output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(output: torch.Tensor,\n",
    "             target: torch.Tensor) -> torch.Tensor:\n",
    "    return torch_mse_loss(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, mode, input_size, hidden_size, num_layers, dropout, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Save the mode for use in forward\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Define the appropriate RNN type based on the mode\n",
    "        if mode == \"RNN\": \n",
    "            self.rnn = nn.RNN(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout,\n",
    "                nonlinearity='relu',\n",
    "                bias=True,\n",
    "                batch_first=False\n",
    "            )\n",
    "        elif mode == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout,\n",
    "                bias=True,\n",
    "                batch_first=False,\n",
    "                bidirectional=False\n",
    "            )\n",
    "        elif mode == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout,\n",
    "                batch_first=False\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Undefined mode, enter 'RNN', 'GRU' or 'LSTM'\")\n",
    "        \n",
    "        # BatchNorm on the hidden size\n",
    "        self.bn = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "        input_size = feature_dim\n",
    "        \n",
    "        # Reshape input and permute dimensions\n",
    "        x = x.view(batch_size, seq_len, input_size)\n",
    "        x = x.permute(1, 0, 2)  # [seq_len, batch_size, input_size]\n",
    "        \n",
    "        # Forward pass through the appropriate RNN type\n",
    "        if self.mode in [\"RNN\", \"GRU\", \"LSTM\"]:\n",
    "            out, _ = self.rnn(x)\n",
    "        else:\n",
    "            raise ValueError(\"Undefined mode, enter 'RNN', 'GRU' or 'LSTM'\")\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        out = self.bn(out[-1])  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Architecture for Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_97(nn.Module):\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    def __init__(self, \n",
    "                    num_conv_layers=4,\n",
    "                    conv_dropout=0.2, \n",
    "                    num_fc_layers=3, \n",
    "                    kernel_size=(5, 5),\n",
    "                    stride = 1, \n",
    "                    padding = 2,\n",
    "                    pooling_size = 2,\n",
    "                    linear_dropout=0.1, \n",
    "                    activation=nn.ReLU,\n",
    "                    hidden_units=1024,\n",
    "                    out_channels = 16, # Starting output channels for the first conv layer\n",
    "                    use_fc_batchnorm=True,\n",
    "                    use_cnn_batchnorm=True,\n",
    "                    input_shape=(6, 74, 918)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define dynamic convolutional layers\n",
    "        conv_layers = []\n",
    "        in_channels = input_shape[0]  # Assuming the input has 6 channels\n",
    "        height, width = input_shape[1], input_shape[2]\n",
    "\n",
    "        for _ in range(num_conv_layers):\n",
    "            conv_layers.append(nn.Conv2d(in_channels=in_channels,\n",
    "                                            out_channels=out_channels,\n",
    "                                            kernel_size=(kernel_size[0], kernel_size[1]),\n",
    "                                            stride=stride,\n",
    "                                            padding=padding))\n",
    "            if use_cnn_batchnorm:  # Add BatchNorm2d if enabled\n",
    "                conv_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            conv_layers.append(activation())\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=pooling_size))\n",
    "            conv_layers.append(nn.Dropout(conv_dropout))\n",
    "            \n",
    "            in_channels = out_channels  # Update input channels for the next layer\n",
    "            out_channels *= 2  # Double output channels for each subsequent layer\n",
    "            \n",
    "            # this is just to track the sizes \n",
    "            # Update dimensions after convolution and pooling\n",
    "            height = (height + 2 * padding - kernel_size[0]) // stride + 1  # Convolution output height\n",
    "            width = (width + 2 * padding - kernel_size[1]) // stride + 1    # Convolution output width\n",
    "            height, width = height // pooling_size, width // pooling_size  # After max pooling with kernel_size=2\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate the flattened dimension after convolution layers\n",
    "        input_dim = height*width*in_channels\n",
    "\n",
    "        # Define dynamic fully connected layers\n",
    "        fc_layers = []\n",
    "        for i in range(num_fc_layers - 1): #added halfing of hidden units each layer\n",
    "            fc_layers.append(nn.Linear(input_dim, hidden_units))\n",
    "            if use_fc_batchnorm:  # Add BatchNorm1d if enabled\n",
    "                fc_layers.append(nn.BatchNorm1d(hidden_units))\n",
    "            fc_layers.append(activation())\n",
    "            fc_layers.append(nn.Dropout(linear_dropout))\n",
    "            input_dim = hidden_units  # Update for the next layer\n",
    "            hidden_units = hidden_units // 2 # halve hidden units\n",
    "        # Final output layer with a single output feature\n",
    "        fc_layers.append(nn.Linear(input_dim, 1))\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv_layers(input_data)\n",
    "        #print(f\"Shape after conv layers (before flatten): {x.shape}\")  # Debug shape\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"Flattened size before fully connected layers: {x.shape}\")  # Debug shape after flattening\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Architecture of optimized CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        # 1x1 convolution to get a spatial attention map\n",
    "        self.attention_conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generate the attention map\n",
    "        attention_map = self.attention_conv(x)\n",
    "        attention_map = self.sigmoid(attention_map)  # Apply sigmoid to get attention weights\n",
    "        return x * attention_map  # Apply attention to the input feature map\n",
    "    \n",
    "class CNN_97_withAtt(nn.Module):\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_conv_layers=4,\n",
    "                 conv_dropout=0.2, \n",
    "                 num_fc_layers=3, \n",
    "                 kernel_size=(5, 5),\n",
    "                 stride = 1, \n",
    "                 padding = 2,\n",
    "                 pooling_size = 2,\n",
    "                 linear_dropout=0.1, \n",
    "                 activation=nn.ReLU,\n",
    "                 hidden_units=1024,\n",
    "                 out_channels = 16, # Starting output channels for the first conv layer\n",
    "                 use_fc_batchnorm=True,\n",
    "                 use_cnn_batchnorm=True,\n",
    "                 input_shape=(6, 74, 918)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define dynamic convolutional layers\n",
    "        conv_layers = []\n",
    "        in_channels = input_shape[0]  # Assuming the input has 6 channels\n",
    "        height, width = input_shape[1], input_shape[2]\n",
    "\n",
    "        for _ in range(num_conv_layers):\n",
    "            conv_layers.append(nn.Conv2d(in_channels=in_channels,\n",
    "                                         out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size[0], kernel_size[1]),\n",
    "                                         stride=stride,\n",
    "                                         padding=padding))\n",
    "            if use_cnn_batchnorm:  # Add BatchNorm2d if enabled\n",
    "                conv_layers.append(nn.BatchNorm2d(out_channels))\n",
    "            conv_layers.append(activation())\n",
    "            conv_layers.append(nn.MaxPool2d(kernel_size=pooling_size))\n",
    "            conv_layers.append(nn.Dropout(conv_dropout))\n",
    "\n",
    "            # Add Spatial Attention after each convolution block\n",
    "            conv_layers.append(SpatialAttention(out_channels))\n",
    "            \n",
    "            in_channels = out_channels  # Update input channels for the next layer\n",
    "            out_channels *= 2  # Double output channels for each subsequent layer\n",
    "            \n",
    "            # this is just to track the sizes \n",
    "            # Update dimensions after convolution and pooling\n",
    "            height = (height + 2 * padding - kernel_size[0]) // stride + 1  # Convolution output height\n",
    "            width = (width + 2 * padding - kernel_size[1]) // stride + 1    # Convolution output width\n",
    "            height, width = height // pooling_size, width // pooling_size  # After max pooling with kernel_size=2\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate the flattened dimension after convolution layers\n",
    "        input_dim = height*width*in_channels\n",
    "\n",
    "        # Define dynamic fully connected layers\n",
    "        fc_layers = []\n",
    "        for i in range(num_fc_layers - 1): #added halfing of hidden units each layer\n",
    "            fc_layers.append(nn.Linear(input_dim, hidden_units))\n",
    "            if use_fc_batchnorm:  # Add BatchNorm1d if enabled\n",
    "                fc_layers.append(nn.BatchNorm1d(hidden_units))\n",
    "            fc_layers.append(activation())\n",
    "            fc_layers.append(nn.Dropout(linear_dropout))\n",
    "            input_dim = hidden_units  # Update for the next layer\n",
    "            hidden_units = hidden_units // 2 # halve hidden units\n",
    "        # Final output layer with a single output feature\n",
    "        fc_layers.append(nn.Linear(input_dim, 1))\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv_layers(input_data)\n",
    "        #print(f\"Shape after conv layers (before flatten): {x.shape}\")  # Debug shape\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"Flattened size before fully connected layers: {x.shape}\")  # Debug shape after flattening\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test all trained models with synthetic data\n",
    "\n",
    "*IMPORTANT NOTE: Results do not match the results of the submitted report. Synthetic dummy data need to be used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameter\n",
    "model_baseline = SpectrVelCNNRegr()\n",
    "\n",
    "# Load pretrained model\n",
    "model_path = MODEL_DIR / \"baseline\"\n",
    "model_baseline.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_baseline.eval()\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = model_baseline.loss_fn\n",
    "\n",
    "# # Test model\n",
    "test_data_path = DATA_ROOT / \"dummy_stacked_spectrograms.npy\"\n",
    "test_data = np.transpose(np.load(test_data_path), (2,0,1))\n",
    "\n",
    "spectrogram = torch.tensor(test_data).unsqueeze(0).to(DEVICE).float()\n",
    "target = torch.tensor([10.0]).to(DEVICE).squeeze()\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    test_output = model_baseline(spectrogram).squeeze()\n",
    "    test_loss = loss_fn(test_output.squeeze(), target)\n",
    "    test_rmse = test_loss.sqrt()\n",
    "\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal hyperparameters obtained from training\n",
    "OPTIMAL_CONFIG = {\n",
    "    \"mode\": \"GRU\",  \n",
    "    \"hidden_size\": 256,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"output_size\": 1,  # Single output for regression\n",
    "}\n",
    "\n",
    "def load_model(config, model_path):\n",
    "    \"\"\"Load the trained model with the given configuration.\"\"\"\n",
    "    model = RNN(\n",
    "        mode=config[\"mode\"],\n",
    "        input_size=INPUT_SIZE,\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "        output_size=config[\"output_size\"],\n",
    "    ).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the model\n",
    "model_path = MODEL_DIR / \"GRU_best.pth\"\n",
    "if not model_path.exists():\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "model = load_model(OPTIMAL_CONFIG, model_path)\n",
    "print(f\"Model loaded successfully on {DEVICE}.\")\n",
    "\n",
    "def test_dummy_sample_with_rmse(model):\n",
    "    \n",
    "    # Create a dummy input tensor and GT \n",
    "    dummy_input = torch.randn(1, SEQ_LEN, INPUT_SIZE).to(DEVICE)   \n",
    "    ground_truth = torch.tensor([1.0]).to(DEVICE).squeeze()  \n",
    "\n",
    "    # Perform forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "\n",
    "    # Compute RMSE\n",
    "    mse_loss = nn.MSELoss()(output.squeeze(), ground_truth)\n",
    "    rmse = mse_loss.sqrt().item()\n",
    "\n",
    "    print(f\"Dummy input shape: {dummy_input.shape}\")\n",
    "    print(f\"Model output: {output.item():.4f}\")\n",
    "    print(f\"Ground truth: {ground_truth.item():.4f}\")\n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_dummy_sample_with_rmse(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best CNN with preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic test data\n",
    "test_data_path = DATA_ROOT / \"dummy_stacked_spectrograms.npy\"\n",
    "spectrogram = np.transpose(np.load(test_data_path), (2,0,1))\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "# Define power and phase channels\n",
    "NUM_POWER_CHANNELS = 4\n",
    "PHASE_SPECTROGRAM_LIMITS = (-np.pi, np.pi)\n",
    "\n",
    "spectrogram_processed = spectrogram.copy()\n",
    "# Apply Sobel filter with ksize=31 on x-axis only to power channels\n",
    "for ch in range(NUM_POWER_CHANNELS):\n",
    "    # Get the channel data\n",
    "    channel_data = spectrogram_processed[:, :, ch].astype(np.float64)\n",
    "\n",
    "    # Apply Sobel filter in the x-direction\n",
    "    sobelx = cv2.Sobel(channel_data, cv2.CV_64F, dx=1, dy=0, ksize=31)\n",
    "\n",
    "    # Compute the gradient magnitude\n",
    "    sobelx = np.sqrt(sobelx**2)\n",
    "\n",
    "    # Replace the channel data with the Sobel filtered data\n",
    "    spectrogram_processed[:, :, ch] = cv2.normalize(sobelx, None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "# Normalize the phase channels (channels 4 and 5)\n",
    "spectrogram_processed[:, :, NUM_POWER_CHANNELS:] -= PHASE_SPECTROGRAM_LIMITS[0]\n",
    "spectrogram_processed[:, :, NUM_POWER_CHANNELS:] /= PHASE_SPECTROGRAM_LIMITS[1] - PHASE_SPECTROGRAM_LIMITS[0]\n",
    "\n",
    "spectrogram_processed = torch.tensor(spectrogram_processed).unsqueeze(0).to(DEVICE).float()\n",
    "target = torch.tensor([10.0]).to(DEVICE).squeeze()\n",
    "\n",
    "\n",
    "# Define model parameter\n",
    "model_prepro = CNN_97(\n",
    "    num_conv_layers=2,  # Example value, use the same as the trained model\n",
    "    num_fc_layers=3,\n",
    "    conv_dropout=0,\n",
    "    linear_dropout=0,\n",
    "    kernel_size=(5, 5),\n",
    "    activation=nn.ReLU,  # Example activation function, change as needed\n",
    "    hidden_units=64,\n",
    "    padding=1,\n",
    "    stride=1,\n",
    "    pooling_size=1,\n",
    "    out_channels=8,\n",
    "    use_fc_batchnorm=True,\n",
    "    use_cnn_batchnorm=True\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load pretrained model\n",
    "model_path = MODEL_DIR / \"CNNprepro_best\"\n",
    "model_prepro.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_prepro.eval()\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = model_prepro.loss_fn\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    test_output = model_prepro(spectrogram_processed).squeeze()\n",
    "    test_loss = loss_fn(test_output.squeeze(), target)\n",
    "    test_rmse = test_loss.sqrt()\n",
    "\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Optimized CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameter\n",
    "model_optimCNN = CNN_97_withAtt(\n",
    "    num_conv_layers=3,  # Example value, use the same as the trained model\n",
    "    num_fc_layers=3,\n",
    "    conv_dropout=0,\n",
    "    linear_dropout=0,\n",
    "    kernel_size=(5, 7),\n",
    "    activation=nn.ReLU,  # Example activation function, change as needed\n",
    "    hidden_units=64,\n",
    "    padding=0,\n",
    "    stride=2,\n",
    "    pooling_size=1,\n",
    "    out_channels=8,\n",
    "    use_fc_batchnorm=True,\n",
    "    use_cnn_batchnorm=True\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load pretrained model\n",
    "model_path = MODEL_DIR / \"CNNoptimized_best\"\n",
    "model_optimCNN.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_optimCNN.eval()\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = model_optimCNN.loss_fn\n",
    "\n",
    "# # Test model\n",
    "test_data_path = DATA_ROOT / \"dummy_stacked_spectrograms.npy\"\n",
    "test_data = np.transpose(np.load(test_data_path), (2,0,1))\n",
    "\n",
    "spectrogram = torch.tensor(test_data).unsqueeze(0).to(DEVICE).float()\n",
    "target = torch.tensor([10.0]).to(DEVICE).squeeze()\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    test_output = model_optimCNN(spectrogram).squeeze()\n",
    "    test_loss = loss_fn(test_output.squeeze(), target)\n",
    "    test_rmse = test_loss.sqrt()\n",
    "\n",
    "\n",
    "print(f\"Test RMSE: {test_rmse.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
